---
title: "Day_82 02. ì°¢ì€ ëª¨ë¸ ê¾¸ê²¨ ë„£ê¸°: Quantization ì‹¤ìŠµ(with torch, tensorrt)"

categories:
  - Boostcamp_AI_Tech/Week_18/Day_82
tags:
  - ëª¨ë¸ìµœì í™”
---
  
# ì°¢ì€ ëª¨ë¸ ê¾¸ê²¨ ë„£ê¸°: Quantization ì‹¤ìŠµ(with torch, tensorrt)

## 1. Quantization ì¢…ë¥˜

### 1.1 Review & Overview

**Quantization**

- ê¸°ì¡´ì˜ high precision(ì¼ë°˜ì ìœ¼ë¡œ fp32) Neural network ì˜ weights ì™€ activation ì„ ë” ì ì€ bit(low precision)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒ
- Quantized Matrix Multiplication, Activation, Layer fusion, ...

![](../assets/images/2300e2b7.png)

**Quantization approach êµ¬ë¶„**

- Post Training Quantization(PTQ): í•™ìŠµ í›„ì— quantization parameter(scale, shift)ë¥¼ ê²°ì •
- Quantization Aware Training(QAT): í•™ìŠµ ê³¼ì •ì— quantization ì„ emulate í•¨ìœ¼ë¡œì¨, quantization ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ì„±ëŠ¥í•˜ë½ì„ ì™„í™”í•¨

### 1.2 Post Training Quantization(PTQ)

**PTQ ê¸°ë²• ì •ë¦¬**

- Dynamic range quantization(weight only quantization): weight ë§Œ quantize ë¨(8-bit), inference ì‹œì—ëŠ” floating-point ë¡œ 
ë³€í™˜ë˜ì–´ ìˆ˜í–‰
- Full integer quantization(weight and activation quantization): weight ì™€ ë”ë¶ˆì–´ ëª¨ë¸ì˜ ì…ë ¥ ë°ì´í„°, activation(ì¤‘ê°„ ë ˆì´ì–´ì˜
output)ë“¤ ë˜í•œ quantize
- Float 16 quantization: fp32 ì˜ ë°ì´í„° íƒ€ì…ì˜ weight ë¥¼ fp16 ìœ¼ë¡œ quantize

![](../assets/images/fd8134a0.png)

**ì–´ë–¨ë•Œ ì–´ë–¤ ê¸°ë²•ì„ ì ìš©í•´ì•¼í• ê¹Œ?(From TF[2])**

![](../assets/images/ba83d043.png)

![](../assets/images/1939e39c.png)

**ì°¸ê³ : PyTorch ì˜ êµ¬ë¶„**

- Dynamic range quantization(Dynamic quantization)ì„ ë³„ë„ì˜ ë²”ì£¼ë¡œ êµ¬ë¶„
- Post Training Quantization ì€ Static quantization ì´ë¼ê³ ë„ ì¹­í•¨
- Dynamic quantization ì˜ ê²½ìš°, model ìˆ˜í–‰ ì‹œê°„ì´ weights ë¥¼ load í•˜ëŠ” ê²ƒì´ ì‹¤ì œ matrix multiplication ë³´ë‹¤ ë” ì˜¤ë˜ ê±¸ë¦¬ëŠ”
LSTM, Transformer ê¸°ë°˜ì˜ ëª¨ë¸ì— íš¨ê³¼ì ì´ë¼ëŠ” ì–¸ê¸‰ì´ ìˆìŒ

![](../assets/images/870439aa.png)

**1) Dynamic range quantization(weight only quantization)**

- ë„¤íŠ¸ì›Œí¬ì˜ Weight ë§Œ quantize ë¨(8bit)
- Pros ğŸ˜:
  - ë³„ë„ì˜ calibration(validation) ë°ì´í„°ê°€ í•„ìš”í•˜ì§€ ì•ŠìŒ
  - ëª¨ë¸ì˜ ìš©ëŸ‰ ì¶•ì†Œ(8bit ê¸°ì¤€ 1/4)
- Cons ğŸ˜¥:
  - ì‹¤ì œ ì—°ì‚°ì€ floating point ë¡œ ìˆ˜í–‰ë¨
  - ì‹¤ì œ ì†ë„ì˜ ì´ì ì´ í¬ì§€ ì•ŠìŒ

**2) Full integer quantization(weight and activation quantization)**

- Weight ì™€ ë”ë¶ˆì–´ ëª¨ë¸ì˜ ì…ë ¥ ë°ì´í„°, activation(ì¤‘ê°„ ë ˆì´ì–´ì˜ output)ë“¤ë„ quantize ë¨
- Pros ğŸ˜:
  - ëª¨ë¸ì˜ ìš©ëŸ‰ ì¶•ì†Œ(8bit ê¸°ì¤€ 1/4)
  - ë” ì ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, cache ì¬ì‚¬ìš©ì„± ì¦ê°€
  - ë¹ ë¥¸ ì—°ì‚°(fixed point 8bit ì—°ì‚°ì„ ì§€ì›í•˜ëŠ” ê²½ìš°)
- Cons ğŸ˜¥:
  - Activation ì˜ parameter ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•˜ì—¬ calibration ë°ì´í„°ê°€ í•„ìš”í•¨  
    (ì£¼ë¡œ training ë°ì´í„°ì—ì„œ ì‚¬ìš©, ì•½ 100ê°œì˜ ë°ì´í„°) $\rightarrow$ í•­ìƒì€ ì•„ë‹˜
  - TF ì—ì„œëŠ” 100ê°œ ì •ë„ë©´ ë˜ê³  tensorRT ì—ì„œëŠ” 1000ê°œ ì •ë„ í•„ìš”

- TensorRT ì˜ quantization ì€ bias(zero point)ê°€ ì—†ëŠ” Symmetric quantization  
Why? ì—°ì‚°ì´ í›¨ì”¬ ê°„ë‹¨í•˜ê³ , (ê²½í—˜ì ìœ¼ë¡œ) ì„±ëŠ¥ì— í° ì˜í–¥ì„ ì£¼ì§€ ì•Šì•˜ë‹¤

![](../assets/images/43ef16b7.png)

**2) Full integer quantization; TensorRT(NVIDIA) calibration ì˜ˆì‹œ**

- Calibration?; ì„±ëŠ¥ ì €í•˜ë¥¼ ìµœì†Œë¡œ í•˜ëŠ” threshold ì°¾ê¸°

![](../assets/images/f889e811.png)










# Further Reading
 
- [Pytorch official docs: quantization](https://pytorch.org/docs/stable/quantization.html)










