---
title: "Day_39 02. BERT ì–¸ì–´ëª¨ë¸ ê¸°ë°˜ì˜ ë¬¸ì¥ í† í° ë¶„ë¥˜"

categories:
  - Boostcamp_AI_Tech/Week_9/Day_39
tags:
  - KLUE
---
  
# BERT ì–¸ì–´ëª¨ë¸ ê¸°ë°˜ì˜ ë¬¸ì¥ í† í° ë¶„ë¥˜

## 1. ë¬¸ì¥ í† í° ë¶„ë¥˜ task ì†Œê°œ

### 1.1 ë¬¸ì¥ í† í° ê´€ê³„ ë¶„ë¥˜ task

ì£¼ì–´ì§„ ë¬¸ì¥ì˜ ê° **token** ì´ ì–´ë–¤ ë²”ì£¼ì— ì†í•˜ëŠ”ì§€ ë¶„ë¥˜í•˜ëŠ” task

![]({{site.url}}/assets/images/e854c887.png)

í† í° ë¶„ë¥˜ task ëŠ” ë¬¸ì¥ì´ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ê²Œ ë˜ë©´ ê°ê° token ì— ëŒ€í•´ classification layer ê°€ ë¶€ì°©ì´ ë˜ì„œ ê° token ì´
ì–´ë–¤ token ì¸ì§€ ì…ë ¥ëœ ë¬¸ì¥ì˜ ê° token ë“¤ì„ ë¶„ë¥˜í•˜ëŠ” task ì„

**Named Entity Recognition (NER)**

- ê°œì²´ëª… ì¸ì‹ì€ ë¬¸ë§¥ì„ íŒŒì•…í•´ì„œ ì¸ëª…, ê¸°ê´€ëª…, ì§€ëª… ë“±ê³¼ ê°™ì€ ë¬¸ì¥ ë˜ëŠ” ë¬¸ì„œì—ì„œ íŠ¹ì •í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ê³  ìˆëŠ” ë‹¨ì–´ ë˜ëŠ” ì–´êµ¬(ê°œì²´)
ë“±ì„ ì¸ì‹í•˜ëŠ” ê³¼ì •ì„ ì˜ë¯¸í•œë‹¤.

![]({{site.url}}/assets/images/01d6704b.png)

ì¬ë°ŒëŠ” ì˜ˆì‹œê°€ ìˆìŒ

![]({{site.url}}/assets/images/a384a970.png)

**ê°™ì€ ë‹¨ì–´**ë¼ë„ ë¬¸ë§¥ì—ì„œ ë‹¤ì–‘í•œ ê°œì²´(Entity)ë¡œ ì‚¬ìš©ë¨

ê°œì²´ëª…ì¸ì‹ê¸°ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ ì´ ëª¨ë¸ì´ ë¬¸ë§¥ì„ ì˜¬ë°”ë¥´ê²Œ íŒŒì•…í•˜ê³  ìˆëŠ”ê°€ ì•„ë‹Œê°€ ì´ê¸° ë•Œë¬¸ì— ë¬¸ë§¥ì„ ê³ ë ¤í•˜ëŠ” ê²ƒì´ êµ‰ì¥íˆ ì¤‘ìš”!

ì§€ê¸ˆ ë‚˜ì˜¨ ì˜ˆì œëŠ” ì¹´ì¹´ì˜¤ë¸Œë ˆì¸ì—ì„œ ê°œë°œí•œ `PORORO` ë¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì„  
ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ë©´ ê°œì²´ëª…ì¸ì‹ í˜¹ì€ ê´€ê³„ì¶”ì¶œ, ë‘ë¬¸ì¥ ê´€ê³„ë¶„ì„ ì‹¬ì§€ì–´ summary ê¹Œì§€ ë­ ê±°ì˜ í•œêµ­ì–´ë¡œ í•  ìˆ˜ ìˆëŠ” 
ëª¨ë“  ìì—°ì–´ì²˜ë¦¬ê°€ í•˜ë‚˜ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë‹¤ ë“¤ì–´ê°€ìˆìŒ  
ê·¸ë˜ì„œ `PORORO` ë¥¼ í™œìš©í•˜ë©´ ë¹ ë¥´ê²Œ proto typing ì„ í•´ì•¼í•œë‹¤ë˜ê°€ í˜¹ì€ êµ¬ìƒí•œ ì•„ì´ë””ì–´ë¥¼ ë¹ ë¥´ê²Œ êµ¬í˜„í•´ë³´ëŠ” ê·¸ëŸ° test ë¥¼ 
í•˜ëŠ”ë° êµ‰ì¥íˆ ìš©ì´í•¨

**Part-of-speech tagging (POS TAGGING)**

- í’ˆì‚¬ë€ ë‹¨ì–´ë¥¼ ë¬¸ë²•ì  ì„±ì§ˆì˜ ê³µí†µì„±ì— ë”°ë¼ ì–¸ì–´í•™ìë“¤ì´ ëª‡ ê°ˆë˜ë¡œ ë¬¶ì–´ ë†“ì€ ê²ƒì´ë‹¤.
- í’ˆì‚¬ íƒœê¹…ì€ ì£¼ì–´ì§„ ë¬¸ì¥ì˜ ê° ì„±ë¶„ì— ëŒ€í•˜ì—¬ ê°€ì¥ ì•Œë§ëŠ” í’ˆì‚¬ë¥¼ íƒœê¹…í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.

![]({{site.url}}/assets/images/6f78ffbd.png)

í’ˆì‚¬íƒœê¹…, í˜•íƒœì†Œíƒœê¹… ê°™ì€ ì˜ˆì‹œê°€ ìˆìŒ

í˜•íƒœì†Œíƒœê¹…ì€ ì–¸ì–´ê°€ ê°€ì§€ëŠ” ê°€ì¥ ìµœì†Œì˜ ë‹¨ìœ„ë¡œ ë¶„ë¥˜í•˜ëŠ” ê²ƒì„ ì–˜ê¸°í•¨

ê·¸ë˜ì„œ ê°œì²´ëª… ì¸ì‹ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ì£¼ì–´ì§„ ë¬¸ì¥ì— ëŒ€í•´ ë¬¸ë§¥ì„ íŒŒì•…í•˜ë©´ í˜•íƒœì†Œ ë¶„ì„ì„ í•  ìˆ˜ ê°€ ìˆì„ ê²ƒì„

![]({{site.url}}/assets/images/c528f75f.png)

`PORORO` ì—ë„ POS tagging ì„ ì§€ì›í•˜ê³  ìˆìŒ

### 1.2 ë¬¸ì¥ token ë¶„ë¥˜ë¥¼ ìœ„í•œ ë°ì´í„°

**kor_ner**

- í•œêµ­í•´ì–‘ëŒ€í•™êµ ìì—°ì–´ ì²˜ë¦¬ ì—°êµ¬ì‹¤ì—ì„œ ê³µê°œí•œ í•œêµ­ì–´ NER ë°ì´í„°ì…‹
- ì¼ë°˜ì ìœ¼ë¡œ, NER ë°ì´í„°ì…‹ì€ pos tagging ë„ í•¨ê»˜ ì¡´ì¬

![]({{site.url}}/assets/images/31d9d16c.png)

ê°œì²´ëª… ì¸ì‹ì´ ê°€ëŠ¥í•œ ì½”ë“œë¥¼ í™œìš©í•´ì„œ POS taggin ì´ ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ë„ í•™ìŠµí•  ìˆ˜ ìˆìŒ

ì´ ë°ì´í„°ì…‹ì˜ ê°€ì¥ í° íŠ¹ì§•ì¤‘ì˜ í•˜ë‚˜ëŠ” ê°œì²´ëª…ì¸ì‹ì— ëŒ€í•´ì„œ ê°œì²´ëª… tagging ì„ BIO ë¼ëŠ” tag ë¡œ êµ¬ì„±í–ˆìŒ  

kor_ner ì—ì„œ ë‹¤ë£¨ëŠ” ê°œì²´ëª…ì€ ë‹¤ìŒê³¼ ê°™ìŒ
- Entity tag ì—ì„œ B ì˜ ì˜ë¯¸ëŠ” ê°œì²´ëª…ì˜ ì‹œì‘(Begin)ì„ ì˜ë¯¸í•˜ê³ , I ì˜ ì˜ë¯¸ëŠ” ë‚´ë¶€(Inside)ë¥¼ ì˜ë¯¸í•˜ë©°, O ëŠ” 
ë‹¤ë£¨ì§€ ì•ŠëŠ” ê°œì²´ëª…(Outside)ë¥¼ ì˜ë¯¸í•œë‹¤.
  - B : Begin ì´ë¼ëŠ” ì˜ë¯¸
  - I : Inner ì˜ ì˜ë¯¸
  - O : Out ì˜ ì˜ë¯¸
- ì¦‰, B-PER ì€ ì¸ë¬¼ëª… ê°œì²´ëª…ì˜ ì‹œì‘ì„ ì˜ë¯¸í•˜ë©°, I-PER ëŠ” ì¸ë¬¼ëª… ê°œì²´ëª…ì˜ ë‚´ë¶€ ë¶€ë¶„ì„ ëœ»í•œë‹¤.
- kor_ner ë°ì´í„°ì…‹ì—ì„œ ë‹¤ë£¨ëŠ” ê°œì²´ëª…ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

![]({{site.url}}/assets/images/b023156e.png)

## 2. ë¬¸ì¥ í† í° ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì‹¤ìŠµ

### 2.1 ë¬¸ì¥ í† í° ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ

**NER fine-tuning with BERT**

![]({{site.url}}/assets/images/5183f204.png)

ê°ê°ì˜ token ì— ëŒ€í•´ì„œ classification layer ë¥¼ ë¶€ì°©í•´ì„œ ë¶„ë¥˜í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ task ë¥¼ ì§„í–‰í•¨

**ì£¼ì˜ì **

ì •ë‹µì€ ì•„ë‹ˆì§€ë§Œ ê°œì²´ëª…ì¸ì‹ì„ í•  ë•Œ í•­ìƒ ìŒì ˆë‹¨ìœ„ë¡œ ë¶„ì„ì„ í•˜ê²Œ ë§Œë“¦  
ì™œëƒë©´ tokenizer ì˜ ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ”ë° BERT ê°™ì€ ê²½ìš°ì—” WordPiece tokenizer ë¥¼ ì‚¬ìš©í•˜ê²Œ ë¨  
ê·¸ëŸ¬ë©´ WordPiece tokenizer ê°€ ì˜¬ë°”ë¥´ê²Œ ìë¥´ì§€ ëª»í•œ ì¡´ì¬ì— ëŒ€í•´ì„œëŠ” ê°œì²´ëª…ë„ error ê°€ ë‚  ìˆ˜ ë°–ì— ì—†ìŒ  
ì˜ˆë¥¼ ë“¤ì–´ì„œ "ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤" ë¼ëŠ” ë¬¸ì¥ì´ ìˆì„ ë•Œ WordPiece tokenizer ê°€ "ì´ìˆœ", "ì‹ ì€" ì´ë ‡ê²Œ
ë¶„ë¦¬ë¥¼ í–ˆë‹¤ê³  ê°€ì •í•´ë³´ì  
"ì´ìˆœ" ì€ PER ë¡œ ë¶„ë¥˜ê°€ ë  ìˆ˜ ìˆìŒ  
ê·¸ëŸ¬ë©´ "ì‹ " ë„ PER ë¡œ ë¶„ë¥˜ê°€ ë˜ì–´ì•¼ í•¨  
ê·¸ëŸ°ë° ì• ì´ˆì— tokenizer ê°€ "ì´ìˆœì‹ " ì„ "ì´", "ìˆœ", "ì‹ " ì²˜ëŸ¼ ë”°ë¡œë”°ë¡œ ë¶„ë¥˜í•œê²Œ ì•„ë‹ˆê³  "ì´ìˆœ" ê³¼ "ì‹ ì€" ì´ë¼ê³  ë¶„ë¦¬ë¥¼
í•œ ê²½ìš°ì— "ì‹ ì€" ì€ ì•„ë¬´ë¦¬ ì˜ë¼ë„ PER ì´ ë‚˜ì˜¬ ìˆ˜ ì—†ìŒ  
ê·¸ë˜ì„œ tokenizer ìì²´ê°€ token ì„ ì˜ ëª» ì˜ëê¸°ì— ë°œìƒí•˜ëŠ” ë¬¸ì œì„  
ê·¸ë˜ì„œ ìŒì ˆë‹¨ìœ„ë¡œ ìë¥´ëŠ”ê±¸ ë§ì´ ì¶”ì²œë“œë¦¼  
ê°œì²´ëª… ì¸ì‹ê¸°ë¥¼ ìŒì ˆë‹¨ìœ„ë¡œ í•™ìŠµì„ í•˜ê²Œë˜ë©´ ì•„ë˜ì™€ ê°™ì´ ë°ì´í„°ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆìŒ

![]({{site.url}}/assets/images/9dd010ff.png)

í˜•íƒœì†Œ ë‹¨ìœ„ì˜ í† í°ì„ **ìŒì ˆ ë‹¨ìœ„**ì˜ í† í°ìœ¼ë¡œ ë¶„í•´í•˜ê³ , Entity tag ì—­ì‹œ ìŒì ˆ ë‹¨ìœ„ë¡œ ë§¤í•‘ì‹œì¼œ ì£¼ì–´ì•¼ í•¨

**í•™ìŠµ ê³¼ì •**

![]({{site.url}}/assets/images/4ec05905.png)

---

# ì‹¤ìŠµ

## ë¬¸ì¥ í† í° ë‹¨ìœ„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ

ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œ ë°›ì

`!git clone https://github.com/kmounlp/NER.git`

ëª¨ë“  íŒŒì¼ì„ ê°€ì ¸ì™€ì„œ ì¶œë ¥ì„ í•´ë³´ì

```python
import os
import glob

file_list = []
for x in os.walk('/content/NER/'):
    for y in glob.glob(os.path.join(x[0], '*_NER.txt')):    # ner.*, *_NER.txt
        file_list.append(y)

file_list = sorted(file_list)

for file_path in file_list:
    print(file_path)
```

![]({{site.url}}/assets/images/23e111c2.png)

ë°ì´í„° ìƒ˜í”Œì„ í™•ì¸í•´ë³´ì

```python
from pathlib import Path

file_path = file_list[0]
file_path = Path(file_path)
raw_text = file_path.read_text().strip()

print(raw_text[0:1000])
```

![]({{site.url}}/assets/images/8a61f1b4.png)

í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ tokenizing ë˜ì–´ìˆê³  BIO tag ë¡œ ê°œì²´ëª… tag ê°€ ë¶€ì°©ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

ì´ê±¸ ìŒì ˆë‹¨ìœ„ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„ ë¨¼ì € ê±°ì¹  ê²ƒì„

ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” ê³¼ì •ì„ ë¨¼ì € ì§„í–‰í•˜ì

ì´ ë°ì´í„°ì…‹ì€ ê¸°ì¡´ì˜ ìš°ë¦¬ê°€ ë´¤ë˜ ë°ì´í„°ì…‹ê³¼ í˜•íƒœê°€ ë‹¤ë¦„

ì–´ë–»ê²Œ ë‹¤ë¥´ëƒë©´?  
ê¸°ì¡´ì—ëŠ” ëª¨ë“  ë°ì´í„°ì…‹ì´ line-by-line ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆì—ˆìŒ  
ì²«ë²ˆì§¸ line ì— ì²«ë²ˆì§¸ í–‰ì„ ë³´ë©´ sentence ê°€ ìˆê³  ë‘ë²ˆì§¸ì—ëŠ” label ì´ ë¶€ì°©ë˜ì–´ìˆê³  ê·¸ë ‡ê¸° ë•Œë¬¸ì—  
line-by-line ìœ¼ë¡œë§Œ ì½ìœ¼ë©´ ë¬¸ì œì—†ì´ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•  ìˆ˜ ìˆì—ˆìŒ  
ê·¸ëŸ°ë° ì´ ë°ì´í„°ì…‹ì€ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ enter ë¡œ êµ¬ë¶„ë˜ì–´ ìˆëŠ”ê±¸ ë³¼ ìˆ˜ ìˆìŒ  
BERT ë¡œ ì…ë ¥ì´ ë“¤ì–´ê°ˆ ë•ŒëŠ” ì´ê²ƒì„ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ í•©ì³ì„œ ì…ë ¥ì´ ë˜ì–´ì•¼ í•¨  
ê·¸ëŸ¬ë‹ˆê¹Œ ì´ë ‡ê²Œ ì„¸ë¡œë¡œ ë˜ì–´ìˆëŠ” ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ê³  ì„¸ë¡œë¡œ ë‚˜ëˆ„ì–´ì ¸ìˆëŠ” BIO tag ë“¤ì„ ê°ê°ì˜
token ì— ë§ëŠ” label ë¡œì„œ ì…ë ¥ì„ ë„£ì–´ì¤˜ì•¼ í•™ìŠµì´ ì´ë£¨ì–´ì§€ê²Œ ë¨  
ì´ëŸ¬í•œ ê³¼ì •ì„ í•  ìˆ˜ ìˆëŠ” ì „ì²˜ë¦¬ ê³¼ì •ì„ ì§„í–‰í•´ë³´ì

```python
import re

def read_file(file_list):
    token_docs = []
    tag_docs = []
    for file_path in file_list:
        # print("read file from ", file_path)
        file_path = Path(file_path)
        raw_text = file_path.read_text().strip()
        raw_docs = re.split(r'\n\t?\n', raw_text)
        for doc in raw_docs:
            tokens = []
            tags = []
            for line in doc.split('\n'):
                if line[0:1] == "$" or line[0:1] == ";" or line[0:2] == "##":
                    continue
                try:
                    token = line.split('\t')[0]
                    tag = line.split('\t')[3]   # 2: pos, 3: ner
                    for i, syllable in enumerate(token):    # ìŒì ˆ ë‹¨ìœ„ë¡œ ì˜ë¼ì„œ
                        tokens.append(syllable)
                        modi_tag = tag
                        if i > 0:
                            if tag[0] == 'B':
                                modi_tag = 'I' + tag[1:]    # BIO tagë¥¼ ë¶€ì°©í• ê²Œìš” :-)
                        tags.append(modi_tag)
                except:
                    print(line)
            token_docs.append(tokens)
            tag_docs.append(tags)

    return token_docs, tag_docs
```

ë°ì´í„° ì „ì²´ë¥¼ ë‹¤ ì½ê³  ì´ì¤‘ enter ë¥¼ ê¸°ì¤€ìœ¼ë¡œ document ë¡œì„œ êµ¬ë¶„í•˜ê²Œ ë¨  
ê°ê°ì˜ line ì„ ì½ì–´ë‚˜ê°€ë©´ì„œ ê·¸ token ë“¤ì„ ë¬¸ì¥ìœ¼ë¡œ ë¶€ì°©í•˜ëŠ” ê³¼ì •ì„ ê±°ì¹˜ê²Œ ë¨  
ìš°ë¦¬ì˜ ë°ì´í„°ì…‹ì€ tokenizing ë‹¨ìœ„ê°€ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë˜ì–´ìˆì—ˆìŒ  
ê·¸ë˜ì„œ ì´ê²ƒì„ ìŒì ˆë‹¨ìœ„ë¡œ ë°”ê¾¸ëŠ” ê³¼ì •ì„ ìœ„í•´ ë°˜ë³µë¬¸ì„ ëŒë©´ì„œ ìŒì ˆ ë‹¨ìœ„ë¡œ token ì„ ìë¥´ëŠ” ê³¼ì •ì„ ê±°ì¹˜ë„ë¡ í•¨  
ê°ê°ì˜ `syllable(ìŒì ˆ)` ìŒì ˆ ë‹¨ìœ„ ì •ë³´ë¥¼ ê°€ì ¸ì™€ì„œ token ë’¤ì— ê³„ì†í•´ì„œ append ë¥¼ í•˜ë©´ì„œ ë¬¸ì¥ì„ ë§Œë“¤ì–´ ë‚˜ê°  
ì´ ë•Œ ì˜ë¼ì§„ ìŒì ˆì— ëŒ€í•´ì„œ ë§ˆì°¬ê°€ì§€ë¡œ BIO tag ë¥¼ ë¶€ì°©í•˜ëŠ” ê³¼ì •ì„ ê±°ì¹¨  

ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ tag ì— ë°ì´í„°ì…‹ì„ í™•ì¸í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŒ

```python
texts, tags = read_file(file_list[:])

print(len(texts))
print(len(tags))

print(texts[0], end='\n\n') # ìŒì ˆ ë‹¨ìœ„ë¡œ ì˜ ì˜ë ¸ë„¤ìš”!
print(tags[0])
```

![]({{site.url}}/assets/images/a28ff3a8.png)
![]({{site.url}}/assets/images/a60ae493.png)

ê¸°ì¡´ì˜ ë°ì´í„°ì…‹ì´ ì „ë¶€ ìŒì ˆë‹¨ìœ„ë¡œ ì˜ë¼ì¡Œê³  tag ë„ ì˜ ë¶€ì°©ë˜ì—ˆìŒ

ë°ì´í„°ê°€ ë‹¤ ì¤€ë¹„ë˜ì—ˆìœ¼ë‹ˆ í•™ìŠµì„ ìœ„í•œ ê³¼ì •ì„ ì§„í–‰í•˜ì

í•™ìŠµì„ í•˜ë ¤ë©´ ë¨¼ì € ì´ tag ë“¤ì„ ìˆ«ìë¡œ ë°”ê¿”ì¤˜ì•¼ í•¨

vocab id ì²˜ëŸ¼ tag label id ë¡œ ë°”ê¿”ì¤˜ì•¼ í•¨

```python
unique_tags = set(tag for doc in tags for tag in doc)
tag2id = {tag: id for id, tag in enumerate(unique_tags)}
id2tag = {id: tag for tag, id in tag2id.items()}

for i, tag in enumerate(unique_tags):
    print(tag)  # í•™ìŠµì„ ìœ„í•œ label listë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
```

![]({{site.url}}/assets/images/752c9ad1.png)

ë°ì´í„°ê°€ ë³´ìœ í•˜ê³  ìˆëŠ” ê°œì²´ëª… tag ë“¤ì˜ ì¢…ë¥˜ì„

ë°ì´í„°ì…‹ì„ í™•ì¸í•´ë³´ì

```python
import numpy as np
import matplotlib.pyplot as plt

texts_len = [len(x) for x in texts]

plt.figure(figsize=(16,10))
plt.hist(texts_len, bins=50, range=[0,800], facecolor='b', density=True, label='Text Length')
plt.title('Text Length Histogram')
plt.legend()
plt.xlabel('Number of Words')
plt.ylabel('Probability')
```

![]({{site.url}}/assets/images/9ee08691.png)

ë¬¸ì¥ì˜ ê¸¸ì´ë‚˜ í‰ê· ê¸¸ì´ë¥¼ í™•ì¸í•˜ë©´ ë‚˜ì¤‘ì— ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬ ê³¼ì •ì„ í†µí•´ ì œê±°ë¥¼ í•´ì£¼ê±°ë‚˜ í•„í„°ë§ í•  ìˆ˜ ìˆìŒ

ê° NER íƒœê·¸ì— í¬í•¨ë˜ì–´ ìˆëŠ” ë°ì´í„°ì˜ ê°œìˆ˜ë¥¼ í™•ì¸í•´ë³´ì

```python
for tag in list(tag2id.keys()) : 
    globals()[tag] = 0

for tag in tags : 
    for ner in tag : 
        globals()[ner] += 1

for tag in list(tag2id.keys()) : 
    print('{:>6} : {:>7,}'. format(tag, globals()[tag]))
```

![]({{site.url}}/assets/images/92bb822e.png)

ê°ê°ì˜ tag ì— ëŒ€í•´ì„œ ë°ì´í„°ê°€ ì–´ëŠì •ë„ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ê°œìˆ˜ë¥¼ íŒŒì•…

ê°ê°ì˜ tag ë§ˆë‹¤ ê°€ì§€ê³  ìˆëŠ” ë°ì´í„°ì˜ ê°œìˆ˜ê°€ ë‹¤ ë‹¤ë¦„  
ì¼ë°˜ì ìœ¼ë¡œ ê°œìˆ˜ê°€ ë¶€ì¡±í•œ tag ì— ëŒ€í•´ì„œëŠ” í•™ìŠµ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ë°–ì— ì—†ìŒ  
ê·¸ëŸ´ë•ŒëŠ” í•´ë‹¹ tag ì— ê´€ë ¨ëœ ë°ì´í„°ì…‹ì„ ë” ì¶”ê°€í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ë³´ì™„ì„ í•  ìˆ˜ ìˆìŒ

train ê³¼ test set ì„ ë¶„ë¦¬í•´ì£¼ì

```python
from sklearn.model_selection import train_test_split
train_texts, test_texts, train_tags, test_tags = train_test_split(texts, tags, test_size=.2) 

print('Train ë¬¸ì¥ : {:>6,}' .format(len(train_texts)))
print('Train íƒœê·¸ : {:>6,}' .format(len(train_tags)))
print('Test  ë¬¸ì¥ : {:>6,}' .format(len(test_texts)))
print('Test  íƒœê·¸ : {:>6,}' .format(len(test_tags)))
```

![]({{site.url}}/assets/images/f302f45e.png)

BERT tokenizer ì™€ model ì„ ê°€ì ¸ì™€ì„œ ì‹¤ì§ˆì ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•´ë³´ì

```python
from transformers import AutoModel, AutoTokenizer, BertTokenizer
MODEL_NAME = "bert-base-multilingual-cased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
```

```python
pad_token_id = tokenizer.pad_token_id # 0
cls_token_id = tokenizer.cls_token_id # 101
sep_token_id = tokenizer.sep_token_id # 102
pad_token_label_id = tag2id['O']    # tag2id['O']
cls_token_label_id = tag2id['O']
sep_token_label_id = tag2id['O']
```

í˜„ì¬ ëª¨ë¸ì˜ ê°ê°ì˜ token ì— ëŒ€í•´ì„œ token ì— í•´ë‹¹í•˜ëŠ” label ì´ ë¶€ì°©ì´ ë˜ì–´ ìˆëŠ”ë° í•˜ì§€ë§Œ [CLS] í† í°ì´ë‚˜ í˜¹ì€ [SEP] í† í°
ê·¸ë¦¬ê³  [PAD] í† í°ì— ëŒ€í•´ì„œëŠ” label ì´ ë°ì´í„°ì…‹ì— ì¡´ì¬í•˜ì§€ ì•ŠìŒ  
ê·¸ë˜ì„œ ì„ì˜ë¡œ ê·¸ëŸ° ë°ì´í„°ì…‹ì„ ë§Œë“¤ì–´ì£¼ê²Œ ë¨  
ì–´ì¨‹ë“  ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°ˆ ë•ŒëŠ” ë¬¸ì¥ì˜ í˜•íƒœë¡œ ë“¤ì–´ê°€ê²Œ ë í…ë° ê·¸ ë¬¸ì¥ì— ìˆëŠ” [CLS] í† í°ì€ O tag ë¡œ ë‚˜ì˜¤ê³  [SEP] í† í°ë„
O tag ë¡œ ë‚˜ì˜¬ ìˆ˜ ìˆê²Œ [PAD] í† í°ë„ O tag ë¡œ ë‚˜ì˜¤ê²Œ label ì„ ì§€ì •í•´ì¤Œ

ê¸°ì¡´ì˜ tokenizer ëŠ” WordPiece tokenizer ë¥¼ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì— ì…ë ¥ëœ ê²°ê³¼ ëŒ€í•´ tokenizer ë¥¼ íƒœìš°ë©´ tokenizer ê²°ê³¼ê°€
ë‹¤ë¥´ê²Œ ë‚˜ì˜¬ ê²ƒì„  
ì™œëƒí•˜ë©´ ìš°ë¦¬ì˜ ë°ì´í„°ëŠ” ìŒì ˆë‹¨ìœ„ë¡œ ë§Œë“¤ì–´ì ¸ìˆê¸° ë•Œë¬¸ì„  
ìŒì ˆ ë‹¨ìœ„ë¡œ tokenizing í•  ìˆ˜ ìˆëŠ” ê·¸ëŸ° tokenizer ë¥¼ ë§Œë“¤ì–´ì¤˜ì•¼ í•¨  

```python
# ê¸°ì¡´ í† í¬ë‚˜ì´ì €ëŠ” wordPiece tokenizerë¡œ tokenizing ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
# ë°ì´í„° ë‹¨ìœ„ë¥¼ ìŒì ˆ ë‹¨ìœ„ë¡œ ë³€ê²½í–ˆê¸° ë•Œë¬¸ì—, tokenizerë„ ìŒì ˆ tokenizerë¡œ ë°”ê¿€ê²Œìš”! :-)

def ner_tokenizer(sent, max_seq_length):    
    pre_syllable = "_"
    input_ids = [pad_token_id] * (max_seq_length - 1)
    attention_mask = [0] * (max_seq_length - 1)
    token_type_ids = [0] * max_seq_length
    sent = sent[:max_seq_length-2]

    for i, syllable in enumerate(sent):
        if syllable == '_':
            pre_syllable = syllable
        if pre_syllable != "_":
            syllable = '##' + syllable  # ì¤‘ê°„ ìŒì ˆì—ëŠ” ëª¨ë‘ prefixë¥¼ ë¶™ì…ë‹ˆë‹¤.
            # ì´ìˆœì‹ ì€ ì¡°ì„  -> [ì´, ##ìˆœ, ##ì‹ , ##ì€, ì¡°, ##ì„ ]
        pre_syllable = syllable

        input_ids[i] = (tokenizer.convert_tokens_to_ids(syllable))
        attention_mask[i] = 1
    
    input_ids = [cls_token_id] + input_ids
    input_ids[len(sent)+1] = sep_token_id
    attention_mask = [1] + attention_mask
    attention_mask[len(sent)+1] = 1
    return {"input_ids":input_ids,
            "attention_mask":attention_mask,
            "token_type_ids":token_type_ids}
```

ê·¸ë˜ì„œ `ner_tokenizer()` ë¼ëŠ” í•¨ìˆ˜ë¥¼ ìƒˆë¡œ ë§Œë“¤ì—ˆìŒ 

ì´ tokenizer ëŠ” ë§ˆì°¬ê°€ì§€ë¡œ max_seq_length ë¥¼ ë°›ì•„ì„œ max_seq_length ë§Œí¼ truncation ë„ í•˜ê³  padding ë„ ì œê³µí•¨

ì´ tokenizer ì˜ í•µì‹¬ì€ ì–´ì ˆì˜ ì¤‘ê°„ì— ë“¤ì–´ê°€ëŠ” ìŒì ˆë“¤ì€ ëª¨ë‘ '##' ìœ¼ë¡œ ì´ë£¨ì–´ì§„ prefix ë¥¼ ë¶™ì´ëŠ” ê²ƒì„

ì´ tokenizer ê°€ ë°˜í™˜í•˜ëŠ” ê²ƒì€ ê¸°ì¡´ì˜ tokenizer ê°€ ë°˜í™˜í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•˜ê²Œ ë§Œë“¤ì–´ì ¸ ìˆìŒ  

input_ids, attention_mask, token_type_ids ë¥¼ ë°˜í™˜í•¨

ì§ˆë¬¸ì´ í•˜ë‚˜ ìƒê²¼ì„ ìˆ˜ë„ ìˆëŠ”ë°ìš”!

ê¸°ì¡´ì˜ BERT ëŠ” WordPiece ë‹¨ìœ„ë¡œ ë˜ì–´ìˆê³  ì œê°€ ì„ì˜ë¡œ ë§ëŠ”ê±°ëŠ” ìŒì ˆë‹¨ìœ„ë¡œ ë§Œë“¤ì–´ì§€ê²Œ ë¨  
ì–´ì¨Œë“  ìŒì ˆë‹¨ìœ„ë¡œ ë§Œë“¤ì–´ì§„ê²ƒë„ ì „ë¶€ vocab id ê°€ ë¶€ì°©ì´ ë˜ì–´ì•¼ í• í…ë° WordPiece ë¡œ ë§Œë“¤ì–´ì§„ tokenizer ë¥¼ ì‚¬ìš©í•  ë•Œ 
ìŒì ˆ ë‹¨ìœ„ë¡œ ì˜ë¼ì§„ê²ƒì— ëŒ€í•´ì„œ ì „ë¶€ ë‹¤ out-of-vocabulary ê·¸ë‹ˆê¹Œ [UNK] í† í°ìœ¼ë¡œ ë³€í™˜ë˜ì§€ ì•Šì„ê¹Œ ë¼ëŠ” ê±±ì •ì„ í•  ìˆ˜ ìˆì„ê²ƒ
ê°™ì€ë° multi-lingual model ê°™ì€ ê²½ìš°ì—ëŠ” í•œêµ­ì–´ì˜ ëŒ€í•œ vocab ì´ 8,000ê°œ ë°–ì— ì¡´ì¬í•˜ì§€ ì•Šê³  ê·¸ ì•ˆì˜ ìˆëŠ” í•œêµ­ì–´ë“¤ì´ 
ê±°ì˜ë‹¤ ìŒì ˆë¡œë§Œ ë˜ì–´ìˆìŒ ê·¸ë˜ì„œ ìŒì ˆë‹¨ìœ„ tokenizer ë¥¼ ì ìš©í•´ë„ vocab id ê°€ ì–´ëŠ ì •ë„ ë‹¤ íšë“í•  ìˆ˜ ìˆê²Œ ë¨

í•˜ë‚˜ì˜ ìƒ˜í”Œì„ ë³´ì

```python
print(ner_tokenizer(train_texts[0], 5))
```

![]({{site.url}}/assets/images/ecca0a87.png)

[CLS] í† í°ê³¼ [SEP] í† í° ì˜ ë¶€ì°©ë˜ì–´ ìˆê³  ì•ˆì— ë¬¸ì¥ë“¤ ì˜ ë“¤ì–´ê°€ ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

train data ë¥¼ tokenizing í•˜ê³  tokenize ëœ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ë„£ëŠ” ê·¸ëŸ° ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ ë¨

```python
tokenized_train_sentences = []
tokenized_test_sentences = []
for text in train_texts:    # ì „ì²´ ë°ì´í„°ë¥¼ tokenizing í•©ë‹ˆë‹¤.
    tokenized_train_sentences.append(ner_tokenizer(text, 128))
for text in test_texts:
    tokenized_test_sentences.append(ner_tokenizer(text, 128))
```

train data ë¥¼ tokenize í•´ì„œ tokenized_train_sentence ì— ì €ì¥í•˜ê²Œ ë¨

tokenizer ê°™ì€ ê²½ìš°ëŠ” truncation ê³¼ padding ì´ ë“¤ì–´ê°€ê²Œ ëœë‹¤ê³  í–ˆëŠ”ë° ìš°ë¦¬ì˜ label ë°ì´í„°ë„ ë§ˆì°¬ê°€ì§€ë¡œ 
truncation ê³¼ padding ì´ í•„ìš”í•¨

ê¸°ì¡´ label ë°ì´í„°ì— [PAD] í† í° í˜¹ì€ truncation ëœ ì‚¬ì´ì¦ˆë§Œí¼ ê¸¸ì´ë¥¼ ë§ì¶°ì£¼ëŠ” ê³¼ì •ì„ `encode_tags()` ë¼ëŠ” í•¨ìˆ˜ë¥¼
í†µí•´ì„œ ì§„í–‰í•˜ê² ìŒ

```python
def encode_tags(tags, max_seq_length):
    # label ì—­ì‹œ ì…ë ¥ tokenê³¼ ê°œìˆ˜ë¥¼ ë§ì¶°ì¤ë‹ˆë‹¤ :-)
    tags = tags[:max_seq_length-2]
    labels = [tag2id[tag] for tag in tags]
    labels = [tag2id['O']] + labels

    padding_length = max_seq_length - len(labels)
    labels = labels + ([pad_token_label_id] * padding_length)

    return labels

encode_tags(train_tags[0], 5)
```

![]({{site.url}}/assets/images/c177288a.png)

ì´ë ‡ê²Œ í•˜ë©´ train data ì—ì„œ 5ê°œë¡œ truncation ì´ ì˜¬ë°”ë¥´ê²Œ ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

train labels ì—­ì‹œ `encode_tags()` í•¨ìˆ˜ë¥¼ í†µí•´ì„œ ì „ë¶€ ë§Œë“¤ì–´ ì¤Œ

```python
train_labels = []
test_labels = []

for tag in train_tags:
    train_labels.append(encode_tags(tag, 128))

for tag in test_tags:
    test_labels.append(encode_tags(tag, 128))

len(train_labels), len(test_labels)
```

ì´ë ‡ê²Œ í•˜ë©´ train data ë§Œë“¤ì–´ì¡Œê³  train lable ë§Œë“¤ì–´ ì¡ŒìŒ

ë‹¤ìŒìœ¼ë¡œëŠ” 

```python
import torch

# ì—¬ê¸° ë¶€í„°ëŠ” ì´ì œ ì§€ê²¨ì›Œì§€ì£ ? :-)
class TokenDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val) for key, val in self.encodings[idx].items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = TokenDataset(tokenized_train_sentences, train_labels)
test_dataset = TokenDataset(tokenized_test_sentences, test_labels)
```

dataset ì„ ë§Œë“¤ì–´ì£¼ê³  ì…ë ¥ì€ `__getitem__` ì— ë“¤ì–´ê°€ê²Œ ë˜ê³  label ì€ ì‚¬ì „ì— ì •ì˜ë¥¼ í•´ë‘” label ì´ ìˆœì°¨ì ìœ¼ë¡œ
ë“¤ì–´ê°€ê²Œ ë¨

ê¸°ì¡´ì˜ single sentence classification ê·¸ë¦¬ê³  ë‘ ë¬¸ì¥ê°„ ê´€ê³„ë¶„ë¥˜ ì´ëŸ° ì• ë“¤ì€ `BertForSentenceClassification` ëª¨ë¸ì„
ì‚¬ìš©í–ˆëŠ”ë° ì´ë²ˆì— í•´ì•¼í•˜ëŠ” ê²ƒì€ ê°ê°ì˜ token ìœ„ì— token ë§ˆë‹¤ classification layer ê°€ ë¶€ì°©ì´ ë˜ì„œ í•´ë‹¹ token ì´ ì–´ë–¤
label ê°’ì¸ì§€ ë¶„ë¥˜í•˜ëŠ” ê³¼ì •ì„ ì§„í–‰í•˜ê²Œ ë  ê²ƒì„

transformers ì—ì„œ ì—­ì‹œ `BertForTokenClassification` ì´ë¼ê³  í•˜ëŠ” class ë¥¼ ì œê³µí•´ì£¼ê³  ìˆìŒ

```python
from transformers import BertForTokenClassification, Trainer, TrainingArguments
import sys
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=5,              # total number of training epochs
    per_device_train_batch_size=8,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    logging_dir='./logs',            # directory for storing logs
    logging_steps=100,
    learning_rate=3e-5,
    save_total_limit=5
)
```

train arguments ë™ì¼í•˜ê²Œ ì±„ì›Œì£¼ê³  

ë§ˆì°¬ê°€ì§€ë¡œ ëª¨ë¸ì˜ initialize ë¥¼ `BertForTokenClassification` ìœ¼ë¡œ í•´ì£¼ê²Œ ë˜ê³  ì¤‘ìš”í•œê²Œ í•˜ë‚˜ê°€ ë” ìˆìŒ

```python
model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(unique_tags))
model.to(device)

trainer = Trainer(
    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=test_dataset            # evaluation dataset
)
```

ê¸°ì¡´ì—ëŠ” MODEL_NAME íŒŒë¼ë¯¸í„° í•˜ë‚˜ë§Œ ë„£ê³  ë¶„ë¥˜ë¡œ ë„˜ì–´ê°”ì—ˆëŠ”ë° ì–˜ëŠ” num_labels ì—ì„œ unique_tags(ìš°ë¦¬ê°€ êµ¬ë¶„í•´ì•¼í•˜ëŠ” label)ì´ 
ëª‡ê°œì¸ì§€ë¥¼ ëª…í™•í•˜ê²Œ ì§€ì •í•´ì¤˜ì•¼ í•¨  

```python
trainer.train() # 1 epochì— ëŒ€ëµ 5ë¶„ ì •ë„ ê±¸ë¦½ë‹ˆë‹¤.
```

í•™ìŠµì´ ì™„ë£Œë˜ë©´ ì‹¤ì œ NER inference ë¥¼ í•´ë³´ë„ë¡ í•˜ì

ì—­ì‹œ ì¤‘ìš”í•œ ê²ƒì€ ê¸°ì¡´ì— ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ë˜ ê²ƒì€ WordPiece tokenizer ë¥¼ ì‚¬ìš©í–ˆì§€ë§Œ ì´ë²ˆì— ìš°ë¦¬ëŠ” ìŒì ˆ tokenizer ë¥¼ ì‚¬ìš©í–ˆìŒ  
ê·¸ë˜ì„œ ì…ë ¥ëœ ë¬¸ì¥ì— ëŒ€í•´ì„œë„ ë°˜ë“œì‹œ ìŒì ˆ tokenizer ë¥¼ ê±°ì¹œí›„ì— ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ì•¼ í•¨  

```python
def ner_inference(text) : 
  
    model.eval()
    text = text.replace(' ', '_')

    predictions , true_labels = [], []
    
    tokenized_sent = ner_tokenizer(text, len(text)+2)
    input_ids = torch.tensor(tokenized_sent['input_ids']).unsqueeze(0).to(device)
    attention_mask = torch.tensor(tokenized_sent['attention_mask']).unsqueeze(0).to(device)
    token_type_ids = torch.tensor(tokenized_sent['token_type_ids']).unsqueeze(0).to(device)    
    
    with torch.no_grad():
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids)
        
    logits = outputs['logits']
    logits = logits.detach().cpu().numpy()
    label_ids = token_type_ids.cpu().numpy()

    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
    true_labels.append(label_ids)

    pred_tags = [list(tag2id.keys())[p_i] for p in predictions for p_i in p]

    print('{}\t{}'.format("TOKEN", "TAG"))
    print("===========")
    # for token, tag in zip(tokenizer.decode(tokenized_sent['input_ids']), pred_tags):
    #   print("{:^5}\t{:^5}".format(token, tag))
    for i, tag in enumerate(pred_tags):
        print("{:^5}\t{:^5}".format(tokenizer.convert_ids_to_tokens(tokenized_sent['input_ids'][i]), tag))
```

ê·¸ë˜ì„œ `text` ê°€ ì…ë ¥ì´ ë˜ì—ˆì„ ë•Œ ì‚¬ì „ì— ë§Œë“¤ì–´ ë’€ë˜ `ner_tokenizer` ë¥¼ ê±°ì³ì„œ tokenized_sent ë¥¼ ë§Œë“¤ê²Œ ë˜ê³  
ì´ê²ƒì„ input_ids, attention_mask, token_type_ids value ë¡œ ê°€ì ¸ì˜¤ê²Œ ë˜ê³  ì´ê²ƒì„ ëª¨ë¸ì— ê·¸ëŒ€ë¡œ ì…ë ¥ìœ¼ë¡œ ë„£ê²Œ ë¨

ê°ê°ì˜ token ì— ëŒ€í•´ì„œ softmax ê°€ ìµœëŒ€ë¡œ ë˜ëŠ” ê°’ì´ ë¬´ì—‡ì¸ì§€ ê·¸ ê°’ì„ ê°€ì ¸ì™€ì„œ token ê²°ê³¼ë¥¼ ë°˜í™˜í•  ìˆ˜ ìˆìŒ

ì‹¤ì œ ì˜ˆì œë¥¼ í™•ì¸í•´ ë³´ì

```python
text = 'ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.'

ner_inference(text)
```

![]({{site.url}}/assets/images/4a93d0bd.png)

```python
text = 'ë¡œìŠ¤íŠ¸ì•„í¬ëŠ” ìŠ¤ë§ˆì¼ê²Œì´íŠ¸ RPGê°€ ê°œë°œí•œ ì¿¼í„°ë·° ì•¡ì…˜ MMORPG ê²Œì„ì´ë‹¤.'
ner_inference(text)
```

![]({{site.url}}/assets/images/904d0409.png)

```python
text = '2014ë…„ 11ì›” 12ì¼ ìµœì´ˆ ê³µê°œí–ˆìœ¼ë©° 2018ë…„ 11ì›” 7ì¼ë¶€í„° ì˜¤í”ˆ ë² íƒ€ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•˜ë‹¤ 2019ë…„ 12ì›” 4ì¼ ì •ì‹ ì˜¤í”ˆí–ˆë‹¤.'
ner_inference(text)
```

![]({{site.url}}/assets/images/4037fccf.png)
![]({{site.url}}/assets/images/eb481f16.png)

ê°œì²´ëª… ì¸ì‹ ê³¼ì •ì„ í†µí•´ì„œ ë¬¸ì¥ì— ëŒ€í•œ token ë¶„ë¥˜ task ë¥¼ í•™ìŠµì„ ì‹¤ìŠµí•´ë´¤ìŒ

ë‹¤ìŒì—” ë³¸ë¬¸ ë‚´ì—ì„œ ì •ë‹µ token ì´ ì–´ë””ì— ì¡´ì¬í•˜ëŠ”ì§€ë¥¼ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ê¸°ê³„ë…í•´ task ë¥¼ ì‹¤ìŠµí•´ë³´ì

---

# ì‹¤ìŠµ

## ê¸°ê³„ë…í•´ ëª¨ë¸ í•™ìŠµ

```python
import torch
import sys

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
```

ì´ë²ˆì— ì‚¬ìš©í•  dataset ì€ KorQUaD ë¼ëŠ” ë°ì´í„° ì„

```
!mkdir dataset

!wget https://korquad.github.io/dataset/KorQuAD_v1.0_train.json
!wget https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json
```

í•œêµ­ì–´ ê¸°ê³„ë…í•´ ë°ì´í„°ì…‹ì„

```
!mv /content/KorQuAD_v1.0_train.json dataset
!mv /content/KorQuAD_v1.0_dev.json dataset
```

ì´ ë°ì´í„°ì…‹ì€ json í˜•íƒœë¡œ ë˜ì–´ìˆìŒ  
í•™ìŠµí•˜ê¸°ì— í¸í•œ í˜•íƒœë¡œ ê°€ì ¸ì˜¤ë„ë¡ í•˜ì

```python
# ë°ì´í„°ì…‹ì„ íŒŒì‹±í•´ì˜¤ê² ìŠµë‹ˆë‹¤ :-)

import json
from pathlib import Path

def read_squad(path):
    path = Path(path)
    with open(path, 'rb') as f:
        squad_dict = json.load(f)

    contexts = []
    questions = []
    answers = []
    for group in squad_dict['data']:
        for passage in group['paragraphs']:
            context = passage['context']
            for qa in passage['qas']:
                question = qa['question']
                for answer in qa['answers']:
                    contexts.append(context)
                    questions.append(question)
                    answers.append(answer)

    return contexts, questions, answers


train_contexts, train_questions, train_answers = read_squad('dataset/KorQuAD_v1.0_train.json')
val_contexts, val_questions, val_answers = read_squad('dataset/KorQuAD_v1.0_dev.json')

```

ì´ ê³¼ì •ì„ ë°ì´í„° íŒŒì‹±í•œë‹¤ê³  ì–˜ê¸°í•¨
íŒŒì‹±ì„ í†µí•´ì„œ ë°ì´í„° ì•ˆì— ì¡´ì¬í•˜ëŠ” paragraph ì™€ question ê·¸ë¦¬ê³  ì •ë‹µ ê´€ë ¨ëœ answer ë°ì´í„° ì´ëŸ° ìš”ì†Œë“¤ì„ ê°€ì ¸ì˜¤ëŠ” ê³¼ì •ì„ 
ê±°ì¹¨

```python
print(train_contexts[52471])
print(len(train_contexts[52471]))
```

![]({{site.url}}/assets/images/48f10214.png)

ì´ë ‡ê²Œ í•˜ë©´ train_contexts ì— ë³¸ë¬¸ì— ëŒ€í•œ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

```python
print(train_questions[52471])
```

![]({{site.url}}/assets/images/aa1440f8.png)

```python
print(train_answers[52471]) # ë³¸ë¬¸ ë‚´ì—ì„œ ì •ë‹µì´ ì‹œì‘ë˜ëŠ” ìŒì ˆ ìˆœì„œê°€ 'answer start'ì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
```

![]({{site.url}}/assets/images/b48f78ca.png)

ì§ˆë¬¸ê³¼ ë‹µë³€ê¹Œì§€ í™•ì¸ í•  ìˆ˜ ìˆìŒ

`answer_start` ë¼ëŠ”ê²Œ ë³´ì´ëŠ”ë° ë³¸ë¬¸ ë‚´ì—ì„œ ì •ë‹µì´ ì‹œì‘í•˜ëŠ” ìŒì ˆ ìˆœì„œë¥¼ ì˜ë¯¸í•¨  
ì´ ê²½ìš°ì—” ë³¸ë¬¸ ë‚´ì—ì„œ 568 ë²ˆì§¸ ìŒì ˆì— answer ê°€ ì‹œì‘í•œë‹¤ ë¼ëŠ” ê²ƒì„ ì˜ë¯¸
ì •ë‹µì˜ `text` ì˜ ê¸¸ì´ë¥¼ ì‚¬ìš©í•˜ë©´ end ì˜ ìœ„ì¹˜ë¥¼ ìë™ì ìœ¼ë¡œ ì•Œ ìˆ˜ ìˆìŒ

ëª¨ë¸ì„ ìœ„í•´ í•™ìŠµ ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ ì¤˜ì•¼ í•¨

```python
def add_end_idx(answers, contexts):
    for answer, context in zip(answers, contexts):
        # ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ ì •ë‹µ ë°ì´í„°ë¥¼ ë§Œë“¤ê² ìŠµë‹ˆë‹¤.
        # ì •ë‹µ ë°ì´í„°ëŠ” startìŒì ˆê³¼ end ìŒì ˆë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
        # ëª¨ë¸ì€ ì „ì²´ í† í° ì¤‘ì—ì„œ start tokenê³¼ end tokenì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.
        gold_text = answer['text']
        start_idx = answer['answer_start']
        end_idx = start_idx + len(gold_text)
        

        # sometimes squad answers are off by a character or two â€“ fix this
        # ì‹¤ì œ ë³¸ë¬¸ì—ì„œ í•´ë‹¹ ìŒì ˆ ë²ˆí˜¸ë¡œ ì˜ë¼ëƒˆì„ ë•Œ, ì •ë‹µê³¼ ê°™ì€ì§€ ê²€ì‚¬í•´ì„œ start, endë¥¼ ë³´ì •í•©ë‹ˆë‹¤ :-)
        # 'ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤' -> 'ì´ìˆœì‹ ' -> start: 0, end: 4
        if context[start_idx:end_idx] == gold_text:
            answer['answer_end'] = end_idx
        elif context[start_idx-1:end_idx-1] == gold_text:
            answer['answer_start'] = start_idx - 1
            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character
        elif context[start_idx-2:end_idx-2] == gold_text:
            answer['answer_start'] = start_idx - 2
            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters
    return answers

train_answers = add_end_idx(train_answers, train_contexts)
val_answers = add_end_idx(val_answers, val_contexts)
```

ëª¨ë¸ì€ answer start position ê·¸ë¦¬ê³  answer end position ì´ 2ê°œë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ê²Œ ë¨

```python
from transformers import AutoModel, AutoTokenizer, BertTokenizer
MODEL_NAME = "bert-base-multilingual-cased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)
val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)
```

ëª¨ë¸ì€ ì•ì˜ ì‹¤ìŠµê³¼ ë™ì¼í•˜ê²Œ ì‚¬ìš©í•¨

```python
def add_token_positions(encodings, answers):
    start_positions = []
    end_positions = []
    # ì´ì œ ìŒì ˆ indexë¥¼ token indexì™€ mappingí•˜ëŠ” ì‘ì—…ì„ í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤ :-)
    for i in range(len(answers)):
        # tokenizerì˜ char_to_token í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë©´ ìŒì ˆ ìˆ«ìë¥¼ token indexë¡œ ë°”ê¿”ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))
        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))
        # ì•„ë˜ ë¶€ë¶„ì€ truncationì„ ìœ„í•œ ê³¼ì •ì…ë‹ˆë‹¤.
        # if start position is None, the answer passage has been truncated
        if start_positions[-1] is None:
            start_positions[-1] = tokenizer.model_max_length

        # if end position is None, the 'char_to_token' function points to the space before the correct token - > add + 1
        if end_positions[-1] is None:
            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] + 1)

        # ì¶”ê°€ëœ ì˜ˆì™¸ ì²˜ë¦¬, ì˜ˆë¥¼ë“¤ì–´ì„œ tokenizerì™€ model inputì˜ max_lengthê°€ 512ì¸ë°, startì™€ end positionì´ 600ê³¼ 610 ì´ë©´ ë‘˜ë‹¤ max_lengthë¡œ ë³€ê²½í•´ì•¼í•¨.
        # ì–´ì°¨í”¼ max_lengthê°€ 512ì¸ ëª¨ë¸ì€ ì •ë‹µì„ ë³¼ ìˆ˜ ì—†ìŒ.
        if start_positions[-1] is None or start_positions[-1] > tokenizer.model_max_length:
            start_positions[-1] = tokenizer.model_max_length
        
        if end_positions[-1] is None or end_positions[-1] > tokenizer.model_max_length:
            end_positions[-1] = tokenizer.model_max_length

    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})
    return encodings

train_encodings = add_token_positions(train_encodings, train_answers)
val_encodings = add_token_positions(val_encodings, val_answers)
```

ì´ ë¶€ë¶„ì´ ê¸°ê³„ë…í•´ì—ì„œ í•µì‹¬ì¸ ë¶€ë¶„ì´ ë  ìˆ˜ ìˆìŒ

ìš°ë¦¬ê°€ êµ¬ì¶•í•œ ë°ì´í„°ëŠ” ìŒì ˆ ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ê²Œ ë¨

í•˜ì§€ë§Œ BERT ëª¨ë¸ì€ WordPiece ë‹¨ìœ„ë¡œ ë˜ì–´ìˆìŒ  
ê·¸ë ‡ê¸° ë•Œë¬¸ì— ìŒì ˆë‹¨ìœ„ì— ìˆëŠ” ìˆ«ìë¥¼ token index ë¡œ ë°”ê¿”ì¤˜ì•¼ í•¨  
ê·¸ë˜ì•¼ì§€ í•´ë‹¹ token index ê°€ ì •ë‹µì—ì„œ ì •ë‹µì˜ ì‹œì‘ì´ë‹¤ ë¼ëŠ” label ì„ í•™ìŠµí•  ìˆ˜ ìˆìŒ

ì´ ë•Œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜ê°€ `char_to_token()` ì„ ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë©´ ìŒì ˆ ìˆ«ìë¥¼ token index ë¡œ ë°”ê¿”ì¤„ ìˆ˜ ìˆìŒ

ì˜ˆì™¸ì²˜ë¦¬ ë¶€ë¶„ë„ ìˆìŒ  
BERT ëª¨ë¸ì€ 512 í† í°ì´ maximum ì¸ë° ë§Œì•½ ì •ë‹µì´ 512 í† í°ì„ ë„˜ì–´ì„œëŠ” ìœ„ì¹˜ì— ì¡´ì¬í•œë‹¤ë©´ ê·¸ê±°ëŠ” í•™ìŠµì„ í•˜ë‚˜ë§ˆë‚˜ê°€ ë¨  
ê·¸ë˜ì„œ ê·¸ ë¶€ë¶„ì— ëŒ€í•œ ì˜ˆì™¸ì²˜ë¦¬ë¥¼ ì¶”ê°€í–ˆìŒ

í•™ìŠµë°ì´ ìì²´ëŠ” start_position ì— ëŒ€í•œ ì •ë³´ì™€ end_position ì— ëŒ€í•˜ ì •ë³´ë¥¼ í† í° ë‹¨ìœ„ë¡œ ê°€ì§€ê²Œ ë¨

ì´ë ‡ê²Œ ë°ì´í„°ë¥¼ ë§Œë“¤ê³ ë‚˜ë©´ dataset ì„ ë§Œë“¤ê³ 

```python
import torch

class SquadDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}

    def __len__(self):
        return len(self.encodings.input_ids)

train_dataset = SquadDataset(train_encodings)
val_dataset = SquadDataset(val_encodings)
```

ì´ ë•Œ ë°˜í™˜í•˜ëŠ” ê°’ì´ ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ë˜ tokenized ëœ ê²°ê³¼ì„ ê·¸ë¦¬ê³  ì—¬ê¸°ì— ì •ë‹µì— ëŒ€í•œ token index ì •ë³´ë¥¼ í•¨ê»˜
ë°˜í™˜í•´ì„œ label ë¡œ ì‚¬ìš©í•˜ê²Œ ë¨

```python
from transformers import BertForQuestionAnswering
model = BertForQuestionAnswering.from_pretrained(MODEL_NAME)
```

ì—­ì‹œ huggingface ì—ì„œ ê¸°ê³„ë…í•´ë¥¼ ìœ„í•œ `BertForQuestionAnswering` ë¥¼ ì œê³µí•˜ê³  ìˆìŒ

```python
from transformers import BertForQuestionAnswering, Trainer, TrainingArguments
import sys
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=1,              # total number of training epochs
    per_device_train_batch_size=8,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    logging_dir='./logs',            # directory for storing logs
    logging_steps=100,
    learning_rate=3e-5,
    save_total_limit=5
)
```

```python
model = BertForQuestionAnswering.from_pretrained(MODEL_NAME)
model.to(device)

trainer = Trainer(
    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset            # evaluation dataset
)
```

ê¸°ê³„ë…í•´ë¥¼ ìœ„í•œ ëª¨ë¸ ì™„ì„±!

```python
trainer.train() # 1 epochì— ëŒ€ëµ 1ì‹œê°„ ì •ë„ ê±¸ë¦½ë‹ˆë‹¤.
```

ëª¨ë¸ì´ í•™ìŠµë˜ê³  ë‚˜ë©´ ëª¨ë¸ì„ ì €ì¥í•´ì¤Œ

```python
trainer.save_model(".")
```

pipeline ì„ í™œìš©í•´ì„œ inference ë¥¼ í•´ë³´ì

```python
from transformers import pipeline

nlp = pipeline("question-answering", model=model, tokenizer=tokenizer, device=0)

context = r"""
ì´ìˆœì‹ (æèˆœè‡£, 1545ë…„ 4ì›” 28ì¼ ~ 1598ë…„ 12ì›” 16ì¼ (ìŒë ¥ 11ì›” 19ì¼))ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ì—ˆë‹¤.
ë³¸ê´€ì€ ë•ìˆ˜(å¾·æ°´), ìëŠ” ì—¬í•´(æ±è«§), ì‹œí˜¸ëŠ” ì¶©ë¬´(å¿ æ­¦)ì˜€ìœ¼ë©°, í•œì„± ì¶œì‹ ì´ì—ˆë‹¤.
ë¬¸ë°˜ ê°€ë¬¸ ì¶œì‹ ìœ¼ë¡œ 1576ë…„(ì„ ì¡° 9ë…„) ë¬´ê³¼(æ­¦ç§‘)ì— ê¸‰ì œí•˜ì—¬ ê·¸ ê´€ì§ì´ ë™êµ¬ë¹„ë³´ ê¶Œê´€, í›ˆë ¨ì› ë´‰ì‚¬, ë°œí¬ì§„ ìˆ˜êµ°ë§Œí˜¸, ì¡°ì‚°ë³´ ë§Œí˜¸, ì „ë¼ì¢Œë„ ìˆ˜êµ°ì ˆë„ì‚¬ë¥¼ ê±°ì³ ì •í—ŒëŒ€ë¶€ ì‚¼ë„ìˆ˜êµ°í†µì œì‚¬ì— ì´ë¥´ë €ë‹¤.
"""

print(nlp(question="ì´ìˆœì‹ ì´ íƒœì–´ë‚œ ë‚ ì§œëŠ”?", context=context))
print(nlp(question="ì´ìˆœì‹ ì˜ ë³¸ê´€ì€?", context=context))
print(nlp(question="ì´ìˆœì‹ ì˜ ì‹œí˜¸ëŠ”?", context=context))
print(nlp(question="ì´ìˆœì‹ ì˜ ê³ í–¥ì€?", context=context))
print(nlp(question="ì´ìˆœì‹ ì˜ ë§ˆì§€ë§‰ ì§ì±…ì€?", context=context))
```

![]({{site.url}}/assets/images/9cbb9654.png)

pipeline ì— "question-answering" ì„ ë„£ì–´ì„œ ì‚¬ìš©


> ì‘ì„±ì      
```
* ê¹€ì„±í˜„ (bananaband657@gmail.com)  
1ê¸° ë©˜í† 
ê¹€ë°”ë‹¤ (qkek983@gmail.com)
ë°•ìƒí¬ (parksanghee0103@gmail.com)  
ì´ì •ìš° (jungwoo.l2.rs@gmail.com)
2ê¸° ë©˜í† 
ë°•ìƒí¬ (parksanghee0103@gmail.com)  
ì´ì •ìš° (jungwoo.l2.rs@gmail.com)
ì´ë…•ìš° (leenw2@gmail.com)
ë°•ì±„í›ˆ (qkrcogns2222@gmail.com)
```
[CC BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/2.0/kr/)
