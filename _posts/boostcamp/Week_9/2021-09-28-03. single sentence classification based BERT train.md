---
title: "Day_38 03. BERT ê¸°ë°˜ ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ"

categories:
  - Boostcamp_AI_Tech/Week_9
tags:
  - KLUE
---
  
# BERT Pre-Training

## 1. BERT ì–¸ì–´ëª¨ë¸ ê¸°ë°˜ì˜ ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜

### 1.1 KLUE ë°ì´í„°ì…‹ ì†Œê°œ

**í•œêµ­ì–´ ìì—°ì–´ ì´í•´ ë²¤ì¹˜ë§ˆí¬(Korean Language Understanding Evaluation, KLUE)**

![]({{site.url}}/assets/images/30403957.png)

![]({{site.url}}/assets/images/3603361b.png)

![]({{site.url}}/assets/images/e6d33bf2.png)

![]({{site.url}}/assets/images/88c82bda.png)

![]({{site.url}}/assets/images/f5dfa21f.png)

![]({{site.url}}/assets/images/b36f79a6.png)

![]({{site.url}}/assets/images/ad358430.png)

### 1.2 ì˜ì¡´ êµ¬ë¬¸ ë¶„ì„

**ë‹¨ì–´ë“¤ ì‚¬ì´ì— ê´€ê³„ë¥¼ ë¶„ì„í•˜ëŠ” task**

![]({{site.url}}/assets/images/f518e5e7.png)

1. íŠ¹ì§•
   - ì§€ë°°ì†Œ: ì˜ë¯¸ì˜ ì¤‘ì‹¬ì´ ë˜ëŠ” ìš”ì†Œ
   - ì˜ì¡´ì†Œ: ì§€ë°°ì†Œê°€ ê°–ëŠ” ì˜ë¯¸ë¥¼ ë³´ì™„í•´ì£¼ëŠ” ìš”ì†Œ(ìˆ˜ì‹)
   - ì–´ìˆœê³¼ ìƒëµì´ ììœ ë¡œìš´ í•œêµ­ì–´ì™€ ê°™ì€ ì–¸ì–´ì—ì„œ ì£¼ë¡œ ì—°êµ¬ëœë‹¤.
2. ë¶„ë¥˜ ê·œì¹™
   - ì§€ë°°ì†ŒëŠ” í›„ìœ„ì–¸ì–´ì´ë‹¤. ì¦‰ ì§€ë°°ì†ŒëŠ” í•­ìƒ ì˜ì¡´ì†Œë³´ë‹¤ ë’¤ì— ìœ„ì¹˜í•œë‹¤.
   - ê° ì˜ì¡´ì†Œì˜ ì§€ë°°ì†ŒëŠ” í•˜ë‚˜ì´ë‹¤.
   - êµì°¨ ì˜ì¡´ êµ¬ì¡°ëŠ” ì—†ë‹¤.
3. ë¶„ë¥˜ ë°©ë²•
   - Sequence labeling ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ë‚˜ëˆˆë‹¤.
   - ì• ì–´ì ˆì— ì˜ì¡´ì†Œê°€ ì—†ê³  ë‹¤ìŒ ì–´ì ˆì´ ì§€ë°°ì†Œì¸ ì–´ì ˆì„ ì‚­ì œí•˜ë©° ì˜ì¡´ ê´€ê³„ë¥¼ ë§Œë“ ë‹¤.

![]({{site.url}}/assets/images/2eaeb019.png)

**ì–´ë”° ì¨ìš”?**

![]({{site.url}}/assets/images/c28e4711.png)

- ë³µì¡í•œ **ìì—°ì–´** í˜•íƒœë¥¼ ê·¸ë˜í”„ë¡œ **êµ¬ì¡°í™”**í•´ì„œ í‘œí˜„ ê°€ëŠ¥! ê° **ëŒ€ìƒ**ì— ëŒ€í•œ ì •ë³´ ì¶”ì¶œì´ ê°€ëŠ¥!

êµ¬ë¦„ê·¸ë¦¼ $\rightarrow$ ìƒˆí„¸êµ¬ë¦„ì„ ê·¸ë¦° ê²ƒ $\rightarrow$ ë‚´ê°€ ê·¸ë¦° ê²ƒ

"ë‚˜"ëŠ” "êµ¬ë¦„ê·¸ë¦¼" ì„ ê·¸ë ¸ë‹¤.

"êµ¬ë¦„ ê·¸ë¦¼"ì€ "ìƒˆí„¸êµ¬ë¦„"ì„ ê·¸ë¦° ê²ƒì´ë‹¤.

## 2. ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ task ì†Œê°œ

### 2.1 ë¬¸ì¥ ë¶„ë¥˜ task

**ì£¼ì–´ì§„ ë¬¸ì¥ì´ ì–´ë–¤ ì¢…ë¥˜ì˜ ë²”ì£¼ì— ì†í•˜ëŠ”ì§€ë¥¼ êµ¬ë¶„í•˜ëŠ” task**

1. ê°ì •ë¶„ì„(Sentiment Analysis)
   - ë¬¸ì¥ì˜ ê¸ì • ë˜ëŠ” ë¶€ì • ë° ì¤‘ë¦½ ë“± ì„±í–¥ì„ ë¶„ë¥˜í•˜ëŠ” í”„ë¡œì„¸ìŠ¤
   - ë¬¸ì¥ì„ ì‘ì„±í•œ ì‚¬ëŒì˜ ëŠë‚Œ, ê°ì • ë“±ì„ ë¶„ì„í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê¸°ì—…ì—ì„œ ëª¨ë‹ˆí„°ë§, ê³ ê°ì§€ì›, ë˜ëŠ” ëŒ€ìŠ¥ã„¹ì— ëŒ€í•œ í•„í„°ë§ ë“±ì„
   ìë™í™”í•˜ëŠ” ì‘ì—…ì— ì£¼ë¡œ ì‚¬ìš©
   - í™œìš© ë°©ì•ˆ
     - í˜ì˜¤ ë°œì–¸ ë¶„ë¥˜ : ëŒ“ê¸€, ê²Œì„ ëŒ€í™” ë“± í˜ì˜¤ ë°œì–¸ì„ ë¶„ë¥˜í•˜ì—¬ ì¡°ì¹˜ë¥¼ ì·¨í•˜ëŠ” ìš©ë„ë¡œ í™œìš©
     - ê¸°ì—… ëª¨ë‹ˆí„°ë§ : ì†Œì…œ, ë¦¬ë·° ë“± ë°ì´í„°ì— ëŒ€í•´ ê¸°ì—… ì´ë¯¸ì§€, ë¸Œëœë“œ ì„ í˜¸ë„, ì œí’ˆí‰ê°€ ë“± ê¸ì • ë˜ëŠ” ë¶€ì •ì  ìš”ì¸ì„ ë¶„ì„
2. ì£¼ì œ ë¼ë²¨ë§(Topic Labeling)
   - ë¬¸ì¥ì˜ ë‚´ìš©ì„ ì´í•´í•˜ê³  ì ì ˆí•œ ë²”ì£¼ë¥¼ ë¶„ë¥˜í•˜ëŠ” í”„ë¡œì„¸ìŠ¤
   - ì£¼ì œë³„ë¡œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ êµ¬ì„±í•˜ëŠ” ë“± ë°ì´í„° êµ¬ì¡°í™”ì™€ êµ¬ì„±ì— ìš©ì´
   - í™œìš© ë°©ì•ˆ
     - ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ë¶„ë¥˜ : ëŒ€ìš©ëŸ‰ì˜ ë¬¸ì„œë¥¼ ë²”ì£¼í™”
     - VoC(Voice of Customer) : ê³ ê°ì˜ í”¼ë“œë°±ì„ ì œí’ˆ ê°€ê²©, ê°œì„ ì , ë””ìì¸ ë“± ì ì ˆí•œ ì£¼ì œë¡œ ë¶„ë¥˜í•˜ì—¬ ë°ì´í„°ë¥¼ êµ¬ì¡°í™”
3. ì–¸ì–´ê°ì§€(Language Detection)
   - ë¬¸ì¥ì´ ì–´ë–¤ ë‚˜ë¼ ì–¸ì–´ì¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” í”„ë¡œì„¸ìŠ¤
   - ì£¼ë¡œ ë²ˆì—­ê¸°ì—ì„œ ì •í™•í•œ ë²ˆì—­ì„ ìœ„í•´ ì…ë ¥ ë¬¸ì¥ì´ ì–´ë–¤ ë‚˜ë¼ì˜ ì–¸ì–´ì¸ì§€ íƒ€ê²ŸíŒ… í•˜ëŠ” ì‘ì—…ì´ ê°€ëŠ¥
   - í™œìš© ë°©ì•ˆ
     - ë²ˆì—­ê¸° : ë²ˆì—­í•  ë¬¸ì¥ì— ëŒ€í•´ ì ì ˆí•œ ì–¸ì–´ë¥¼ ê°ì§€í•¨
     - ë°ì´í„° í•„í„°ë§ : íƒ€ê²Ÿ ì–¸ì–´ ì´ì™¸ ë°ì´í„°ëŠ” í•„í„°ë§
4. ì˜ë„ ë¶„ë¥˜(Intent Classification)
   - ë¬¸ì¥ì´ ê°€ì§„ ì˜ë„ë¥¼ ë¶„ë¥˜í•˜ëŠ” í”„ë¡œì„¸ìŠ¤
   - ì…ë ¥ ë¬¸ì¥ì´ ì§ˆë¬¸, ë¶ˆë§Œ, ëª…ë ¹ ë“± ë‹¤ì–‘í•œ ì˜ë„ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì ì ˆí•œ í”¼ë“œë°±ì„ ì¤„ ìˆ˜ ìˆëŠ” ê³³ìœ¼ë¡œ ë¼ìš°íŒ… ì‘ì—…ì´ ê°€ëŠ¥
   - í™œìš© ë°©ì•ˆ
     - ì±—ë´‡ : ë¬¸ì¥ì˜ ì˜ë„ì¸ ì§ˆë¬¸, ëª…ë ¹, ê±°ì ˆ ë“±ì„ ë¶„ì„í•˜ê³  ì ì ˆí•œ ë‹µë³€ì„ ì£¼ê¸° ìœ„í•´ í™œìš©

### 2.2 ë¬¸ì¥ ë¶„ë¥˜ë¥¼ ìœ„í•œ ë°ì´í„°

**Kor_hate**

- í˜ì˜¤ í‘œí˜„ì— ëŒ€í•œ ë°ì´í„°
- íŠ¹ì • ê°œì¸ ë˜ëŠ” ì§‘ë‹¨ì— ëŒ€í•œ ê³µê²©ì  ë¬¸ì¥
- ë¬´ë¡€, ê³µê²©ì ì´ê±°ë‚˜ ë¹„ê¼¬ëŠ” ë¬¸ì¥
- ë¶€ì •ì ì´ì§€ ì•Šì€ ë¬¸ì¥

![]({{site.url}}/assets/images/ec5d1ca7.png)

**Kor_sarcasm**

- ë¹„ê¼¬ì§€ ì•Šì€ í‘œí˜„ì˜ ë¬¸ì¥
- ë¹„ê¼¬ëŠ” í‘œí˜„ì˜ ë¬¸ì¥

![]({{site.url}}/assets/images/bd9d325f.png)

**Kor_sae**

- ì˜ˆ/ì•„ë‹ˆì˜¤ë¡œ ë‹µë³€ ê°€ëŠ¥í•œ ì§ˆë¬¸
- ëŒ€ì•ˆ ì„ íƒì„ ë¬»ëŠ” ì§ˆë¬¸
- Wh- ì§ˆë¬¸ (who, what, where, when, why, how)
- ê¸ˆì§€ ëª…ë ¹
- ìš”êµ¬ ëª…ë ¹
- ê°•í•œ ìš”êµ¬ ëª…ë ¹

![]({{site.url}}/assets/images/0636c1ef.png)

**Kor_3i4k**

- ë‹¨ì–´ ë˜ëŠ” ë¬¸ì¥ ì¡°ê°
- í‰ì„œë¬¸
- ì§ˆë¬¸
- ëª…ë ¹ë¬¸
- ìˆ˜ì‚¬ì  ì§ˆë¬¸
- ìˆ˜ì‚¬ì  ëª…ë ¹ë¬¸
- ì–µì•½ì— ì˜ì¡´í•˜ëŠ” ì˜ë„

![]({{site.url}}/assets/images/7c0b58eb.png)

## 3. ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ

### 3.1 ëª¨ë¸ êµ¬ì¡°ë„

**BERT ì˜ [CLS] token ì˜ vector ë¥¼ classification í•˜ëŠ” Dense layer ì‚¬ìš©

![]({{site.url}}/assets/images/47555ff5.png)

**ì£¼ìš” ë§¤ê°œë³€ìˆ˜**

- input_idx : sequence token ì„ ì…ë ¥
- attention_mask : [0, 1] ë¡œ êµ¬ì„±ëœ ë§ˆìŠ¤í¬ì´ë©° íŒ¨ë”© í† í°ì„ êµ¬ë¶„
- token_type_idx : [0, 1] ë¡œ êµ¬ì„±ë˜ì—ˆìœ¼ë©° ì…ë ¥ì˜ ì²« ë¬¸ì¥ê³¼ ë‘ë²ˆì§¸ ë¬¸ì¥ êµ¬ë¶„
- position_ids : ê° ì…ë ¥ ì”¨í€€ìŠ¤ì˜ ì„ë² ë”© ì¸ë±ìŠ¤
- inputs_embeds : input_ids ëŒ€ì‹  ì§ì ‘ ì„ë² ë”© í‘œí˜„ì„ í• ë‹¹
- labels : loss ê³„ì‚°ì„ ìœ„í•œ ë ˆì´ë¸”
- Next_sentence_label : ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡ loss ê³„ì‚°ì„ ìœ„í•œ ë ˆì´ë¸”

### 3.2 í•™ìŠµ ê³¼ì •

![]({{site.url}}/assets/images/d462108b.png)

---

# ì‹¤ìŠµ

## BERT ë¥¼ í™œìš©í•œ ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ ì‹¤ìŠµ

datasets ë¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì¶œì‹œí•¨  
ì–´ë–¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ëƒë©´?  
ëª¨ë“  dataset ë“¤ì„ ë‹¤ ê·¸ë“¤ì˜ hub ì— ì €ì¥ì„ í•´ë‘   
ë§ˆì¹˜ ìš°ë¦¬ê°€ bert base model ì„ ë¶ˆëŸ¬ì˜¨ ê²ƒ ì²˜ëŸ¼ dataset ì˜ ì´ë¦„ë§Œ ë„£ìœ¼ë©´ ë‚´ê°€ ì›í•˜ëŠ” value ì•ˆì— dataset ì´ ë“¤ì–´ì˜¤ê²Œ ë¨

```python
import torch
import datasets
import sys

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

# ì‚¬ìš©ê°€ëŠ¥í•œ dataset list ë¶ˆëŸ¬ì˜¤ê¸°
dataset_list = datasets.list_datasets()

# dataset list í™•ì¸
for datas in dataset_list:
    if 'ko' in datas:
        print(datas)
```

ì˜ì–´ë¿ë§Œ ì•„ë‹ˆë¼ ì „ì„¸ê³„ ì–¸ì–´ì—ì„œ ìì—°ì–´ ê´€ë ¨ datasets ê°€ ì „ë¶€ ì´ hub ì•ˆì— ì €ì¥ì´ ë˜ì–´ìˆë‹¤ê³  ë³´ë©´ ë¨

ìš°ë¦¬ë‚˜ë¼ ë§ì˜ ê´€ë ¨ëœ datasets ë„ ì¡´ì¬í•¨

```python
# nsmc ë°ì´í„° ë¡œë“œ
dataset = datasets.load_dataset('nsmc') # nsmc, hate, sarcasm

# ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸
print(dataset)
```

![]({{site.url}}/assets/images/cc3a8c57.png)

ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ë ¤ëŠ” dataset ì€ `nsmc` ë¼ëŠ” dataset ì„

naver ì—ì„œ ì œê³µí•œ ì˜í™” ëŒ“ê¸€ì˜ ê°ì„±ë¶„ë¥˜ dataset ì„

ë‹¨ì¾ ë¬¸ì¥ ë¶„ë¥˜ì—ëŠ” ê°ì„±ë¶„ë¥˜ ë§ê³ ë„ ì‚¬ìš© í•  ìˆ˜ ìˆëŠ” `hate`, `sarcasm` ë„ ìˆìŒ

dictionary í˜•íƒœë¡œ ë˜ì–´ìˆê³  train ê³¼ test ê°€ ë‚˜ë‰˜ì–´ì ¸ ìˆìŒ

### ì²«ë²ˆì§¸ë¡œ ìš°ë¦¬ê°€ ë§Œë“¤ì–´ì•¼ í•˜ëŠ” ê²ƒì€ dataset ì„ ë§Œë“¤ì–´ì•¼ í•¨

dataloader ë¡œ ê°€ê¸°ì „ì˜ ì „ë‹¨ê³„ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ë‹¤ë£¨ê¸° í¸í•˜ê²Œ pandas ë¥¼ ì´ìš©í•´ì„œ DataFrame í˜•íƒœë¡œ ì €ì¥í•¨

```python
import pandas as pd

# í•„ìš”í•œ ë°ì´í„°ì¸ documentì™€ label ì •ë³´ë§Œ pandasë¼ì´ë¸ŒëŸ¬ë¦¬ DataFrame í˜•ì‹ìœ¼ë¡œ ë³€í™˜
train_data = pd.DataFrame({"document":dataset['train']['document'], "label":dataset['train']['label'],})
test_data = pd.DataFrame({"document":dataset['test']['document'], "label":dataset['test']['label'],})

# ë°ì´í„°ì…‹ ê°¯ìˆ˜ í™•ì¸
print('í•™ìŠµ ë°ì´í„°ì…‹ : {}'.format(len(train_data)))
print('í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ : {}'.format(len(test_data)))
```

![]({{site.url}}/assets/images/5b9c4a21.png)

```python
# ë°ì´í„°ì…‹ ë‚´ìš© í™•ì¸
train_data[:5]
```

![]({{site.url}}/assets/images/253ba817.png)

```python
test_data[:5]
```

![]({{site.url}}/assets/images/eca36e3a.png)

ì „ì²˜ë¦¬ ë‹¨ê³„ ì¤‘ì— í•˜ë‚˜ì¸ ë°ì´í„° ì¤‘ë³µì„ ì œê±°í•˜ê³  outlier ë°ì´í„° ì‚­ì œí•˜ê³  label ì´ 0 ê³¼ 1 ì¸ë° ì˜¤íƒˆìê°€ë‚˜ì„œ 3ì´ ìˆë‹¤
ì´ëŸ° ê²ƒë“¤ì„ ì‚­ì œí•´ì•¼ í•¨

ì´ëŸ° ê²ƒë“¤ì„ ì§„í–‰í•˜ëŠ”ê²Œ ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì„

```python
# ë°ì´í„° ì¤‘ë³µì„ ì œì™¸í•œ ê°¯ìˆ˜ í™•ì¸
print("í•™ìŠµë°ì´í„° : ",train_data['document'].nunique()," ë¼ë²¨ : ",train_data['label'].nunique())
print("ë°ìŠ¤íŠ¸ ë°ì´í„° : ",test_data['document'].nunique()," ë¼ë²¨ : ",test_data['label'].nunique())

# ì¤‘ë³µ ë°ì´í„° ì œê±°
train_data.drop_duplicates(subset=['document'], inplace= True)
test_data.drop_duplicates(subset=['document'], inplace= True)

# ë°ì´í„°ì…‹ ê°¯ìˆ˜ í™•ì¸
print('ì¤‘ë³µ ì œê±° í›„ í•™ìŠµ ë°ì´í„°ì…‹ : {}'.format(len(train_data)))
print('ì¤‘ë³µ ì œê±° í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ : {}'.format(len(test_data)))
```

![]({{site.url}}/assets/images/cf429aa9.png)

unique() ë¥¼ ì‚¬ìš©í•´ì„œ ê°œìˆ˜ë¥¼ ë³´ë‹ˆ 150000ê°œë³´ë‹¤ ì¤„ì–´ë“¤ì–´ì„œ ì¤‘ë³µ ë°ì´í„°ê°€ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ

ê·¸ ë‹¤ìŒì—” null ë°ì´í„°ë¥¼ ì œê±°í•˜ì

```python
import numpy as np

# null ë°ì´í„° ì œê±°
train_data['document'].replace('', np.nan, inplace=True)
test_data['document'].replace('', np.nan, inplace=True)
train_data = train_data.dropna(how = 'any')
test_data = test_data.dropna(how = 'any')

print('null ì œê±° í›„ í•™ìŠµ ë°ì´í„°ì…‹ : {}'.format(len(train_data)))
print('null ì œê±° í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ : {}'.format(len(test_data)))
```

![]({{site.url}}/assets/images/c900b959.png)

null ë°ì´í„°ë¥¼ ì œê±°í–ˆë”ë‹ˆ í•˜ë‚˜ì”© ì¤„ì–´ ë“¤ì—ˆìŒ

```python
print(train_data['document'][0])
print(train_data['label'][0])
```

![]({{site.url}}/assets/images/57f7b545.png)

ë‹¤ìŒìœ¼ë¡œëŠ” outlier ë¥¼ ì œê±°í•˜ëŠ” ê²ƒë„ ì „ì²˜ë¦¬ ê³¼ì •ì¤‘ì— í•˜ë‚˜ì„

í•™ìŠµ ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ì™€ í‰ê·  ê¸¸ì´ë¥¼ êµ¬í•˜ê³  ì´ sentence ë“¤ì˜ histogram ì„ ê·¸ë ¤ë³´ì

```python
from matplotlib import pyplot as plt

#í•™ìŠµ ë¦¬ë·° ê¸¸ì´ì¡°ì‚¬
print('í•™ìŠµ ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´ :',max(len(l) for l in train_data['document']))
print('í•™ìŠµ ë¬¸ì¥ì˜ í‰ê·  ê¸¸ì´ :',sum(map(len, train_data['document']))/len(train_data['document']))

plt.hist([len(s) for s in train_data['document']], bins=50)
plt.xlabel('length of data')
plt.ylabel('number of data')
plt.show()
```

![]({{site.url}}/assets/images/a9c17f20.png)

lentgh ìì²´ëŠ” sentence character_length ì„  
í‰ê· ì€ 35 ì •ë„ ë˜ëŠ”ë° í•™ìŠµ ë¬¸ì¥ì˜ ìµœëŒ€ëŠ” 146ê¹Œì§€ë„ ìˆìŒ  
ìš°ë¦¬ê°€ ë§Œë“  BERT ëª¨ë¸ ìì²´ê°€ ì´ length ë¥¼ ì „ë¶€ë‹¤ í¬í•¨í•  ìˆ˜ ìˆì„ë§Œí¼ í° ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— ìš°ì„  í•„í„°ë§ ì—†ì´ ë„˜ì–´ê°€ë„ë¡ í•˜ì

### í•™ìŠµì— ì‚¬ìš©ë  pre-trained ëœ BERT ëª¨ë¸ì„ ê°€ì ¸ì˜¤ì

```python
# Store the model we want to use
from transformers import AutoModel, AutoTokenizer, BertTokenizer
MODEL_NAME = "bert-base-multilingual-cased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
```

### í† í¬ë‚˜ì´ì € ì¤€ë¹„

```python
tokenized_train_sentences = tokenizer(
    list(train_data['document']),
    return_tensors="pt",
    padding=True,
    truncation=True,
    add_special_tokens=True,
    )

print(tokenized_train_sentences[0])
print(tokenized_train_sentences[0].tokens)
print(tokenized_train_sentences[0].ids)
print(tokenized_train_sentences[0].attention_mask)
```

![]({{site.url}}/assets/images/717af4a8.png)

ë°ì´í„°ì…‹ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œ í† í¬ë‚˜ì´ì§•ì„ í•¨    
í† í¬ë‚˜ì´ì§• í•  ë•ŒëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë¬¸ì¥ì„ ë„£ê²Œ ë˜ë©´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ output ì´ ë°˜í™˜ ë¨  
[CLS] ì™€ [SEP] í† í°ì´ ë¶€ì°©ë˜ì—ˆìŒ  

í´ë˜ìŠ¤ë¥¼ ê±°ì³ì„œ ë‚˜ì˜¨ ê²°ê³¼ë¬¼ì—ëŠ” ì–´ë–¤ token ì´ ìˆëŠ”ì§€ í•´ë‹¹ í† í°ì˜ vocab ì€ ë­”ì§€ ê·¸ë¦¬ê³  attention_mask ëŠ” ë­”ì§€ ì´ëŸ° 
ì •ë³´ê°€ ë‹¤ ë“¤ì–´ìˆìŒ

ê·¸ëŸ¬ë©´ ì´ì œ dataset ì´ ê±°ì˜ ì™„ì„±ì´ ë˜ì—ˆìŒ

train ì„ ìœ„í•œ tokenized ëœ data ì„ ì´ë ‡ê²Œ ë§Œë“¤ì–´ ì¤¬ê³ 

```python
tokenized_test_sentences = tokenizer(
    list(test_data['document']),
    return_tensors="pt",
    padding=True,
    truncation=True,
    add_special_tokens=True,
    )
```

test ë¥¼ ìœ„í•œ tokenized ëœ data ë„ ë§Œë“¤ì–´ ì¤Œ

```python
train_label = train_data['label'].values
test_label = test_data['label'].values

print(train_label[0])
```

![]({{site.url}}/assets/images/06412a68.png)

ì´ë ‡ê²Œ sentence, label ì„ ì¤€ë¹„í•´ë†“ê³ 

ì´ê±¸ ì‹¤ì œ ëª¨ë¸ì— ì…ë ¥í•˜ê¸° ìœ„í•œ êµ¬ì¡°ì ì¸ í˜•íƒœë¡œ ë§Œë“¤ì–´ ì¤„ ê±°ì„

hugging face ê°€ ë§Œë“  ê²ƒì´ê¸° ë•Œë¬¸ì— tokenizer ì—ì„œ ë‚˜ì˜¨ class ì •ë³´ key, value ë‘ ëª¨ë¸ì— ë“¤ì–´ê°€ëŠ” key, value ë‘
ì„œë¡œ ì¼ì¹˜í•˜ê²Œ ë¨

ê·¸ë˜ì„œ ì‚¬ì‹¤ìƒ ê³ ì¹ ê²Œ ë§ì´ ì—†ìŒ

```python
class SingleSentDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)
```

dataset ì—ëŠ” `__getitem__` ì´ë¼ëŠ” í•¨ìˆ˜ê°€ ìˆëŠ”ë° í•™ìŠµì´ ì§„í–‰ë˜ë©´ ë§¤ step ì´ ì§„í–‰ì´ ë  ê±°ê³  ì´ê²ƒì´ ë°°ì¹˜ë‹¨ìœ„ë¡œ ë™ì‘í•˜ê²Œ
ë˜ëŠ”ë° ê·¸ step ì— ë§ëŠ” ë°ì´í„°ì…‹ì„ ëª¨ë¸ì— ê°€ì ¸ì˜¤ê²Œ ë˜ëŠ”ê²Œ ë°”ë¡œ ì´ `__getitem__` í•¨ìˆ˜ë¥¼ í†µí•´ì„œ ê°€ì ¸ì˜¤ê²Œ ë¨

ëª¨ë¸ ì…ì¥ì—ì„œëŠ” 

![]({{site.url}}/assets/images/c0c176df.png)

ì´ 3ê°€ì§€ ì •ë³´ê°€ ì „ë‹¬ì´ ë˜ì–´ì•¼ í•˜ëŠ”ë° ì´ 3ê°€ì§€ê°€ ì „ë¶€ key, value í˜•íƒœë¡œ êµ¬ì„±ì´ ë˜ì–´ìˆê³  ëª¨ë¸ì—ëŠ” ê·¸ key, value ê°€ ê·¸ëŒ€ë¡œ
ë“¤ì–´ê°€ë©´ ë¨

```python
train_dataset = SingleSentDataset(tokenized_train_sentences, train_label)
test_dataset = SingleSentDataset(tokenized_test_sentences, test_label)
```

tokenized_train_sentences ì™€ train_label ì„ íŒŒë¼ë¯¸í„°ë¡œ ë„£ì–´ì„œ dataset class ë¥¼ í†µí•´ì„œ dataset ì„ ë§Œë“¤ì–´ ì¤Œ

`BertForSequenceClassification` ì´ë¼ëŠ”ê±¸ ì œê³µí•´ì¤Œ

ë¬¸ì¥ ë¶„ë¥˜ë¥¼ ìœ„í•´ì„œëŠ” BERT ìœ„ì— Classification ìœ„ì— head ë¥¼ ë¶€ì°©í•´ì•¼ í•˜ëŠ”ë° ì´ê±¸ transformers ì—ì„œ ì œê³µí•´ì£¼ê³  ìˆìŒ

```python
from transformers import BertForSequenceClassification, Trainer, TrainingArguments
# ë¬¸ì¥ ë¶„ë¥˜ë¥¼ ìœ„í•´ì„  BERT ìœ„ì— classificationì„ ìœ„í•œ headë¥¼ ë¶€ì°©í•´ì•¼ í•©ë‹ˆë‹¤.
# í•´ë‹¹ ë¶€ë¶„ì„ transformersì—ì„œëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ í•˜ë‚˜ë§Œ í˜¸ì¶œí•˜ë©´ ë©ë‹ˆë‹¤! :-)
```

ëª¨ë¸ initialize ë¥¼ `BertForSequenceClassification` ì´ê±¸ë¡œ í•  ê±°ì„

training arguments ë¥¼ ë´ë³´ì

```python
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=1,              # total number of training epochs
    per_device_train_batch_size=32,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=500,
    save_steps=500,
    save_total_limit=2
)
```

output_dir ê²°ì •í•˜ê³  train_epochs ì„¤ì •í•˜ê³  train_batch_size ê²°ì •í•˜ê³   
warmup_steps : gradient descent ë¥¼ í•˜ë©´ì„œ ì›€ì§ì´ê²Œ ë˜ëŠ” ë²”ìœ„ë¥¼ ì–´ëŠì •ë„ë¡œ í• ê±°ëƒë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ë³´ì¸ 
learning_rate ë¼ëŠ”ê²Œ ìˆëŠ”ë° ë³´í†µ ì´ê±°ë¥¼ ê³ ì •ëœ ê°’ì„ í•˜ê²Œ ë˜ëŠ”ë° BERT ëŠ” íŠ¹ì´í•˜ê²Œ warmup_steps ë¼ëŠ” ì•„ì´ë””ì–´ë¥¼ ì‚¬ìš©í•¨  
ê·¸ë˜ì„œ maximum_learning_rate ê°€ ìˆìœ¼ë©´ ì²˜ìŒì— 0ì—ì„œë¶€í„° maximum ê¹Œì§€ ì˜¬ë¼ê°”ë‹¤ê°€ weight_decay ëŠ” ê·¸ê±¸ ë–¨ì–´ëœ¨ë¦¬ëŠ”
í˜•ì‹ìœ¼ë¡œ êµ¬ì„±ì´ ë˜ì–´ìˆìŒ ì¦‰, warmup_steps ëŠ” learning_rate ë¥¼ ì¡°ì ˆí•˜ëŠ” ëŒ€ìƒì„  
ë§ˆì°¬ê°€ì§€ë¡œ logging_dir ê°™ì€ ê²½ìš°ì—ëŠ” í•™ìŠµì— í•´ë‹¹í•˜ëŠ” log ë¥¼ ì €ì¥í•  ìˆ˜ ìˆìŒ  
loging_steps, save_steps ë‘˜ë‹¤ 500ì´ë©´ 500 step ë§ˆë‹¤ ì €ì¥í•˜ê³  save_total_limit=2 ì´ë©´ ì €ì¥ë˜ëŠ” ëª¨ë¸ì´ 2ê°œë§Œ
ì¡´ì¬í•¨

ì´ë ‡ê²Œ train arguments ë¥¼ ì •ì˜í•´ì£¼ê³  

```python
model = BertForSequenceClassification.from_pretrained(MODEL_NAME)
model.to(device)

trainer = Trainer(
    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
)
```

ëª¨ë¸ì€ BertForSequenceClasssification ìœ¼ë¡œ initialize í•´ì£¼ê³  ê·¸ ë‹¤ìŒì— ëª¨ë¸ì„ GPU ë¡œ ì—…ë¡œë“œí•˜ê³   
ê·¸ ë‹¤ìŒì— Trainer ë¥¼ ë§Œë“¤ê³  train_dataset ì€ trainer ì—ì„œ ì‚¬ìš©ë¨

ê·¸ ë‹¤ìŒì—” trainer.train() ì„ í•˜ë©´ í•™ìŠµì´ ë¨

```python
trainer.train() # 1 epochì— ëŒ€ëµ 30ë¶„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤ :-)
```

`nsmc` ê°™ì€ ê²½ìš° ë°ì´í„°ê°€ ë§ì•„ì„œ 1epoch ì— 30ë¶„ì •ë„ ê±¸ë¦¼

### í•™ìŠµì„ í•˜ê³ ë‚˜ë©´ í‰ê°€ê°€ í•„ìš”í•¨

ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•œ í•¨ìˆ˜ë¥¼ ë§Œë“¦

```python
from sklearn.metrics import precision_recall_fscore_support, accuracy_score

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

trainer = Trainer(
    model=model,
    args=training_args,
    compute_metrics=compute_metrics
)
```

trainer ë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ `evaluate` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ  
`evaluate` ê°€ ì‹¤í–‰ì´ ëì„ ë•Œ compute_matrics ë¼ëŠ” í•¨ìˆ˜ë¥¼ ë¶ˆëŸ¬ì™€ì„œ í‰ê°€ë¥¼ í•˜ê²Œ ë¨  
ì´ í•¨ìˆ˜ì—ì„œ ì…ë ¥ê°’ì€ `pred` ë¼ëŠ” class ì„  
ì´ class ì•ˆì—ëŠ” label_ids ê°€ ì¡´ì¬í•˜ê³  predictions ë¼ëŠ” ê²ƒë„ ì¡´ì¬í•¨  
ì›ë˜ ì •ë‹µë ˆì´ë¸”ì´ ë­”ì§€ ë‚˜ì˜ ëª¨ë¸ì´ ì˜ˆì¸¡í•œê²Œ ë­”ì§€ ì •ë³´ê°€ labels ì™€ preds ì— ì €ì¥ë¨  
ê·¸ëŸ¬ë©´ sklearn ì˜ metrics ë¥¼ ì‚¬ìš©í•´ì„œ precision_recall_fscore ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŒ  
compute_metrics í•¨ìˆ˜ì˜ ë°˜í™˜ê°’ì€ accuracy, f1, precision, recall ì´ë ‡ê²Œ ë°˜í™˜í•˜ë„ë¡ ì„¤ì •í•´ë‘ê³   
trainer ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸ í•´ì¤Œ

```python
trainer.evaluate(eval_dataset=test_dataset)
```

![]({{site.url}}/assets/images/ede9e8fc.png)

trainer.evaluate í•˜ë©´ì„œ eval_dataset=test_dataset ìœ¼ë¡œ ì„¤ì •í•˜ë©´ evaluation ê²°ê³¼ê°€ ë‚˜ì˜¤ê²Œ ë¨

trainer ë¥¼ ì“°ê³  ì‹¶ì§€ ì•Šìœ¼ë©´ ê¸°ì¡´ì— í•™ìŠµí•˜ë˜ ì½”ë“œì²˜ëŸ¼ ì½”ë”©í•˜ë©´ ë¨

```python
# native training using torch

# model = BertForSequenceClassification.from_pretrained(MODEL_NAME)
# model.to(device)
# model.train()

# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# optim = AdamW(model.parameters(), lr=5e-5)

# for epoch in range(3):
#     for batch in train_loader:
#         optim.zero_grad()
#         input_ids = batch['input_ids'].to(device)
#         attention_mask = batch['attention_mask'].to(device)
#         labels = batch['labels'].to(device)
#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
#         loss = outputs[0]
#         loss.backward()
#         optim.step()
```

### ëª¨ë¸ì„ ë§Œë“¤ì—ˆìœ¼ë‹ˆ preiction ì„ í•´ë³´ì

pipeline ì„ ì“°ë©´ prediction ì½”ë“œë¥¼ êµ¬í˜„í•˜ì§€ ì•Šì•„ë„ ë°”ë¡œ prediction ì´ ê°€ëŠ¥í•¨

```python
from transformers import pipeline

nlp_sentence_classif = pipeline('sentiment-analysis',model=model, tokenizer=tokenizer, device=0)

print(nlp_sentence_classif('ì˜í™” ê°œì¬ë°Œì–´ ã…‹ã…‹ã…‹ã…‹ã…‹'))
print(nlp_sentence_classif('ì§„ì§œ ì¬ë¯¸ì—†ë„¤ìš” ã…‹ã…‹',model= model))
print(nlp_sentence_classif('ë„ˆ ë•Œë¬¸ì— ì§„ì§œ ì§œì¦ë‚˜',model= model))
print(nlp_sentence_classif('ì •ë§ ì¬ë°Œê³  ì¢‹ì•˜ì–´ìš”.',model= model))
```

![]({{site.url}}/assets/images/bc731827.png)

pipeline í•­ëª© ì¤‘ì— 'sentiment-analysis' ë¥¼ ì‚¬ìš©í•˜ë©´ label ì´ 0 ì•„ë‹ˆë©´ 1ë¡œ ë°˜í™”ì´ ë˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŒ  
ëª¨ë¸ì´ GPU ë¡œ ì˜¬ë¼ê°€ìˆê¸° ë•Œë¬¸ì— `device=0` ìœ¼ë¡œ ì„¤ì •í•´ë†“ìŒ

pipeline ì½”ë“œë¥¼ ì“°ê¸° ì‹«ë‹¤ë©´ ì´ë ‡ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆìŒ

```python
# predictí•¨ìˆ˜
def sentences_predict(sent):
    model.eval()
    tokenized_sent = tokenizer(
            sent,
            return_tensors="pt",
            truncation=True,
            add_special_tokens=True,
            max_length=128
    )
    tokenized_sent.to(device)
    
    with torch.no_grad():# ê·¸ë¼ë””ì—”íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
        outputs = model(
            input_ids=tokenized_sent['input_ids'],
            attention_mask=tokenized_sent['attention_mask'],
            token_type_ids=tokenized_sent['token_type_ids']
            )

    logits = outputs[0]
    logits = logits.detach().cpu().numpy()
    result = np.argmax(logits)
    return result

print(sentences_predict("ì˜í™” ê°œì¬ë°Œì–´ ã…‹ã…‹ã…‹ã…‹ã…‹"))
print(sentences_predict("ì§„ì§œ ì¬ë¯¸ì—†ë„¤ìš” ã…‹ã…‹"))
print(sentences_predict("ë„ˆ ë•Œë¬¸ì— ì§„ì§œ ì§œì¦ë‚˜"))
print(sentences_predict("ì •ë§ ì¬ë°Œê³  ì¢‹ì•˜ì–´ìš”."))
```

![]({{site.url}}/assets/images/ba093f83.png)

input_sentence ì— ëŒ€í•´ tokenizing ì„ ì§„í–‰í•˜ê³  tokenizing ëœ ê±°ì—ì„œ ëª¨ë¸ë¡œ input ì„ ë„£ê²Œ ë¨  

ì´ë ‡ê²Œ ë§Œë“¤ë©´ class ê°€ 2ê°œê°€ ì•„ë‹Œ ì—¬ëŸ¬ê°œì¸ ê²½ìš°ì—ë„ prediction ì´ ê°€ëŠ¥í•¨

> ì‘ì„±ì      
```
* ê¹€ì„±í˜„ (bananaband657@gmail.com)  
1ê¸° ë©˜í† 
ê¹€ë°”ë‹¤ (qkek983@gmail.com)
ë°•ìƒí¬ (parksanghee0103@gmail.com)  
ì´ì •ìš° (jungwoo.l2.rs@gmail.com)
2ê¸° ë©˜í† 
ë°•ìƒí¬ (parksanghee0103@gmail.com)  
ì´ì •ìš° (jungwoo.l2.rs@gmail.com)
ì´ë…•ìš° (leenw2@gmail.com)
ë°•ì±„í›ˆ (qkrcogns2222@gmail.com)
```
[CC BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/2.0/kr/)
