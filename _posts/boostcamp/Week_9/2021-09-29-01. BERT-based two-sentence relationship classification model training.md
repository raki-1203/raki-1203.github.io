---
title: "Day_39 01. BERT ê¸°ë°˜ ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ"

categories:
  - Boostcamp_AI_Tech/Week_9
tags:
  - KLUE
---
  
# BERT ê¸°ë°˜ ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ

## 1. ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜ task ì†Œê°œ

### 1.1 ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜ task

**ì£¼ì–´ì§„ 2ê°œì˜ ë¬¸ì¥ì— ëŒ€í•´, ë‘ ë¬¸ì¥ì˜ ìì—°ì–´ ì¶”ë¡ ê³¼ ì˜ë¯¸ë¡ ì ì¸ ìœ ì‚¬ì„±ì„ ì¸¡ì •í•˜ëŠ” task**

![]({{site.url}}/assets/images/6453147c.png)

**Natural Language Inference (NLI)**
- ì–¸ì–´ëª¨ë¸ì´ ìì—°ì–´ì˜ ë§¥ë½ì„ ì´í•´í•  ìˆ˜ ìˆëŠ”ì§€ ê²€ì¦í•˜ëŠ” task
- ì „ì²´ë¬¸ì¥(Premise)ê³¼ ê°€ì„¤ë¬¸ì¥(Hypothesis)ì„ Entailment(í•¨ì˜), Contradiction(ëª¨ìˆœ), Neutral(ì¤‘ë¦½) ìœ¼ë¡œ ë¶„ë¥˜

![]({{site.url}}/assets/images/869e9d43.png)

label 3ê°œë¥¼ classification í•˜ëŠ” ë¬¸ì œ

**Semantic text pair**
- ë‘ ë¬¸ì¥ì˜ ì˜ë¯¸ê°€ ì„œë¡œ ê°™ì€ ë¬¸ì¥ì¸ì§€ ê²€ì¦í•˜ëŠ” task

![]({{site.url}}/assets/images/c1f0385d.png)

2ê°€ì§€ class ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œ

## 2. ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ

### 2.1 Information Retrieval Question and Answering (IRQA)

**ì‹œìŠ¤í…œ êµ¬ì¡°ë„**

![]({{site.url}}/assets/images/333535ce.png)

ì´ task ëŠ” ì±—ë´‡ì„ ìœ„í•œ task ì¸ë° ê·¸ ì¤‘ì— IRQA ë¼ëŠ” task ì„  
ì‚¬ì „ì— ì •ì˜í•´ë†“ì€ QA set ì—ì„œ ê°€ì¥ ì ì ˆí•œ ë‹µë³€ì„ ì°¾ì•„ë‚´ëŠ” ê³¼ì •ì„ IRQA ë¼ê³  í•¨  

ì €ë²ˆ ì‹¤ìŠµì—ì„œ [CLS] í† í°ì„ ì´ìš©í•´ cosine similarity ë¥¼ ì´ìš©í•´ ì±—ë´‡ì„ ê°œë°œí•œê²ƒê³¼ ìœ ì‚¬í•œ task ì„

ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” `ë‚˜ì´ ì•Œë ¤ì¤˜` ë¼ëŠ” ì§ˆë¬¸ì„ í•˜ê²Œë˜ë©´ BERT ë¥¼ í†µí•´ì„œ sentence embedding ì„ í•˜ê²Œ ë¨  
ì—¬ê¸°ê¹Œì§€ëŠ” ì´ì „ ì±—ë´‡ê³¼ ë™ì¼í•¨  
ê·¸ë¦¬ê³  ì‚¬ì „ì— ì •ì˜í•œ table ì—ì„œ Question ê³¼ Answer ê°€ pair ë¡œ ë˜ì–´ìˆëŠ” table ë„ ì—­ì‹œ sentence embedding ì„ í•˜ê³ 
ê¸°ì¡´ì— ì¡´ì¬í•˜ë˜ Question ê³¼ ë‚˜ì˜ Query ì˜ ìœ ì‚¬ë„ë¥¼ ë¹„êµë¥¼í•´ì„œ ê°€ì¥ ì ì ˆí•œ ë¬¸ì¥ì„ ë°˜í™˜í•  ìˆ˜ ìˆìŒ  
ì—¬ê¸° task ê¹Œì§€ëŠ” ê¸°ì¡´ì˜ í–ˆë˜ task ì™€ ë™ì¼í•œ task ì¸ë° ì´ task ë’·ë‹¨ì— paraphrase detection ì´ë¼ëŠ” ë‘ ë¬¸ì¥ê°„ì˜ ê´€ê³„
ë¶„ë¥˜ task ë¥¼ í•™ìŠµí•œ ëª¨ë¸ì„ ë¶€ì°©ì‹œí‚¬ ì˜ˆì •ì„  
ì´ ëª¨ë¸ì„ ë¶€ì°©í•˜ëŠ” ì´ìœ ëŠ” top-n ê°œë¡œ ë¬¸ì¥ì„ ë°˜í™˜í•  ìˆ˜ ìˆëŠ”ë° ê·¸ ì¤‘ì— ë¬´ì¡°ê±´ top-1 ì´ ì •ë‹µì´ ë˜ì§€ ì•ŠëŠ” ê²½ìš°ë„ ìˆìŒ  
ê·¸ë˜ì„œ paraphrase detection ëª¨ë¸ì´ ì§„ì§œë¡œ ë‚´ê°€ ì§ˆì˜í•œ query ì™€ ì‚¬ì „ì— ì •ì˜ëœ question ì´ ì‹¤ì œë¡œ ìœ ì‚¬í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ê³ 
ìˆëŠ”ì§€ë¥¼ ê²€ì¦í•˜ê³  ê·¸ ê²€ì¦ í•„í„°ë§ì„ í†µê³¼í•´ì•¼ì§€ë§Œ ë‹µë³€ì´ ë‚˜ì˜¤ê²Œ ë˜ëŠ” task ì„

---

# ì‹¤ìŠ´

## ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜ë¥¼ ìœ„í•œ í•™ìŠµ ë°ì´í„° êµ¬ì¶•

ì´ë²ˆ task ì˜ ëª©ì ì€ paraphrase ëœ ë‘ ë¬¸ì¥ì´ ì„œë¡œ ê°™ì€ ì˜ë¯¸ë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ ë¶„ë¥˜í•˜ëŠ” ê²ƒì„

paraKQC ë¼ëŠ” ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì  
ì´ ë°ì´í„°ëŠ” ë‘ ë¬¸ì¥ì´ pair ëœ ìŒì´ ìˆëŠ”ê²Œ ì•„ë‹ˆë¼ í•˜ë‚˜ì˜ ë¬¸ì¥ì—ëŒ€í•´ 10ê°œì˜ ìœ ì‚¬í•œ paraphrase ëœ ë¬¸ì¥ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆìŒ
ì´ ë°ì´í„°ëŠ” `ì¡°ì›ìµ` ë‹˜ì´ë¼ëŠ” ë¶„ê»˜ì„œ ë§Œë“¤ì–´ ì£¼ì…¨ìŒ

`!git clone https://github.com/warnikchow/paraKQC.git`

ë°ì´í„°ë¥¼ ë°›ì•˜ìœ¼ë©´ ì‹¤ì œë¡œ ë°ì´í„°ê°€ ì–´ë–»ê²Œ ìƒê²¼ëŠ”ì§€ í™•ì¸í•´ë³´ì

```python
data = open('/content/paraKQC/data/paraKQC_v1.txt')
lines = data.readlines()
for i in range(0,15):
    print(lines[i]) # 10ê°œì”© paraphasing 
```

![]({{site.url}}/assets/images/3be7c0a9.png)

ìš°ë¦¬ê°€ í•„ìš”í•œ ê±´ paraphrase ëœ ë°ì´í„°ì™€ paraphrase ë˜ì§€ ì•Šì€ ë°ì´í„° 2ê°€ì§€ê°€ í•„ìš”í•¨  
ê·¸ë˜ì•¼ì§€ë§Œ classification ì´ ê°€ëŠ¥í•´ì§

ì´ ì‹¤ìŠµì˜ ëª©ì ì€ ê·¸ ë°ì´í„°ë¥¼ êµ¬ì¶•í•˜ëŠ”ë° ëª©ì ì´ ìˆìŒ

ê·¸ëŸ¬ë©´ ì§€ê¸ˆì€ 10ê°œì”© ìŒìœ¼ë¡œ ë°ì´í„°ê°€ ë¬¶ì—¬ì ¸ ìˆì§€ë§Œ ì¤„ë¡œë§Œ ë‚˜ì—´ì´ ë˜ì–´ìˆìŒ  
ê·¸ë˜ì„œ 10ê°œì”© ë°ì´í„°ì˜ í˜•íƒœë¡œ ë¬¶ë„ë¡ í•˜ê² ìŒ 

```python
similar_sents = {}
similar_sent = []
total_sent = []
for line in lines:
    line = line.strip()
    sent = line.split('\t')[2]
    total_sent.append(sent)
    similar_sent.append(sent)
    if len(similar_sent) == 10:
        similar_sents[similar_sent[0]] = similar_sent[1:]
        similar_sent = []

print(len(total_sent))  # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì¥ì„ ì°¾ê¸° ìœ„í•œ ì „ì²´ ë¬¸ì¥ pool
```

![]({{site.url}}/assets/images/a10e7384.png)

ë°ì´í„° í˜•íƒœë¡œ ë¬¸ì¥ì„ í•˜ë‚˜ì”© ì½ì–´ë‚˜ê°€ë©´ì„œ ë¬¸ì¥ 10ê°œë¥¼ ì½ê²Œë˜ëŠ” ìˆœê°„ ë°ì´í„° ë°°ì—´ë¡œ ë°ì´í„° ë§ë­‰ì¹˜ë¥¼ ë¹¼ë„ë¡ ë˜ì–´ìˆìŒ  
ì „ì²´ë¬¸ì¥ì€ total_sent ë¼ëŠ” ë°°ì—´ì— ì €ì¥í•´ë’€ìŒ

```python
for i in range(0,15):
    print(total_sent[i])
```

![]({{site.url}}/assets/images/e954f491.png)

total_sent ë¥¼ ì½ì–´ë³´ë©´ ë°ì´í„°ì…‹ì— ìˆë˜ ì „ì²´ ë¬¸ì¥ì´ ìˆœì„œëŒ€ë¡œ ì €ì¥ë˜ì–´ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

```python
print(len(similar_sents))
```

![]({{site.url}}/assets/images/5a72e82f.png)

ê·¸ ë‹¤ìŒì—” 10ê°œì˜ ë¬¶ìŒì´ ëª‡ê°œ ìˆëŠ”ì§€ í™•ì¸í•´ë³´ë©´ 999ê°œ ì˜ paraphrase ëœ ë¬¶ìŒì´ ì €ì¥ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

```python
for i, key in enumerate(similar_sents.keys()):  # 10ê°œì˜ ë¬¸ì¥ ì¤‘, ì²« ë²ˆì§¸ ë¬¸ì¥ì„ key
    print('\n', key)                            # ë‚˜ë¨¸ì§€ 9ê°œì˜ ë¬¸ì¥ì„ value
    for sent in similar_sents[key]:             # í—·ê°ˆë¦¬ë‹ˆê¹Œ ì´ê±¸ similar_sents dictë¼ê³  ì •ì˜í• ê²Œìš” :-)
        print("-", sent)
    if i > 3:
        break
```

![]({{site.url}}/assets/images/b10a5c05.png)
![]({{site.url}}/assets/images/2c265ce6.png)

ì´ ì €ì¥ëœ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ì“°ëŠëƒ?  
ë°ì´í„°ëŠ” ìœ ì‚¬í•œ ë°ì´í„°ì™€ ìœ ì‚¬í•˜ì§€ ì•Šì€ ë°ì´í„° ì´ë ‡ê²Œ 0 ê³¼ 1 ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ í•„ìš”í•¨  
ê·¼ë° ìš°ë¦¬ê°€ ê°€ì§„ ë°ì´í„°ëŠ” ì „ë¶€ë‹¤ ìœ ì‚¬í•œ ë°ì´í„°ë§Œ ê°€ì§€ê³  ìˆìŒ  
ê·¸ë˜ì„œ ìœ ì‚¬í•˜ì§€ ì•Šì€ ë°ì´í„°ë¥¼ êµ¬ì¶•í•  ë•Œ ì–´ë–»ê²Œ í•˜ëƒë©´?  
ì•„ê¹Œ total_sent ë¼ëŠ” ë°°ì—´ì— ì „ì²´ ë¬¸ì¥ì„ ì €ì¥í•´ë’€ìŒ  
ê·¸ ì¤‘ì—ì„œ random ìœ¼ë¡œ ì„ íƒí•˜ê²Œ ë˜ë©´ ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì¥ ì˜ë¯¸ê°€ ë‹¤ë¥¸ ë¬¸ì¥ì´ ì„ íƒë  ìˆ˜ ìˆìŒ  
í•˜ì§€ë§Œ ì´ë ‡ê²Œ random ì¸ ìƒíƒœì—ì„œ ì„ íƒì„ í•˜ê²Œ ë˜ë©´ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ  
ì´ë ‡ê²Œ ë˜ë©´ ë‘ ì˜ë¯¸ê°€ ì •ë§ë¡œ ìƒë°˜ëœ ê²°ê³¼ë¥¼ ê°€ì§€ê³  í•™ìŠµì„ í•˜ê²Œë¨  
ê·¸ëŸ¬ë©´ ë¬¸ì œê°€ ì •ë§ë¡œ ì‰¬ìš´ ë¬¸ì œë¥¼ í‘¸ëŠ” task ë¡œ ë°”ë€Œê²Œ ë¨  
ê·¸ë˜ì„œ ì–´ë–»ê²Œ í• ê±°ëƒë©´?  

![]({{site.url}}/assets/images/06e6fe93.png)

ì´ key ê°’ value ë¥¼ ê°€ì§€ê³  ìˆëŠ” ë¬¸ì¥ì„ ë‘ê³  ì´ ë¬¸ì¥ì„ BERT ë¥¼ í†µí•´ì„œ sentence embedding ì„ í•  ê²ƒì„  
ê·¸ëŸ¬ë©´ key ë¬¸ì¥ì˜ sentence embedding ê²°ê³¼ê°€ ë‚˜ì˜¬ê±°ê³  ê·¸ë¦¬ê³  total_sent ì— ë“¤ì–´ìˆëŠ” ë¬¸ì¥ì¤‘ì— ì´ ë¬¸ì¥ì„
ëª¨ë‘ sentence embedding ì„ í•˜ê³  ê°€ì¥ embedding ê²°ê³¼ê°€ ìœ ì‚¬í•œ ì• ë¥¼ ì„ íƒí•˜ë„ë¡ í•  ê²ƒì„  
ê·¸ëŸ¬ë‹ˆê¹Œ ëª¨ë¸ì´ ë´¤ì„ë•ŒëŠ” sentence embedding ì¦‰, embedding ê²°ê³¼ê°€ ìœ ì‚¬í•œ ë‘ ë¬¸ì¥ì´ ìˆì§€ë§Œ ì‹¤ì œë¡œ ì´ ë‘ë¬¸ì¥ì€
ì˜ë¯¸ë¡ ì ìœ¼ë¡  ë‹¤ë¦„  
ì´ëŸ¬ë©´ ë¬¸ì œê°€ êµ‰ì¥íˆ ì–´ë ¤ì›Œì§€ê²Œ ë¨  
ëª¨ë¸ ì…ì¥ì—ì„œëŠ” ì–´ë ¤ìš´ ë¬¸ì œë¥¼ í•™ìŠµí•˜ê²Œ ë˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— ë‚˜ì¤‘ì— ìš°ë¦¬ê°€ test ë¥¼ í• ë•Œë„ ì–´ë ¤ìš´ ë¬¸ì œì— ëŒ€í•´ì„œë„ ì˜¬ë°”ë¥´ê²Œ
í‰ê°€ë¥¼ í•  ìˆ˜ ìˆì„ê²ƒì„

sentence embedding ì„ ì–»ê¸° ìœ„í•´ BERT ë¥¼ ê°€ì ¸ì™€ì•¼ í•¨

```python
import torch
from transformers import AutoModel, AutoTokenizer

MODEL_NAME = "bert-base-multilingual-cased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(MODEL_NAME)
model.to('cuda:0')
```

ê³„ì† ì‚¬ìš©í–ˆë˜ `bert-base-multilingual-cased` ëª¨ë¸ì„ ì‚¬ìš©í•˜ê² ìŒ

ê·¸ëŸ¬ë©´ [CLS] í† í°ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆì–´ì•¼í•¨

```python
def get_cls_token(sent_A):
    model.eval()
    tokenized_sent = tokenizer(
            sent_A,
            return_tensors="pt",
            truncation=True,
            add_special_tokens=True,
            max_length=32
    ).to('cuda:0')
    with torch.no_grad():# ê·¸ë¼ë””ì—”íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
        outputs = model(
            input_ids=tokenized_sent['input_ids'],
            attention_mask=tokenized_sent['attention_mask'],
            token_type_ids=tokenized_sent['token_type_ids']
            )
    logits = outputs.last_hidden_state[:,0,:].detach().cpu().numpy()
    return logits

print(get_cls_token("ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤."))
```

![]({{site.url}}/assets/images/1c6a1e77.png)
- ë„ˆë¬´ ê¸¸ì–´ì„œ ì¼ë¶€ë§Œ ìº ì³í•¨

[CLS] í† í°ì„ ê°€ì ¸ì˜¤ê³  ê·¸ í† í°ì˜ embedding ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í–ˆìŒ

ë¬¼ë¡  ì´ì „ ì‹¤ìŠµì—ì„œ í•´ì™”ë˜ ê²ƒì²˜ëŸ¼ ëª¨ë¸ì˜ inference ë¥¼ ë‚ ë¦¬ê³  pooled_output ì„ ê°€ì ¸ì˜¤ê²Œ ë˜ë©´ ë˜‘ê°™ì´ [CLS] í† í°ì„
ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ

"ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤." ë¬¸ì¥ì— ëŒ€í•´ [CLS] í† í° 768 ì°¨ì›ì˜ ë²¡í„°í‹€ ì˜¬ë°”ë¥´ê²Œ ê°€ì ¸ì˜¨ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

```python
total_sent_vector = {}
for i, sent in enumerate(total_sent):   # ì „ì²´ ë¬¸ì¥ poolì„ ì „ë¶€ embedding!
    total_sent_vector[sent] = get_cls_token(sent)   # {key, value} = {ë¬¸ì¥, vector}
    if i % 500==0:
        print(i)
```

![]({{site.url}}/assets/images/78185298.png)

paraphrase ëœ key sentence ì˜ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì¥ì„ ì°¾ê¸° ìœ„í•´ ì „ì²´ total_sent ìœ„ì—ì„œ ì €ì¥í–ˆë˜ ì „ì²´ ë¬¸ì¥ì„ ì „ë¶€
embedding ì„ í•˜ì

key, value ë¡œ ì €ì¥í• ê±´ë° ì „ì²´ total ë¬¸ì¥ì— ëŒ€í•´ key=ë¬¸ì¥, value=vector ë¡œ ì €ì¥í•˜ë„ë¡ í•¨

total_sent ë¥¼ ë°˜ë³µë¬¸ì„ ëŒë©´ì„œ vector ë¥¼ ê°€ì ¸ì˜¤ê²Œ ë¨  
ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ì „ì²´ ë¬¸ì¥ì— ëŒ€í•´ ê°ê°ì˜ vector ê°€ ì €ì¥ì´ ë¨

ê·¸ëŸ¬ë©´ ë‹¤ìŒì˜ ëª©ì ì€ similar_sent dictionary ë¼ê³  ì •ì˜í–ˆë˜ ë¶€ë¶„ì´ ìˆì—ˆëŠ”ë° ê±°ê¸°ì— ìˆëŠ” key ê°’ê³¼ total_sent ë¥¼ 
ì „ë¶€ ë¹„êµë¥¼í•˜ë©° ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ ë¬¸ì¥ ë²¡í„°ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ í•˜ì

```python
import numpy as np

def custom_cosine_similarity(a,b):
    numerator = np.dot(a,b.T)
    a_norm = np.sqrt(np.sum(a * a))
    b_norm = np.sqrt(np.sum(b * b, axis=-1))

    denominator = a_norm * b_norm
    return numerator/denominator

non_similar_sents = {}

for key in similar_sents.keys():    # similar_sents dictì˜ sentenceë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    key_sent_vector = total_sent_vector[key]    # ì „ì²´ ë¬¸ì¥ poolì—ì„œ í•´ë‹¹ sentì˜ vectorì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    sentence_similarity = {}                    # ë‹¤ìŒìœ¼ë¡œëŠ” ì „ì²´ ë¬¸ì¥ poolì˜ ëª¨ë“  vectorì™€ ë¹„êµí•˜ë©°
    for sent in total_sent:                     # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì¥ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        if sent not in similar_sents[key] and sent != key:
            sent_vector = total_sent_vector[sent]
            similarity = custom_cosine_similarity(key_sent_vector, sent_vector)
            sentence_similarity[sent] = similarity
    sorted_sim = sorted(sentence_similarity.items(), key=lambda x: x[1], reverse=True)
    non_similar_sents[key] = sorted_sim[0:10]   # similar_sents dictì˜ ë¬¸ì¥ê³¼ ê°€ì¥ ìœ ì‚¬í•œ 10ê°œì˜ ë¬¸ì¥ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

for i, key in enumerate(non_similar_sents.keys()):
    print('\n', key)
    for sent in non_similar_sents[key]:
        print("-", sent)
    if i > 3:
        break
```

![]({{site.url}}/assets/images/6a504c8d.png)
![]({{site.url}}/assets/images/c8c65700.png)

ì „ì²´ ë¬¸ì¥ pool ì„ ëŒ€ìƒìœ¼ë¡œ ëª¨ë“  ë²¡í„°ë¥¼ ì €ì¥í•´ë’€ê³  ê·¸ëŸ¬ë©´ í•´ë‹¹ ë”•ì…”ë„ˆë¦¬ì— similar_sents dict ì— ìˆëŠ” sentence ë¥¼ 
key ê°’ìœ¼ë¡œ ë„£ê²Œë˜ë©´ ë˜‘ê°™ì´ ë²¡í„°ê°’ì„ ë°˜í™˜ì„ í•¨  
ì´ê²Œ ìš°ë¦¬ê°€ êµ¬í•˜ê³ ì í•˜ëŠ” sentence embedding ê²°ê³¼ê°€ ë¨  
ë‹¤ìŒì€ ì „ì²´ ë¬¸ì¥ pool ì—ì„œ ëª¨ë“  ë²¡í„°ë¥¼ ë‹¤ ë¹„êµë¥¼í•˜ë©° ê°€ì¥ ìœ ì‚¬í•œ ë²¡í„°ë¥¼ ê°€ì ¸ì˜´  
total_sent ë¥¼ ëŒ€ìƒìœ¼ë¡œ ë°˜ë³µë¬¸ì„ ëŒê³  ê·¸ë¦¬ê³  ì§€ê¸ˆ similar_sents dict ê°™ì€ ê²½ìš°ì—ëŠ” key ê°’ì´ sentence ì´ê³  
value ê°€ ìœ ì‚¬í•œ ë¬¸ì¥ 9ê°œê°€ ìˆì—ˆìŒ  
ê·¸ë˜ì„œ ì´ 9ê°œ ë¬¸ì¥ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ”ì§€ë¥¼ ê²€ì‚¬ë¥¼ í•˜ê²Œ ë¨  
ê·¸ë˜ì„œ total_sent ë¥¼ ëŒ€ìƒìœ¼ë¡œ ë´¤ì„ ë•Œ 9ê°œ ë¬¸ì¥ì— í•´ë‹¹ë˜ì§€ ì•Šìœ¼ë©´ embedding ì˜ cosine similarity ë¥¼ êµ¬í•˜ê²Œ ë¨  
ì „ì²´ total_sent ì— ëŒ€í•œ cosine similarity ê²°ê³¼ë¥¼ ì €ì¥í•  ìˆ˜ ìˆìŒ  
ê·¸ë¦¬ê³  ê·¸ ì €ì¥ëœ ë”•ì…”ë„ˆë¦¬ë¥¼ ì •ë ¬ì„ í•´ì£¼ê²Œ ë˜ê³  ê·¸ ìƒìœ„ 10ê°œë§Œ non_similar_sents ë”•ì…”ë„ˆë¦¬ì— ì €ì¥í•˜ê²Œ ë¨  
ì´ëŸ° ë°©ë²•ìœ¼ë¡œ ì½”ë“œëŠ” ì§„í–‰ë˜ê³  ìœ ì‚¬í•˜ì§€ ì•Šì€ ë¬¸ì¥ ë°ì´í„°ê°€ ì™„ì„±ë¨

ì´ë ‡ê²Œ í•™ìŠµë°ì´í„°ë¥¼ ë§Œë“¤ì—ˆê³  ê¸°ì¡´ì— ìˆì—ˆë˜ key sentence ì™€ ìœ ì‚¬í•œ 9ê°œì˜ ë¬¸ì¥ì€ 1ë¡œ tagging ì´ ë  ê±°ê³  ì–˜ëŠ” ìœ ì‚¬í•˜ë‹¤
ê·¸ë¦¬ê³  ë°©ê¸ˆ ë§Œë“  ë°ì´í„°ì…‹ì€ ìœ ì‚¬í•˜ì§€ ì•Šë‹¤ë¼ê³  tagging ì´ ë  ê²ƒì„  
ê·¸ë˜ì„œ 0 ê³¼ 1 ë¡œ classification ì„ ì§„í–‰í•  ìˆ˜ ìˆê²Œë¨

ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ ë°ì´í„°ë¥¼ `para_kqc_sim_data.txt` ë¡œ ì €ì¥í•˜ì

```python
output = open('para_kqc_sim_data.txt', 'w', encoding='utf-8')   # ì´ê±¸ ë°ì´í„°ë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤ :-)

for i, key in enumerate(similar_sents.keys()):
    for sent in similar_sents[key]:
        output.write(key + '\t' + sent + '\t1\n')

for i, key in enumerate(non_similar_sents.keys()):
    for sent in non_similar_sents[key]:
        output.write(key + '\t' + sent[0] + '\t0\n')

output.close()
```

ì´ë ‡ê²Œ í•™ìŠµë°ì´í„°ë¥¼ êµ¬ì¶•í–ˆê³  ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„ì§œë¡œ ë‘ ë¬¸ì¥ì˜ ê´€ê³„ë¥¼ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” BERT ëª¨ë¸ì„ í•™ìŠµí•˜ë„ë¡ í•˜ì

---

# ì‹¤ìŠµ

## BERT ëª¨ë¸ì„ í™œìš©í•œ ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜ í•™ìŠµ

í•™ìŠµë°ì´í„°ë¥¼ í™•ì¸í•´ë³´ì

```python
import torch
import sys

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

data = open('/content/para_kqc_sim_data.txt', 'r', encoding='utf-8')
lines = data.readlines()

# ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸
print(lines[0:10])
```

![]({{site.url}}/assets/images/58645531.png)

ë³´ë©´ ë‘ ë¬¸ì¥ì€ `\t` ìœ¼ë¡œ êµ¬ë¶„ë˜ì–´ìˆê³  `\t` ë’¤ì—ëŠ” label ì´ ë˜ì–´ìˆëŠ” text íŒŒì¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŒ

ì´ text íŒŒì¼ì„ ë‹¤ë£¨ê¸° ì‰½ê²Œ `pandas` ë¡œ ë¶ˆëŸ¬ì˜¤ì

```python
import random
random.shuffle(lines)

train = {'sent_a':[], 'sent_b':[], 'label':[]}
test = {'sent_a':[], 'sent_b':[], 'label':[]}
for i, line in enumerate(lines):
    if i < len(lines) * 0.8:
        line = line.strip()
        train['sent_a'].append(line.split('\t')[0])
        train['sent_b'].append(line.split('\t')[1])
        train['label'].append(int(line.split('\t')[2]))
    else:
        line = line.strip()
        test['sent_a'].append(line.split('\t')[0])
        test['sent_b'].append(line.split('\t')[1])
        test['label'].append(int(line.split('\t')[2]))
```

train data ì™€ test data ëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ë¨  
`sent_a` í•­ëª©ì´ ìˆê³  ë¬¸ì¥1ë²ˆ, ê·¸ë¦¬ê³  `sent_b` ë¬¸ì¥2ë²ˆ ê·¸ë¦¬ê³  ê·¸ ë‘ë¬¸ì¥ì˜ ê´€ê³„ê°€ ìœ ì‚¬í•˜ëƒ ìœ ì‚¬í•˜ì§€ ì•ŠëŠëƒë¼ê³  
label ì„ tagging í•˜ë„ë¡ í•˜ê² ìŒ

ì´ ë•Œ ë‚˜ëˆŒë•Œ, í˜„ì¬ êµ¬ì¶•ëœ ë°ì´í„°ì˜ 80% ëŠ” í•™ìŠµë°ì´í„°ë¡œ ì“°ê³  20% ëŠ” í‰ê°€ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ë„ë¡ í•˜ê² ìŒ

```python
import pandas as pd

train_data = pd.DataFrame({"sent_a":train['sent_a'], "sent_b":train['sent_b'], "label":train['label']})
test_data = pd.DataFrame({"sent_a":test['sent_a'], "sent_b":test['sent_b'], "label":test['label']})

# ë°ì´í„° ì¤‘ë³µì„ ì œì™¸í•œ ê°¯ìˆ˜ í™•ì¸
print("í•™ìŠµë°ì´í„° : ",train_data.groupby(['sent_a', 'sent_b']).ngroups," ë¼ë°¸ : ",train_data['label'].nunique())
print("ë°ìŠ¤íŠ¸ ë°ì´í„° : ",test_data.groupby(['sent_a', 'sent_b']).ngroups," ë¼ë²¨ : ",test_data['label'].nunique())

# ì¤‘ë³µ ë°ì´í„° ì œê±°
train_data.drop_duplicates(subset=['sent_a', 'sent_b'], inplace= True)
test_data.drop_duplicates(subset=['sent_a', 'sent_b'], inplace= True)

# ë°ì´í„°ì…‹ ê°¯ìˆ˜ í™•ì¸
print('ì¤‘ë³µ ì œê±° í›„ í•™ìŠµ ë°ì´í„°ì…‹ : {}'.format(len(train_data)))
print('ì¤‘ë³µ ì œê±° í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ : {}'.format(len(test_data)))
```

![]({{site.url}}/assets/images/a2688956.png)

ì¤‘ë³µë°ì´í„°ë¥¼ ì œê±°í•˜ê³  outlier ë°ì´í„°ë“¤ë„ í™•ì¸ì„ í•´ë´ì•¼ í•¨

```python
import numpy as np
import matplotlib.pyplot as plt

# null ë°ì´í„° ì œê±°
train_data.replace('', np.nan, inplace=True)
test_data.replace('', np.nan, inplace=True)

train_data = train_data.dropna(how = 'any')
test_data = test_data.dropna(how = 'any')

print('null ì œê±° í›„ í•™ìŠµ ë°ì´í„°ì…‹ : {}'.format(len(train_data)))
print('null ì œê±° í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ : {}'.format(len(test_data)))
```

![]({{site.url}}/assets/images/d8c34154.png)

null ë°ì´í„°ë„ ì œê±°í•´ë³´ì.  
ê·¸ë ‡ì§€ë§Œ ë³€í™”ëŠ” ì—†ì—ˆìŒ  
ìµœì¢…ì ìœ¼ë¡œ ì´ í•™ìŠµë°ì´í„°ëŠ” 15183ê°œì˜ ë¬¸ì¥ pair sentence ë¥¼ ì´ìš©í• ê±°ê³  ì•½ 4000 ë¬¸ì¥ì„ ì´ìš©í•´ì„œ ëª¨ë¸ì´ ì˜¬ë°”ë¥´ê²Œ í•™ìŠµì´
ëëŠ”ì§€ í‰ê°€ë¥¼ ì§„í–‰í•˜ë„ë¡ í•˜ê² ìŒ

ìƒ˜í”Œ ë°ì´í„°ëŠ” ì´ë ‡ê²Œ ìƒê²¼ìŒ

```python
print(train_data['sent_a'][0])
print(train_data['sent_b'][0])
print(train_data['label'][0])
```

![]({{site.url}}/assets/images/94dd5469.png)

```python
# í•™ìŠµ ì „ì œ ë¬¸ì¥ ê¸¸ì´ì¡°ì‚¬
print('í•™ìŠµ ì „ì œ ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ :',max(len(l) for l in train_data['sent_a']))
print('ì „ì œ ë¬¸ì¥ì˜ í‰ê·  ê¸¸ì´ :',sum(map(len, train_data['sent_a']))/len(train_data['sent_a']))

plt.hist([len(s) for s in train_data['sent_a']], bins=50)
plt.xlabel('length of data')
plt.ylabel('number of data')
plt.show()

# í•™ìŠµ ê°€ì • ë¬¸ì¥ ê¸¸ì´ì¡°ì‚¬
print('í•™ìŠµ ê°€ì • ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ :',max(len(l) for l in train_data['sent_b']))
print('ê°€ì • ë¬¸ì¥ì˜ í‰ê·  ê¸¸ì´ :',sum(map(len, train_data['sent_b']))/len(train_data['sent_b']))

plt.hist([len(s) for s in train_data['sent_b']], bins=50)
plt.xlabel('length of data')
plt.ylabel('number of data')
plt.show()
```

![]({{site.url}}/assets/images/f862c998.png)

ì „ì²´ ë¬¸ì¥ì˜ ê¸¸ì´ì™€ í‰ê·  ê¸¸ì´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŒ

ë³¸ê²©ì ìœ¼ë¡œ BERT ë¥¼ í™œìš©í•´ì„œ í•™ìŠµì„ ì§„í–‰í•´ë³´ì

```python
# Store the model we want to use
from transformers import AutoModel, AutoTokenizer, BertTokenizer
MODEL_NAME = "bert-base-multilingual-cased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
```

ì—¬ê¸°ì„œë¶€í„°ëŠ” ì´ì „ ì‹¤ìŠµê³¼ ê±°ì˜ ë™ì¼í•˜ì§€ë§Œ tokenzier ë¥¼ ë¶ˆëŸ¬ì˜¬ë•Œê°€ ë‹¤ë¦„  
ì•ì„œì„œ ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ task ì—ì„œëŠ” tokenizer ì— ë‹¨ì¼ë¬¸ì¥ í•˜ë‚˜ë§Œ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°”ìŒ  
ì´ë ‡ê²Œ ë‘ ë¬¸ì¥ì˜ ê´€ê³„ ë¶„ë¥˜ task ì—ì„œëŠ” ë¬¸ì¥ 2ê°œë¥¼ ì…ë ¥ìœ¼ë¡œ ë„£ê²Œë˜ë©´ ìë™ìœ¼ë¡œ tokenizer ê°€ [CLS] ë¬¸ì¥ A [SEP] ë¬¸ì¥ b [SEP] 
ì´ë ‡ê²Œ ë‹¤ í† í°ì„ ë¶€ì°©í•´ì„œ tokenizing ì„ í•´ì£¼ê³  ê·¸ë¦¬ê³  ê·¸ ì•ˆì—ì„œë„ token_type_ids ë¥¼ segmentA ëŠ” [0, 0, 0, ...] ê·¸ë¦¬ê³ 
segmentB ëŠ” [1, 1, 1, ...] ì´ë ‡ê²Œ ì´ì˜ê²Œ tagging ì„ í•´ì£¼ê²Œ ë¨

```python
tokenized_train_sentences = tokenizer(
    list(train_data['sent_a'][0:]),
    list(train_data['sent_b'][0:]),
    return_tensors="pt",
    padding=True,
    truncation=True,
    add_special_tokens=True,
    max_length=64
    )

print(tokenized_train_sentences[0])
print(tokenized_train_sentences[0].tokens)
print(tokenized_train_sentences[0].ids)
print(tokenized_train_sentences[0].attention_mask)
```

![]({{site.url}}/assets/images/5b2af6bf.png)

ì—¬ê¸°ê¹Œì§€í•˜ë©´ tokenizer ë¥¼ í†µí•´ì„œ í•™ìŠµë°ì´í„°ê°€ ì™„ì„±ì´ ë¨

ë§ˆì°¬ê°€ì§€ë¡œ í‰ê°€ë¥¼ ìœ„í•œ test data ë„ tokenizing ì„ ì§„í–‰í•˜ì

```python
tokenized_test_sentences = tokenizer(
    list(test_data['sent_a'][0:]),
    list(test_data['sent_b'][0:]),
    return_tensors="pt",
    padding=True,
    truncation=True,
    add_special_tokens=True,
    max_length=64
    )
```

label ë„ ë§Œë“¤ì

```python
train_label = train_data['label'].values[0:]
test_label = test_data['label'].values[0:]

print(train_label[0])
```

![]({{site.url}}/assets/images/97e48608.png)

ì˜ ì €ì¥ëœ ê±¸ í™•ì¸í•  ìˆ˜ ìˆìŒ

ì—¬ê¸°ì„œë¶€í„°ëŠ” ì´ì „ì˜ ì‹¤ìŠµ ë‚´ìš©ê³¼ ë‹¤ ë˜‘ê°™ìŒ

Dataset class ë¥¼ ì„ ì–¸í•´ì¤Œ

```python
class MultiSentDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)
```

`__getitem__` ì´ step ì´ ì§„í–‰ë¨ì— ë”°ë¼ì„œ ëª¨ë¸ì— ê³„ì†í•´ì„œ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ê²Œ ë˜ëŠ” ë°ì´í„°ì„

input ê°™ì€ ê²½ìš°ì—ëŠ” tokenizer ë¥¼ í†µí•´ì„œ ë‚˜ì˜¨ ê²°ê³¼ì˜ key, value ê°’ì„ ì…ë ¥ìœ¼ë¡œ ë„£ê²Œë˜ê³  label ì€ ì‚¬ì „ì— ì •ì˜í–ˆë˜
label ì„ ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ê°™ì´ ë„£ê²Œ ë¨

```python
train_dataset = MultiSentDataset(tokenized_train_sentences, train_label)
test_dataset = MultiSentDataset(tokenized_test_sentences, test_label)
```

train data ì™€ test data ë¥¼ dataset ìœ¼ë¡œ ë§Œë“¤ì–´ì¤Œ

ë§ˆì°¬ê°€ì§€ë¡œ BERT ë¥¼ í™œìš©í•´ì„œ training ì„ ì§„í–‰ì„ í• ê±´ë° ëª¨ë¸ ì…ì¥ì—ì„œë³´ë©´ ë‘ ë¬¸ì¥ì´ë“  í•œ ë¬¸ì¥ì´ë“  ì–´ì°¨í”¼ ì…ë ¥ìì²´ê°€
tokenized ëœ sentence ê°€ ë“¤ì–´ê°€ê²Œ ë˜ê³  ë§ˆì§€ë§‰ì— [CLS] í† í° í•˜ë‚˜ë§Œ ë¶„ë¥˜í•˜ê²Œë¨  
ê·¸ë ‡ê¸° ë•Œë¬¸ì— ìš°ë¦¬ê°€ ì‚¬ìš©í•  ëª¨ë¸ì€ ë‹¨ì¼ë¬¸ì¥ì—ì„œ ì‚¬ìš©í–ˆë˜ê²ƒê³¼ ë™ì¼í•œ `BertForSequenceClassification` ì„
ì´ê±°ë¥¼ ë¶ˆëŸ¬ë‹¤ê°€ ì‚¬ìš©í•˜ë©´ ì…ë ¥ëœ sequence ì— ëŒ€í•œ ë¶„ë¥˜ê°€ ê°€ëŠ¥í•´ì§€ê²Œ ë¨

```python
from transformers import BertForSequenceClassification, Trainer, TrainingArguments,  BertConfig

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=8,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    logging_dir='./logs',            # directory for storing logs
    logging_steps=500,
    save_total_limit=2,
)
```

ì•„ë˜ arguments ì˜µì…˜ë“¤ì€ ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ì—ì„œ í–ˆë˜ê²ƒê³¼ ë™ì¼í•˜ê²Œ ë˜ì–´ìˆìŒ

ê·¸ ë‹¤ìŒì€ model ì„ initialize í•´ì£¼ê³  GPU ë¡œ ì—…ë¡œë“œ í•´ì£¼ì 

```python
model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2) 
model.parameters
model.to(device)
```

ê·¸ ë‹¤ìŒì—ëŠ” í‰ê°€ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì£¼ì

```python
from sklearn.metrics import precision_recall_fscore_support, accuracy_score

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }
```

```python
trainer = Trainer(
    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=test_dataset,             # evaluation dataset
    compute_metrics=compute_metrics
)
```

`Trainer()` ì— `training_args` ë¥¼ ì…ë ¥ìœ¼ë¡œ ë„£ê³  ê·¸ ë‹¤ìŒì— train ì„ í•˜ê²Œë˜ë©´ í•™ìŠµì´ ë˜ê²Œ ë¨

```python
trainer.train()
```

í•™ìŠµì´ ëë‚˜ë©´ `evaluate()` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´ì„œ evaluation ì„ í•´ë³´ì

```python
trainer.evaluate(eval_dataset=test_dataset)
```

![]({{site.url}}/assets/images/729d602e.png)

accuracy ëŠ” 98ì  ê°€ê¹Œì´ë‚˜ì™”ê³  f1 ë„ 97ì ê¹Œì§€ ë‚˜ì˜¤ëŠ” ê²ƒì„ ë´ì„œ ëª¨ë¸ì´ êµ‰ì¥íˆ í•™ìŠµì´ ì˜ ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ  

```python
trainer.save_model('./results')
```

ëª¨ë¸ë„ ./results ë¼ëŠ” í´ë”ì— ì €ì¥í•´ë‘ 

ê·¸ëŸ¼ ì‹¤ì œë¡œ ì´ì œ í˜„ì‹¤ì„¸ê³„ì—ì„œ ì“°ëŠ” ë°ì´í„°ë¥¼ ì´ìš©í•´ì„œ test ë¥¼ í•´ë´ì•¼ í•¨

prediction í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ë³´ì

```python
# predictí•¨ìˆ˜
# 0: "non_similar", 1: "similar"
def sentences_predict(sent_A, sent_B):
    model.eval()
    tokenized_sent = tokenizer(
            sent_A,
            sent_B,
            return_tensors="pt",
            truncation=True,
            add_special_tokens=True,
            max_length=64
    )
    
    tokenized_sent.to('cuda:0')
    with torch.no_grad():# ê·¸ë¼ë””ì—”íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
        outputs = model(
            input_ids=tokenized_sent['input_ids'],
            attention_mask=tokenized_sent['attention_mask'],
            token_type_ids=tokenized_sent['token_type_ids']
            )

    logits = outputs[0]
    logits = logits.detach().cpu().numpy()
    result = np.argmax(logits)

    if result == 0:
      result = 'non_similar'
    elif result == 1:
      result = 'similar'
    return result
```

ë‹¨ì¼ ë¬¸ì¥ inference í•˜ëŠ” ë¶€ë¶„ê³¼ êµ‰ì¥íˆ ìœ ì‚¬í•˜ê²Œ ë˜ì–´ìˆê³  tokenizer ì— sentence ê°€ 2ê°œ ë“¤ì–´ê°€ëŠ”ê²ƒë§Œ ë‹¤ë¦„

```python
print(sentences_predict('ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?','ì˜¤ëŠ˜ì˜ ë‚ ì”¨ë¥¼ ì•Œë ¤ì¤˜')) # similar
print(sentences_predict('ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?','ê¸°ë¶„ ì§„ì§œ ì•ˆì¢‹ë‹¤.')) # non_similar
print(sentences_predict('ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?','ì˜¤ëŠ˜ ê¸°ë¶„ ì–´ë– ì„¸ìš”?')) # non_similar
print(sentences_predict('ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?','ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë•Œìš”?')) # non_similar
print(sentences_predict('ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?','ì§€ê¸ˆ ë‚ ì”¨ê°€ ì–´ë•Œìš”?')) # non_similar
print(sentences_predict('ë¬´í˜‘ ì†Œì„¤ ì¶”ì²œí•´ì£¼ì„¸ìš”.','ë¬´í˜‘ ì¥ë¥´ì˜ ì†Œì„¤ ì¶”ì²œ ë¶€íƒë“œë¦½ë‹ˆë‹¤.')) # similar
print(sentences_predict('ë¬´í˜‘ ì†Œì„¤ ì¶”ì²œí•´ì£¼ì„¸ìš”.','íŒíƒ€ì§€ ì†Œì„¤ ì¶”ì²œí•´ì£¼ì„¸ìš”.')) # non_similar
print(sentences_predict('ë¬´í˜‘ ì†Œì„¤ ì¶”ì²œí•´ì£¼ì„¸ìš”.','ë¬´í˜‘ ëŠë‚Œë‚˜ëŠ” ì†Œì„¤ í•˜ë‚˜ ì¶”ì²œí•´ì£¼ì‹¤ ìˆ˜ ìˆìœ¼ì‹¤ê¹Œìš”?')) # similar
```

![]({{site.url}}/assets/images/4918e850.png)

ê²°ê³¼ë¥¼ í•œë²ˆ ë³´ë©´ ì˜ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ

--- 

# ì‹¤ìŠµ

## BERT IRQA ê¸°ë°˜ì˜ ì±—ë´‡ ì‹¤ìŠµ

ì•ì„œì„œ ì‹¤ìŠµí–ˆë˜ê²Œ [CLS] í† í°ì„ ì´ìš©í•´ì„œ ë§Œë“  ì±—ë´‡ì´ ìˆì—ˆê³  ë‘ ë¬¸ì¥ì˜ ìœ ì‚¬ë„ë¥¼ ë¹„êµí•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì—ˆìŒ  
ì´ ë‘ê°€ì§€ ëª¨ë¸ì„ í•˜ë‚˜ë¡œ í•©ì³ì„œ ì±—ë´‡ì„ ë§Œë“¤ì–´ë³´ë„ë¡ í•˜ì

ë¨¼ì € ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œ ë°›ì ì´ ë°ì´í„°ëŠ” ì‹¬ë¦¬ìƒë‹´ê³¼ ê´€ë ¨ëœ ì±—ë´‡ ë°ì´í„°ì„

`!git clone https://github.com/songys/Chatbot_data.git`

git clone ì„ ì´ìš©í•´ ë‹¤ìš´ë¡œë“œ ë°›ê²Œ ë˜ë©´ 

```python
import pandas as pd
data = pd.read_csv('/content/Chatbot_data/ChatbotData.csv')

data.head()
```

![]({{site.url}}/assets/images/0e5734c4.png)

ë°ì´í„°ê°€ ì´ë ‡ê²Œ êµ¬ì„±ë˜ì–´ ìˆìŒ

ì…ë ¥ê³¼ ì¶œë ¥ì´ í•˜ë‚˜ë¡œ ëœ singleton ë°ì´í„°ì…‹ì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

![]({{site.url}}/assets/images/060fd2e8.png)

ìš°ë¦¬ê°€ í•˜ê³ ì í•˜ëŠ”ê²Œ ê°•ì˜ìë£Œì—ì„œ ì„¤ëª…í•œê²ƒê³¼ ë§ˆì°¬ê°€ì§€ë¡œ 1ì°¨ì ìœ¼ë¡œëŠ” [CLS] í† í°ì˜ ìœ ì‚¬ë„ë¥¼ ì´ìš©í•´ì„œ top-n ê°œë¥¼ ë½‘ë„ë¡ í•˜ì

top-n ê°œë¥¼ ë½‘ê³ ë‚˜ë©´ ê·¸ n ê°œë¥¼ í•˜ë‚˜ì”© í™•ì¸ì„ í•˜ë©° paraphrase detection ì„ í•˜ê²Œ ë˜ëŠ” ê²ƒì„

ê·¸ë˜ì„œ paraphrase detection í–ˆì„ ë•Œ ì •ë§ë¡œ question ê³¼ ë‚˜ì˜ query ê°€ ìœ ì‚¬í•œê²Œ í™•ì¸ì´ ëë‹¤ë©´ A ë‹µë³€ì„ ë°˜í™˜í•˜ê²Œë˜ëŠ”
ê·¸ëŸ° êµ¬ì¡°ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŒ

```python
import torch
from transformers import AutoModel, AutoTokenizer

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

MODEL_NAME = "bert-base-multilingual-cased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(MODEL_NAME)
model.to(device)
```

"bert-base-multilingual-cased" ëª¨ë¸ì„ í™œìš©í•´ì„œ [CLS] í† í°ì˜ sentence embedding ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ í•˜ê² ìŒ

ë°ì´í„°ë¥¼ í™•ì¸í•´ë³´ì

```python
chatbot_Question = data['Q'].values
chatbot_Answer = data['A'].values
print(chatbot_Question[0:3])
print(chatbot_Answer[0:3])
```

ì´ì „ì— [CLS] í† í°ì„ ì´ìš©í•œ ì±—ë´‡ì„ ê°œë°œí•  ë•Œ question ê³¼ answer ë¥¼ ì „ë¶€ ë°°ì—´í˜•íƒœë¡œ ì €ì¥í–ˆì—ˆìŒ  
ë§ˆì°¬ê°€ì§€ë¡œ ì±—ë´‡ question ì— ëŒ€í•´ì„œ value ë“¤ì„ ë¦¬ìŠ¤íŠ¸í˜•íƒœë¡œ ë‹¤ ì €ì¥í•´ë‘ì

```python
def get_cls_token(sent_A):
    model.eval()
    tokenized_sent = tokenizer(
            sent_A,
            return_tensors="pt",
            truncation=True,
            add_special_tokens=True,
            max_length=32
    ).to(device)
    with torch.no_grad():# ê·¸ë¼ë””ì—”íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
        outputs = model(
            input_ids=tokenized_sent['input_ids'],
            attention_mask=tokenized_sent['attention_mask'],
            token_type_ids=tokenized_sent['token_type_ids']
            )
    logits = outputs.last_hidden_state[:,0,:].detach().cpu().numpy()
    return logits
```

[CLS] í† í°ì„ ì–»ëŠ” ê²ƒì€ `get_cls_token()` í•¨ìˆ˜ë¥¼ í†µí•´ì„œ í† í°ì„ ì–»ë„ë¡ í•˜ì

ê·¸ëŸ¬ë©´ ì²«ë²ˆì§¸ë¡œ í•´ì•¼í• ê²ƒì€ ì‚¬ì „ì— ì •ì˜ë˜ì—ˆë˜ ë°ì´í„°ì…‹ ê·¸ ë°ì´í„°ì…‹ì— ì €ì¥ëœ ëª¨ë“  sentence ì— ëŒ€í•œ vector ë¥¼ ë˜ë‹¤ì‹œ 
ì–»ì–´ì•¼ í•¨

ê·¸ë˜ì•¼ì§€ ìƒˆë¡œìš´ question ì´ ë“¤ì–´ì™”ì„ ë•Œ ê·¸ ë²¡í„° ë¦¬ìŠ¤íŠ¸ì™€ ë¹„êµë¥¼í•˜ë©° ê°€ì¥ ìœ ì‚¬í•œ ë²¡í„°ë¥¼ íšë“í•  ìˆ˜ ìˆìŒ

ê·¸ë˜ì„œ ì±—ë´‡ question ì— ìˆëŠ” sentence ë¥¼ ë°˜ë³µë¬¸ì„ ëŒë©´ì„œ [CLS] í† í°ì˜ embedding ê²°ê³¼ 768 ì°¨ì›ì§œë¦¬ ë²¡í„°ë¥¼ ì „ë¶€ ì €ì¥
í•˜ë„ë¡í•˜ì

```python
chatbot_Question_vectors = {}
for i, question in enumerate(chatbot_Question):
    chatbot_Question_vectors[i] = get_cls_token(question)
```

ì´ë ‡ê²Œ í•˜ë©´ `chatbot_Question_vectors` ì•ˆì— ì±—ë´‡ question ì— ëŒ€í•œ ëª¨ë“  sentence embedding ë²¡í„°ê°€ ë“¤ì–´ê°€ ìˆê²Œ ë¨

ì´ì œ í•„ìš”í•œ ê²ƒì€ ìš°ë¦¬ì˜ query ê°€ ë“¤ì–´ê°”ì„ ë•Œ ê¸°ì¡´ ë°ì´í„°ì…‹ì— ìˆë˜ sentence embedding ê²°ê³¼ì™€ ê°€ì¥ ìœ ì‚¬í•œ top-n ê°œë¥¼ ì¶œë ¥í•´ë‚´ëŠ”
ê·¸ëŸ° í•¨ìˆ˜ê°€ í•„ìš”í•¨

```python
import numpy as np

def custom_cosine_similarity(a,b):
    numerator = np.dot(a,b.T)
    a_norm = np.sqrt(np.sum(a * a))
    b_norm = np.sqrt(np.sum(b * b, axis=-1))

    denominator = a_norm * b_norm
    return numerator/denominator
```

ê·¸ë˜ì„œ cosine_similarity ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ê³  (ë¬¼ë¡  sklearn ì— ìˆëŠ” í•¨ìˆ˜ë¥¼ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•´ë„ ê´œì°®ìŒ)

```python
def return_top_n_idx(question, n):
    question_vector = get_cls_token(question)
    sentence_similarity = {}
    for i in chatbot_Question_vectors.keys():
        ir_vector = chatbot_Question_vectors[i]
        similarity = custom_cosine_similarity(question_vector, ir_vector)
        sentence_similarity[i] = similarity
    
    sorted_sim = sorted(sentence_similarity.items(), key=lambda x: x[1], reverse=True)
    return sorted_sim[0:n]
```

top-n ê°œë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¦  
ì´ê²ƒë„ ì´ì „ì— í•™ìŠµë°ì´í„°ë¥¼ êµ¬ì¶•í•  ë•Œ ë§Œë“¤ì—ˆë˜ ì½”ë“œì™€ ë™ì¼í•¨  
question ì´ ë“¤ì–´ì˜¤ê³  n ê°œë¥¼ ì§€ì •í•´ì£¼ë©´ question ì— ëŒ€í•œ vector ë¥¼ êµ¬í•˜ê²Œ ë˜ê³  ê¸°ì¡´ì— ì €ì¥ë˜ì–´ ìˆë˜ `chatbot_Question_vectors`
ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ë²¡í„°ë¥¼ ê°€ì ¸ì˜¤ê²Œ ë¨  
ì—­ì‹œ ì •ë ¬ì„í•˜ê³  ì •ë ¬ëœ ê±°ì—ì„œ top-n ê°œë¥¼ ë°˜í™˜í•˜ê²Œ ë˜ë©´ ìœ ì‚¬í•œ ì¸ë±ìŠ¤ê°€ ë‚˜ì˜¤ê²Œ ë¨

ì˜ˆë¥¼ ë“¤ì–´ì„œ text í•´ë³´ì

```python
print(return_top_n_idx("ì˜¤ëŠ˜ ë„ˆë¬´ í˜ë“¤ì–´", 5))  # top 5ê°œ question idë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
```

![]({{site.url}}/assets/images/ce03858d.png)

"ì˜¤ëŠ˜ ë„ˆë¬´ í˜ë“¤ì–´" ë¼ê³  ì…ë ¥ì„ ë„£ê³  n ì— 5ë¥¼ ë„£ì—ˆì„ ë•Œ ê°€ì¥ ìœ ì‚¬í•œ question ì´ ìœ ì‚¬ë„ì™€ í•¨ê»˜ ë°˜í™˜ì´ ë˜ê²Œ ë¨

ì§ì ‘ì ìœ¼ë¡œ ì–´ë–¤ê²Œ ë°˜í™˜ë˜ëŠ”ì§€ í™•ì¸í•´ë³´ì

```python
print('most similar questions')
for result in return_top_n_idx("ì˜¤ëŠ˜ ë„ˆë¬´ í˜ë“¤ì–´", 5):
    print(chatbot_Question[result[0]])
print('\nmost similar answers')
for result in return_top_n_idx("ì˜¤ëŠ˜ ë„ˆë¬´ í˜ë“¤ì–´", 5):
    print(chatbot_Answer[result[0]])
```

![]({{site.url}}/assets/images/846091c8.png)

ë³´í†µì€ top-1 ì˜ ê²°ê³¼ê°€ ë‹µë³€ìœ¼ë¡œ ë‚˜ê°€ê²Œ ë¨

í•˜ì§€ë§Œ top-1 ì˜ ê²°ê³¼ê°€ ì •ë‹µì´ ì•„ë‹Œ ê²½ìš°ë„ ìˆìŒ

```python
print('most similar questions')
for result in return_top_n_idx("ë„ˆ ì´ë¦„ì´ ë­ì•¼?", 5):
    print(chatbot_Question[result[0]])
print('\nmost similar answers')
for result in return_top_n_idx("ë„ˆ ì´ë¦„ì´ ë­ì•¼?", 5):
    print(chatbot_Answer[result[0]])
```

![]({{site.url}}/assets/images/7d9afeb4.png)

"ë„ˆ ì´ë¦„ì´ ë­ì•¼?" ë¼ê³  ë„£ê³  top-n ê°œì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ë´¤ìŒ  
ì´ ë•Œ top-1 ì´ "ìš°ì •ì´ ë­ì•¼?" ë‚˜ì™”ëŠ”ë° í˜•íƒœì ìœ¼ë¡œ êµ‰ì¥íˆ ìœ ì‚¬í•œ ë¬¸ì¥ì´ê¸°ì— BERT ì—ê²ŒëŠ” sentence embedding ì´ ìœ ì‚¬í•˜ê²Œ ë‚˜ì™€ì„œ 
ë‘ ë¬¸ì¥ì´ ë¹„ìŠ·í•œ ê²ƒ ê°™ë‹¤ê³  íŒë‹¨í•  ìˆ˜ ìˆìŒ  
ì‹¤ì œë¡œ ì˜ë¯¸ë¡ ì ìœ¼ë¡  ë‹¤ë¥´ê²Œ ë¨  
ì´ëŸ° ì˜ˆì œë“¤ ë•Œë¬¸ì— ê°€ì¥ ìœ ì‚¬í•œ ì§‘ë‹¨ì˜ paraphrase detection í•˜ëŠ” ëª¨ë¸ì„ ë¶€ì°©í•˜ê²Œ ë˜ëŠ” ê²ƒì„

ì´ ê²½ìš°ì—” "ìš°ì •ì´ ë­ì•¼?" ë¼ëŠ” ë‹µë³€ë³´ë‹¤ëŠ” "ë„ˆ ë­ë‹ˆ?" ë˜ëŠ” "ë„ˆ ëˆ„êµ¬?" ë¼ëŠ” ë¬¸ì¥ì´ ë” ì ì ˆí•´ë³´ì„

ê·¸ëŸ¬ë©´ ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ top-n ê°œë¥¼ ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ì— íƒœì›Œì„œ ì‹¤ì œë¡œ ìœ ì‚¬í•œì§€ ì•„ë‹Œì§€ë¥¼ ê²€ì‚¬í•˜ëŠ” task ë¥¼ ì§„í–‰í•˜ë„ë¡ í•˜ì

ì´ì „ í•™ìŠµì—ì„œ ì‚¬ìš©í•œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ ì‚¬ìš©í•˜ê² ìŒ

```python
from transformers import BertForSequenceClassification

MODEL_NAME = "ë³¸ì¸ì˜ paraphrase detection ëª¨ë¸ íŒŒì¼ ì €ì¥ ìœ„ì¹˜"
classifier_model = BertForSequenceClassification.from_pretrained(MODEL_NAME)
classifier_model.to(device)
```

ì•ì„œì„œ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ í™•ì¸í–ˆë˜ ê²ƒê³¼ ë§ˆì°¬ê°€ì§€ë¡œ inference ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•˜ì

```python
# predictí•¨ìˆ˜
# 0: "non_similar", 1: "similar"
def sentences_predict(sent_A, sent_B):
    classifier_model.eval()
    tokenized_sent = tokenizer(
            sent_A,
            sent_B,
            return_tensors="pt",
            truncation=True,
            add_special_tokens=True,
            max_length=64
    )
    
    tokenized_sent.to('cuda:0')
    with torch.no_grad():# ê·¸ë¼ë””ì—”íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
        outputs = classifier_model(
            input_ids=tokenized_sent['input_ids'],
            attention_mask=tokenized_sent['attention_mask'],
            token_type_ids=tokenized_sent['token_type_ids']
            )

    logits = outputs[0]
    logits = logits.detach().cpu().numpy()
    result = np.argmax(logits)

    # if result == 0:
    #   result = 'non_similar'
    # elif result == 1:
    #   result = 'similar'
    return result
```

ì´ ë•Œ sent_A ì™€ sent_B ê°€ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¤ê²Œ ë˜ê³  ì—­ì‹œ tokenizer ì— ë‘ ë¬¸ì¥ì´ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ê²Œ ë¨  
ê·¸ ë‹¤ìŒì— softmax ê²°ê³¼ê°€ 0ë²ˆ ë ˆì´ë¸”ì´ ë†’ëƒ 1ë²ˆ ë ˆì´ë¸”ì´ ë†’ëƒë¥¼ result ë¥¼ í†µí•´ì„œ ë°˜í™˜í•˜ê²Œ ë¨  

```python
print(sentences_predict('ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?','ì˜¤ëŠ˜ì˜ ë‚ ì”¨ë¥¼ ì•Œë ¤ì¤˜')) # similar
print(sentences_predict('ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?','ê¸°ë¶„ ì§„ì§œ ì•ˆì¢‹ë‹¤.')) # non_similar
print(sentences_predict('ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?','ì˜¤ëŠ˜ ê¸°ë¶„ ì–´ë– ì„¸ìš”?')) # non_similar
print(sentences_predict('ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?','ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë•Œìš”?')) # non_similar
print(sentences_predict('ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?','ì§€ê¸ˆ ë‚ ì”¨ê°€ ì–´ë•Œìš”?')) # non_similar
print(sentences_predict('ë¬´í˜‘ ì†Œì„¤ ì¶”ì²œí•´ì£¼ì„¸ìš”.','ë¬´í˜‘ ì¥ë¥´ì˜ ì†Œì„¤ ì¶”ì²œ ë¶€íƒë“œë¦½ë‹ˆë‹¤.')) # similar
print(sentences_predict('ë¬´í˜‘ ì†Œì„¤ ì¶”ì²œí•´ì£¼ì„¸ìš”.','íŒíƒ€ì§€ ì†Œì„¤ ì¶”ì²œí•´ì£¼ì„¸ìš”.')) # non_similar
print(sentences_predict('ë¬´í˜‘ ì†Œì„¤ ì¶”ì²œí•´ì£¼ì„¸ìš”.','ë¬´í˜‘ ëŠë‚Œë‚˜ëŠ” ì†Œì„¤ í•˜ë‚˜ ì¶”ì²œí•´ì£¼ì‹¤ ìˆ˜ ìˆìœ¼ì‹¤ê¹Œìš”?')) # similar
print(sentences_predict('ë©”ë‚œë¯¼ì´ ë­ì•¼','ë„ˆ ë©”ë‚œë¯¼ì´ì§€?')) # similar
```

![]({{site.url}}/assets/images/59e9ebb1.png)

ê·¸ëŸ¬ë©´ ë‘ ëª¨ë“ˆì„ í•˜ë‚˜ë¡œ í•©ì³ë³´ì

pipeline ì„ í•©ì¹œë‹¤ê³  ë³´ë©´ ë¨

`get_answer()` ë¼ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì

```python
def get_answer(question, n):
    results = return_top_n_idx(question, n) # top nê°œë¥¼ listë¡œ ë°›ê³ 
    for result in results:  # nê°œë¥¼ ë°˜ë³µë¬¸ì„ ëŒë©´ì„œ
        ir_answer = chatbot_Answer[result[0]]
        ir_question = chatbot_Question[result[0]]
        if sentences_predict(question, ir_question) == 1:   # ì´ì§„ë¶„ë¥˜ ëª¨ë¸ì´ query<->questionì˜ ì˜ë¯¸ê°€ ì„œë¡œ ê°™ë‹¤ê³  íŒë‹¨ë˜ë©´?
            return ir_answer    # ì •ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    return chatbot_Answer[results[0][0]]    # "ì˜ ëª¨ë¥´ê² ì–´ìš”."
```

ì‚¬ìš©ìì˜ question ê³¼ top-n ê°œë¥¼ ëª‡ê°œê¹Œì§€ ë³¼ ê²ƒì¸ê°€ ì¸ n ì„ ì…ë ¥ìœ¼ë¡œ ë°›ê²Œ ë¨  
ì²«ë²ˆì§¸, ë‚˜ì˜ question ê³¼ ê°€ì¥ ìœ ì‚¬í•œ question list ë¥¼ ë°˜í™˜ì„ ë°›ìŒ ì´ë•Œ, ì•„ê¹Œ ë§Œë“¤ì—ˆë˜ `return_top_n_idx()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©  
ê·¸ëŸ¬ë©´ results  ì•ˆì— top-n ê°œì˜ ê²°ê³¼ê°’ì´ ì €ì¥ë¨  
ì´ë•Œ results ë¥¼ ë°˜ë³µë¬¸ì„ ëŒë©´ì„œ result ì˜ question ì„ ê°€ì ¸ì˜¤ê³  ê·¸ question ì„ ë‚´ê°€ ë§Œë“  question ì´ 2ê°œì˜ ìœ ì‚¬ë„ê°€ 
1ì¸ì§€ 0ì¸ì§€ë¥¼ íŒë‹¨í•˜ê²Œ ë¨  
ë§Œì•½ 2ê°œê°€ ìœ ì‚¬í•˜ë‹¤ë©´ question ê³¼ ë§µí•‘ë˜ì–´ìˆë˜ ë‹µë³€ì„ ë°˜í™˜í•˜ê²Œ ë¨  
ë§Œì•½ ë°˜ë³µë¬¸ì„ ë‹¤ ëŒê³ ë‚˜ì„œë„ ë‹µë³€ì´ ì˜¬ë°”ë¥´ê²Œ ë‚˜ì˜¤ì§€ ì•Šì•˜ë‹¤ë©´ ê·¸ ë‹¤ìŒ ì •ì±…ì„ ì„¸ìš¸ ìˆ˜ ìˆìŒ  
ì´ ë•Œ ì„¸ìš´ ì •ì±…ì€ top-1 ì„ ë°˜í™˜í•˜ê²Œ ë˜ëŠ” ê²ƒì„  
ì´ë ‡ê²Œ ë§Œë“¤ë©´ "ì˜ ëª¨ë¥´ê² ì–´ìš”" ë¼ê¸° ë³´ë‹¤ëŠ” ë¬´ì–¸ê°€ ë‹µë³€ì´ ë‚˜ì˜¤ê²Œ ë¨  
ê·¸ëŸ¬ë‹ˆê°€ "ì˜ ëª¨ë¥´ê² ì–´ìš”" ë³´ë‹¤ëŠ” ì–´ë–¤ ë‹µë³€ì´ë¼ë„ ë­”ê°€ ë§Œë“¤ì–´ë‚´ëŠ” ê²ƒì„  
ë§Œì•½ì— ì´ëŸ° ë§ë„ ì•ˆë˜ëŠ” ë‹µë³€ì€ ì›ì¹˜ì•ŠëŠ”ë‹¤ë¼ê³  í•˜ë©´ "ì˜ ëª¨ë¥´ê² ì–´ìš”" ë¼ê³  ë‹µë³€ì´ ë‚˜ì˜¤ëŠ”ê²Œ ë” ë§˜ì—ë“ ë‹¤ë©´  
ë§ˆì§€ë§‰ ì¤„ return ì„ "ì˜ ëª¨ë¥´ê² ì–´ìš”" ë¼ê³  ë°˜í™˜í•´ì£¼ë©´ top-n ê°œ ë‚´ì—ì„œ ì •ë‹µì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ "ì˜ ëª¨ë¥´ê² ì–´ìš”" ë¼ëŠ” 
ë‹µë³€ì´ ì¶œë ¥ì´ ë  ê²ƒì„

í•œë²ˆ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë³´ì

```python
print(get_answer("ë„ˆ ì´ë¦„ì´ ë­ì•¼?", 5))
```

![]({{site.url}}/assets/images/367e5280.png)

```python
print(get_answer("ë‚˜ ì§€ê¸ˆ ë„ˆë¬´ ìš°ìš¸í•´", 5))
```

![]({{site.url}}/assets/images/625db503.png)

```python
print(get_answer("ì˜¤ëŠ˜ ê¸°ë¶„ ì–´ë•Œ?", 5))
```

![]({{site.url}}/assets/images/a0f63d33.png)

```python
print(get_answer("ë°”ìœê°€ë³´ë„¤?", 5))
```

![]({{site.url}}/assets/images/38c607f3.png)

```python
print(get_answer("ì–´ë–»ê²Œ í™•ì¸í•˜ëŠ”ë°?", 5))
```

![]({{site.url}}/assets/images/bff69088.png)

ë³´í†µ ììœ ëŒ€í™”ê°€ ê°€ëŠ¥í•œ ì±—ë´‡ì´ë¼ê³  í•˜ë©´ ì „ë¶€ ìƒì„±ëª¨ë¸ë¡œ ìƒìƒí•  ìˆ˜ ìˆì„í…ë° ë°˜ë“œì‹œ ì–´ë–¤ ì§ˆë¬¸ì— ëŒ€í•´ ë°˜ë“œì‹œ ë‚˜ì™€ì•¼í•˜ëŠ” ë‹µë³€ì„
ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ì „ ì •ì˜ë¥¼ í•´ë†“ê³  ì´ëŸ° IRQA ëª¨ë“ˆì„ íƒœì›Œì„œ 1ì°¨ì ìœ¼ë¡œëŠ” IRQA ë‹µë³€ì„ íƒœìš°ê³  ë§Œì•½ì— IRQA ì—ì„œ ë‹µë³€í•  ìˆ˜ ì—†ëŠ”
ì§ˆë¬¸ì´ ë“¤ì–´ì™”ë‹¤ë©´ ê·¸ ë‹¤ìŒì— ìƒì„±ëª¨ë¸ì„ í†µí•´ ë‹µë³€ì„ ë§Œë“¤ì–´ë‚´ëŠ” ê·¸ëŸ° ê³¼ì •ìœ¼ë¡œ ì±—ë´‡ì„ êµ¬í˜„í•¨

> ì‘ì„±ì      
```
* ê¹€ì„±í˜„ (bananaband657@gmail.com)  
1ê¸° ë©˜í† 
ê¹€ë°”ë‹¤ (qkek983@gmail.com)
ë°•ìƒí¬ (parksanghee0103@gmail.com)  
ì´ì •ìš° (jungwoo.l2.rs@gmail.com)
2ê¸° ë©˜í† 
ë°•ìƒí¬ (parksanghee0103@gmail.com)  
ì´ì •ìš° (jungwoo.l2.rs@gmail.com)
ì´ë…•ìš° (leenw2@gmail.com)
ë°•ì±„í›ˆ (qkrcogns2222@gmail.com)
```
[CC BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/2.0/kr/)

@inproceedings{cho2020discourse,
  title={Discourse Component to Sentence (DC2S): An Efficient Human-Aided Construction of Paraphrase and Sentence Similarity Dataset},
  author={Cho, Won Ik and Kim, Jong In and Moon, Young Ki and Kim, Nam Soo},
  booktitle={Proceedings of The 12th Language Resources and Evaluation Conference},
  pages={6819--6826},
  year={2020}
}
