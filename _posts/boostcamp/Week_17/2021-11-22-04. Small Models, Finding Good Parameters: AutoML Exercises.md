---
title: "Day_75 04. 작은 모델, 좋은 파라미터 찾기: AutoML 실습"

categories:
  - Boostcamp_AI_Tech/Week_17
tags:
  - 모델최적화
---
  
# 작은 모델, 좋은 파라미터 찾기: AutoML 실습

## 1. Overview

### 1.1 Review

**AutoML: 기준 성능을 잘 만족하는 적절한 모델, 파라미터 탐색**

![]({{site.url}}/assets/images/boostcamp/c45334d2.png)

소요 시간과 scalability 등의 현실적 문제

### 1.2 Objective

**지난 강의: 충분히 좋은 configuration 찾기**

- 어느정도의 prior 를 개입, 적은 search space 를 잡고,
- 적지만, 대표성을 띄는 좋은 subset 데이터를 정하고(+ n-fold Cross validation 등의 테크닉)
- 학습 과정의 profile 을 보고 early terminate 하는 기법 적용
  - ASHA Scheduler, BOHB(Bayesian Optimization & Hyperband)

등등의 방법으로 Human in the loop 의 결과보다 **"충분히 좋은"** configuration 을 찾을 수 있습니다.

Main Focus $\rightarrow$ 어느정도의 prior 를 개입, 적은 search space 를 잡고

## 2. 코드: Sample 파트

### 2.1 이론과 코드의 연결

**Overview**

- Optuna API 활용(https://optuna.org/)
  - SOTA 알고리즘 구현, 병렬화 용이, Conditional(⭐️) 파라미터 구성 용이
- 과정 overview
  1. Optuna Study 생성(blackbox optimizer 및 관리 담당)
  2. Study 에 최적화할 목적함수 및 시도 횟수, 조건 등을 주고,
  3. Optimize!

![]({{site.url}}/assets/images/boostcamp/b8badfe1.png)

**구현(끼워맞추기)**

![]({{site.url}}/assets/images/boostcamp/f472cb2e.png)

코드로 구현하면

![]({{site.url}}/assets/images/boostcamp/7d4220e0.png)

**구현(실제)**

![]({{site.url}}/assets/images/boostcamp/0a0a6a2c.png)

### 2.2 Architecture config

![]({{site.url}}/assets/images/boostcamp/76cc1a30.png)

**다양한 Backbone model**

![]({{site.url}}/assets/images/boostcamp/2f2a5ef8.png)

특정 data structure 에 block 들을 넣어서 모델을 "조립" 할 수 있을까? = block 들을 임의로 쌓아서 모델을 "생성" 할 수 있을까?

![]({{site.url}}/assets/images/boostcamp/2d6dacb4.png)

![]({{site.url}}/assets/images/boostcamp/7132d8d8.png)

![]({{site.url}}/assets/images/boostcamp/7472303f.png)

**Prerequisite: Optuna search space[1]**

1. Categorical
  ![]({{site.url}}/assets/images/boostcamp/976ab742.png)
2. Continuous
  ![]({{site.url}}/assets/images/boostcamp/be55d1c6.png)
3. Integer
  ![]({{site.url}}/assets/images/boostcamp/9d518d80.png)
4. Conditional(⭐️)
  ![]({{site.url}}/assets/images/boostcamp/834aade9.png)

**Custom search space 구성하기: 예시**

![]({{site.url}}/assets/images/boostcamp/e272248b.png)

- Research Topic 중 하나인 Neural Architecture Search(NAS) 논문
- 모듈 block 들의 조합 및 구성(macro)을 탐색하는 것이 아닌, 모듈 block(micro)을 탐색하는 것에 초점(주요 연구 방향)  
$\rightarrow$ 성능은 좋지만, 아직은 높은 Computational cost
- 아래처럼 모듈 block 으로 생성된 네트워크 구조는 고정함

  ![]({{site.url}}/assets/images/boostcamp/02ebb762.png)

Application 레벨에서는?

이미 나와있는 좋은 모듈 block 들로 + macro 한 구조는 오른쪽 이미지를 차용

**예시 1. 나의 작고 이상한 첫번째 model: 고정된 N, NC, RC**

- N, NC, RC 각각 고정 파라미터로 sample, 모델을 구성

  ![]({{site.url}}/assets/images/boostcamp/9589a622.png)

몇번 반복할 건지 N 을 sample 하고 Normal Cell(NC) 을 결정

후보로는 `["Conv", "DWConv", "Bottleneck", "InvertedResidualv2", "MBConv"]` 이 중에서 그냥 고를거임

NC 로 "Conv" 가 선택된 경우에는 out_channel 을 16, 64 사이에서 랜덤하게 샘플하고, kernel_size 도 1, 3, 5 중에서 선택하고 activation 도
`["ReLU", "ReLU6", "Hardswish"]` 중에서 선택

Reduction Cell(RC) 를 만듦

후보로는 `["InvertedResidualv2", "InvertedRedisidualv3", "MaxPool", "AvgPool"]` 로 잡아놨음

stride 를 줘서 feature map 을 줄여야하니 stride 를 고정시켜놓되 들어가는 parameter 를 바꾸는 식으로 구현

> 레이어가 깊어질 수록 더 많은 feature 를 뽑아야 더 잘되지 않나?
>
> $\rightarrow$ N 을 깊어질수록 더 크게 만들자!

- N, NC, RC 각각 고정 파라미터로 sample, 모델을 구성, N 은 점점 커지게!(1)

  ![]({{site.url}}/assets/images/boostcamp/21ed9cc3.png)

- N, NC, RC 각각 고정 파라미터로 sample, 모델을 구성, N 은 점점 커지게!(2)

  ![]({{site.url}}/assets/images/boostcamp/9b1d15c8.png)

**예시 2. 나의 N 번째 model: 가변, N, NC, RC**

- 동일한 방법으로 탐색의 범위를 넓힐 수 있음

  ![]({{site.url}}/assets/images/boostcamp/2f94c836.png)

**예시 3. 나의 N + 1 번째 model**

- (형식과 조건만 맞추면) 더욱 자유로운 모델도

  ![]({{site.url}}/assets/images/boostcamp/527f12fe.png)

### 2.3 Hyperparam config

**이제는 sampling 에 친숙할 때**

![]({{site.url}}/assets/images/boostcamp/390ba00a.png)

고정할 것은 고정, 원하는 변수는 자유롭게 추가

## 3. 코드: Parse 파트

Config 로부터 모델과 학습 파라미터를 셋업하는 코드를 살펴봅니다.

![]({{site.url}}/assets/images/boostcamp/d2170d9f.png)

### 3.1 yam 에서 Model 만들기 구조

![]({{site.url}}/assets/images/boostcamp/f1c8af57.png)

![]({{site.url}}/assets/images/boostcamp/c4a6fa91.png)

![]({{site.url}}/assets/images/boostcamp/39e2dc93.png)

![]({{site.url}}/assets/images/boostcamp/8a54cc3d.png)

![]({{site.url}}/assets/images/boostcamp/eddf5a31.png)

### 3.2 yaml 에서 Model 만들기 parsing 코드

![]({{site.url}}/assets/images/boostcamp/68d5c472.png)

### 3.2 yaml 에서 Model 만들기 parsing 코드 - 반복 횟수

![]({{site.url}}/assets/images/boostcamp/8b16f22f.png)

depth_multiply : 깊이를 조절하겠다는 의미

### 3.2 yaml 에서 Model 만들기 parsing 코드 - 모듈 생성기

![]({{site.url}}/assets/images/boostcamp/0a343bbb.png)

![]({{site.url}}/assets/images/boostcamp/caa6d7a3.png)

![]({{site.url}}/assets/images/boostcamp/b828ee81.png)

## 4. 코드: 모듈 추가하기

### 4.1 모듈 파트 구조

**File Tree**

![]({{site.url}}/assets/images/boostcamp/8d2b032f.png)

**Module generator(Interface 또는 abstract class)**

- 각 모듈을 parsing 하기 위해서 필요한 최소 기능 구현

  ![]({{site.url}}/assets/images/boostcamp/d49dafd8.png)

### 4.2 모듈 파트 예시: Simplified Bottleneck

**Bottleneck.py 예시: 1. 모듈 구현**

![]({{site.url}}/assets/images/boostcamp/e4b44659.png)

**Bottleneck.py 예시: 2. ModuleGenerator**

![]({{site.url}}/assets/images/boostcamp/b1498550.png)

### 4.3 모듈 추가하기: Inverted Residual v2

**모듈 추가 과정**

1. 구현체 추가(https://github.com/pytorch/vision/blob/master/torchvision/models/mobilenetv2.py)
  ![]({{site.url}}/assets/images/boostcamp/16d44a49.png)

2. Module generator 인터페이스 작업
  ![]({{site.url}}/assets/images/boostcamp/94fc0d12.png)
  ![]({{site.url}}/assets/images/boostcamp/ca4b3e9d.png)

4. `__init__.py` 에 추가(생략)


















# Further Reading

- [[Code] pytorch/vision](https://github.com/pytorch/vision)
- [Datasets, Transforms and Models specific to Computer Vision - pytorch/vision Optuna](https://optuna.org/)




