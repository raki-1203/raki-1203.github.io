---
title: "Day_82 02. ì°¢ì€ ëª¨ë¸ ê¾¸ê²¨ ë„£ê¸°: Quantization ì‹¤ìŠµ(with torch, tensorrt)"

categories:
  - Boostcamp_AI_Tech/Week_18/Day_82
tags:
  - ëª¨ë¸ìµœì í™”
---
  
# ì°¢ì€ ëª¨ë¸ ê¾¸ê²¨ ë„£ê¸°: Quantization ì‹¤ìŠµ(with torch, tensorrt)

## 1. Quantization ì¢…ë¥˜

### 1.1 Review & Overview

**Quantization**

- ê¸°ì¡´ì˜ high precision(ì¼ë°˜ì ìœ¼ë¡œ fp32) Neural network ì˜ weights ì™€ activation ì„ ë” ì ì€ bit(low precision)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒ
- Quantized Matrix Multiplication, Activation, Layer fusion, ...

![]({{site.url}}/assets/images/boostcamp/2300e2b7.png)

**Quantization approach êµ¬ë¶„**

- Post Training Quantization(PTQ): í•™ìŠµ í›„ì— quantization parameter(scale, shift)ë¥¼ ê²°ì •
- Quantization Aware Training(QAT): í•™ìŠµ ê³¼ì •ì— quantization ì„ emulate í•¨ìœ¼ë¡œì¨, quantization ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ì„±ëŠ¥í•˜ë½ì„ ì™„í™”í•¨

### 1.2 Post Training Quantization(PTQ)

**PTQ ê¸°ë²• ì •ë¦¬**

- Dynamic range quantization(weight only quantization): weight ë§Œ quantize ë¨(8-bit), inference ì‹œì—ëŠ” floating-point ë¡œ 
ë³€í™˜ë˜ì–´ ìˆ˜í–‰
- Full integer quantization(weight and activation quantization): weight ì™€ ë”ë¶ˆì–´ ëª¨ë¸ì˜ ì…ë ¥ ë°ì´í„°, activation(ì¤‘ê°„ ë ˆì´ì–´ì˜
output)ë“¤ ë˜í•œ quantize
- Float 16 quantization: fp32 ì˜ ë°ì´í„° íƒ€ì…ì˜ weight ë¥¼ fp16 ìœ¼ë¡œ quantize

![]({{site.url}}/assets/images/boostcamp/fd8134a0.png)

**ì–´ë–¨ë•Œ ì–´ë–¤ ê¸°ë²•ì„ ì ìš©í•´ì•¼í• ê¹Œ?(From TF[2])**

![]({{site.url}}/assets/images/boostcamp/ba83d043.png)

![]({{site.url}}/assets/images/boostcamp/1939e39c.png)

**ì°¸ê³ : PyTorch ì˜ êµ¬ë¶„**

- Dynamic range quantization(Dynamic quantization)ì„ ë³„ë„ì˜ ë²”ì£¼ë¡œ êµ¬ë¶„
- Post Training Quantization ì€ Static quantization ì´ë¼ê³ ë„ ì¹­í•¨
- Dynamic quantization ì˜ ê²½ìš°, model ìˆ˜í–‰ ì‹œê°„ì´ weights ë¥¼ load í•˜ëŠ” ê²ƒì´ ì‹¤ì œ matrix multiplication ë³´ë‹¤ ë” ì˜¤ë˜ ê±¸ë¦¬ëŠ”
LSTM, Transformer ê¸°ë°˜ì˜ ëª¨ë¸ì— íš¨ê³¼ì ì´ë¼ëŠ” ì–¸ê¸‰ì´ ìˆìŒ

![]({{site.url}}/assets/images/boostcamp/870439aa.png)

**1) Dynamic range quantization(weight only quantization)**

- ë„¤íŠ¸ì›Œí¬ì˜ Weight ë§Œ quantize ë¨(8bit)
- Pros ğŸ˜:
  - ë³„ë„ì˜ calibration(validation) ë°ì´í„°ê°€ í•„ìš”í•˜ì§€ ì•ŠìŒ
  - ëª¨ë¸ì˜ ìš©ëŸ‰ ì¶•ì†Œ(8bit ê¸°ì¤€ 1/4)
- Cons ğŸ˜¥:
  - ì‹¤ì œ ì—°ì‚°ì€ floating point ë¡œ ìˆ˜í–‰ë¨
  - ì‹¤ì œ ì†ë„ì˜ ì´ì ì´ í¬ì§€ ì•ŠìŒ

**2) Full integer quantization(weight and activation quantization)**

- Weight ì™€ ë”ë¶ˆì–´ ëª¨ë¸ì˜ ì…ë ¥ ë°ì´í„°, activation(ì¤‘ê°„ ë ˆì´ì–´ì˜ output)ë“¤ë„ quantize ë¨
- Pros ğŸ˜:
  - ëª¨ë¸ì˜ ìš©ëŸ‰ ì¶•ì†Œ(8bit ê¸°ì¤€ 1/4)
  - ë” ì ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, cache ì¬ì‚¬ìš©ì„± ì¦ê°€
  - ë¹ ë¥¸ ì—°ì‚°(fixed point 8bit ì—°ì‚°ì„ ì§€ì›í•˜ëŠ” ê²½ìš°)
- Cons ğŸ˜¥:
  - Activation ì˜ parameter ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•˜ì—¬ calibration ë°ì´í„°ê°€ í•„ìš”í•¨  
    (ì£¼ë¡œ training ë°ì´í„°ì—ì„œ ì‚¬ìš©, ì•½ 100ê°œì˜ ë°ì´í„°) $\rightarrow$ í•­ìƒì€ ì•„ë‹˜
  - TF ì—ì„œëŠ” 100ê°œ ì •ë„ë©´ ë˜ê³  tensorRT ì—ì„œëŠ” 1000ê°œ ì •ë„ í•„ìš”

- TensorRT ì˜ quantization ì€ bias(zero point)ê°€ ì—†ëŠ” Symmetric quantization  
Why? ì—°ì‚°ì´ í›¨ì”¬ ê°„ë‹¨í•˜ê³ , (ê²½í—˜ì ìœ¼ë¡œ) ì„±ëŠ¥ì— í° ì˜í–¥ì„ ì£¼ì§€ ì•Šì•˜ë‹¤

![]({{site.url}}/assets/images/boostcamp/43ef16b7.png)

**2) Full integer quantization; TensorRT(NVIDIA) calibration ì˜ˆì‹œ**

- Calibration?; ì„±ëŠ¥ ì €í•˜ë¥¼ ìµœì†Œë¡œ í•˜ëŠ” threshold ì°¾ê¸°

![]({{site.url}}/assets/images/boostcamp/f889e811.png)

tensorRT ì—ì„œëŠ” ì–´ëŠ ì •ë„ threshold ë¥¼ ì •í•´ì„œ ê·¸ ê°’ì„ ë„˜ì–´ê°€ëŠ” ê°’ë“¤ì€ cliping í˜¹ì€ saturation ì„ ì‹œì¼°ë”ë‹ˆ ì™¼ìª½ì— ëŒ€ë¹„í•´ì„œ ë” ì¢‹ì•˜ë‹¤

ëŒ€ì‹ ì— threshold ë¥¼ ì–´ë–»ê²Œ ì •í• ê¹Œê°€ ì´ìŠˆê°€ ë¨

- Minimize information loss ê´€ì ìœ¼ë¡œ ì ‘ê·¼
- ê° ë„¤íŠ¸ì›Œí¬, ê° ë ˆì´ì–´ ë§ˆë‹¤ activation value ì˜ range, distribution ì´ ë‹¤ë¥´ë‹¤  
  (x ì¶•: value, y ì¶•: normalized histogram counts)

![]({{site.url}}/assets/images/boostcamp/680bc8e1.png)

- "threshold ì—ì„œ saturated ëœ normalized histogram ë¶„í¬(ref_distr(P))" ì™€ 
"ì› histogram ìœ¼ë¡œë¶€í„° quantized, normalized ëœ ë¶„í¬(quant_distr(Q))" ì˜ KL div ê°€ ìµœì†Œì¸ ì§€ì ì„ ê²°ì •

![]({{site.url}}/assets/images/boostcamp/0441b3ee.png)

![]({{site.url}}/assets/images/boostcamp/acb3b3de.png)

![]({{site.url}}/assets/images/boostcamp/1213522a.png)

![]({{site.url}}/assets/images/boostcamp/e1abb796.png)

P ì— ìˆë˜ 0 ì€ Q ì—ì„œë„ 0ì´ë‹¤

P ì™€ Q ì˜ bin ê°œìˆ˜ë¥¼ ë§ì¶°ì£¼ëŠ” ì‘ì—…

- Calibration ë°©ë²•ë§ˆë‹¤ í•„ìš”í•œ ë°ì´í„°ì˜ í¬ê¸°ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆìŒ

![]({{site.url}}/assets/images/boostcamp/b29ca998.png)

- ê²°ê³¼ ì˜ˆì‹œ

![]({{site.url}}/assets/images/boostcamp/de2775bf.png)

ì™¼ìª½ histogram ë¶„í¬ê°€ ì´ë ‡ê²Œ ë˜ì–´ìˆì—ˆëŠ”ë° KL div ë¥¼ minimize í•˜ëŠ” ìœ„ì¹˜ê°€ í° ì„ ì´ë˜ê³  ì—¬ê¸°ê¹Œì§€ê°€ ê°’ì´ ë˜ëŠ”ê±°ê³  
í° ì„  ì˜¤ë¥¸ìª½ì— ìˆëŠ” ê°’ë“¤ì€ ëª°ë ¤ì„œ ë§¨ëì— ê°’ì´ count ê°€ ë§ì€ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ

**3) Float16 quantization**

- Float 32 ëª¨ë¸ì„ float 16 ëª¨ë¸ë¡œ ë³€í™˜
- Pros ğŸ˜:
  - ëª¨ë¸ì˜ ìš©ëŸ‰ ì¶•ì†Œ(1/2)
  - ì ì€ ì„±ëŠ¥ ì €í•˜
  - GPU ìƒì—ì„œ ë¹ ë¥¸ ì—°ì‚°(ëŒ€ì²´ë¡œ fp32ë¥¼ ìƒíšŒ)
- Cons ğŸ˜¥:
  - CPU ìƒì—ì„œëŠ” fixed point ì—°ì‚°ë§Œí¼ì˜ ì†ë„ í–¥ìƒì´ ìˆì§€ëŠ” ì•ŠìŒ

### 1.3 Quantization Aware Training(QAT)

**Overview**

- í•™ìŠµ ê³¼ì •ì— quantization ì„ emulate í•˜ì—¬(fake quantization), inference ì‹œì— ë°œìƒí•˜ëŠ” quantization error ë¥¼ training ì‹œì ì—
ë°˜ì˜ê°€ëŠ¥í•˜ë„ë¡ í•¨
- ë³´í†µì€ ì¼ë°˜ì ì¸ ë°©ë²•ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ê³ , fine-tuning ìœ¼ë¡œ QAT ë¥¼ ì ìš©
- í•™ìŠµ ê³¼ì •ì— emulate ëœ quantization íŒŒë¼ë¯¸í„°ë¥¼ inference ì—ë„ ì‚¬ìš©
- PTQ ëŒ€ë¹„ ì„±ëŠ¥ í•˜ë½ í­ì´ ì ìŒ

![]({{site.url}}/assets/images/boostcamp/91467dc0.png)

**í•™ìŠµ ê³¼ì •**

- í•™ìŠµ ê³¼ì • ì¤‘ quantization ì„ ì ìš©í•˜ê³ , ë‹¤ì‹œ floating pont ë¡œ ë³€í™˜í•¨(backprop ì„ ê³„ì‚°í•˜ê¸° ìœ„í•¨)
- In-out ì— ëŒ€í•œ gradient ë¥¼ linear ë¡œ ê°€ì •(straight-through estimator)í•¨ìœ¼ë¡œì¨ ë„¤íŠ¸ì›Œí¬ í•™ìŠµì„ ìˆ˜í–‰

![]({{site.url}}/assets/images/boostcamp/0e3a1e39.png)

**PTQ vs QAT ì„±ëŠ¥ ë¹„êµ**

- Per channel(ì±„ë„ ë‹¨ìœ„), Per layer(ë ˆì´ì–´(í…ì„œ ë‹¨ìœ„))
- Asymmetric/Symmetric; zero point ìœ /ë¬´

![]({{site.url}}/assets/images/boostcamp/ec8fa47c.png)

## 2. Quantization ì‹¤ìŠµ; PyTorch

### 2.1 Fp16 quantization

**ê¸°ë³¸ ëª¨ë¸ ì…‹ì—…**

- ì˜ˆì œ ìš©ë„ì˜ ê°„ë‹¨í•œ ëª¨ë¸ ì…‹ì—… with CIFAR10

![]({{site.url}}/assets/images/boostcamp/cb13a37b.png)

### 2.2 Fp16 inference

**Fp32 $\rightarrow$ Fp16**

- Input data, Model weight tensor ì˜ dtype ì„ fp16 ìœ¼ë¡œ ë³€í™˜(half() ë¡œ ê°„ë‹¨íˆ ì‚¬ìš© ê°€ëŠ¥)
- í•™ìŠµ ê³¼ì •ì—ì„œ ë˜í•œ Fp16 ìœ¼ë¡œ í•™ìŠµì´ ê°€ëŠ¥í•œë°, ì´ ê²½ìš° numerically unstable í•œ ì—°ì‚°ì´ë‚˜ ë ˆì´ì–´ì˜ ì¼ë¶€ëŠ” fp32 ë¡œ ìœ ì§€í•˜ëŠ” 
Mixed precision ê¸°ëŠ¥ì„ í™œìš©í•´ì•¼í•¨[9]

![]({{site.url}}/assets/images/boostcamp/1c07b695.png)

![]({{site.url}}/assets/images/boostcamp/7cb3bc94.png)

**Fp32 $\rightarrow$ Fp16; ì„±ëŠ¥, ì†ë„ ë¹„êµ**

![]({{site.url}}/assets/images/boostcamp/93244349.png)

### 2.3 Post Training Quantization(PTQ)

**PyTorch ì—ì„œ ìš”êµ¬í•˜ëŠ” ì¤€ë¹„ì‚¬í•­**

![]({{site.url}}/assets/images/boostcamp/a8b83e44.png)

**PyTorch ì—ì„œ ìš”êµ¬í•˜ëŠ” ì¤€ë¹„ì‚¬í•­(PTQ, QAT)**

![]({{site.url}}/assets/images/boostcamp/ca6573a8.png)

**QuantStub, DeQuantStub ì¶”ê°€**

- `__init__` í•¨ìˆ˜ ë‚´ì— QuantStub(), DeQuantStub() ì¶”ê°€
- QuantStub():  
floating point tensor ë¥¼ quantized tensor ë¡œ ë³€í™˜
- DeQuantStub():  
quantized tensor ë¥¼ floating point tensor ë¡œ ë³€í™˜

![]({{site.url}}/assets/images/boostcamp/f9a547e1.png)

- Forward í•¨ìˆ˜ ë‚´ì— ëª¨ë¸ì˜ ì• ë’¤ì— ì„ ì–¸í•œ quant, dequant í•¨ìˆ˜ë¥¼ ì¶”ê°€

![]({{site.url}}/assets/images/boostcamp/cdba9e11.png)

**PTQ ì ìš© ê³¼ì • ì„¤ëª…**

1) Quantization configuration ì„ ìˆ˜í–‰(backend; X86 CPU: fbgemm, ARM CPU: qnnpack)
  ![]({{site.url}}/assets/images/boostcamp/28cc97e5.png)
2) fuse_modules ì ìš© ë° prepare ìˆ˜í–‰
  ![]({{site.url}}/assets/images/boostcamp/1228d3ae.png)
3) Calibration ìˆ˜í–‰
  ![]({{site.url}}/assets/images/boostcamp/76f85a92.png)
4) Convert ìˆ˜í–‰
  ![]({{site.url}}/assets/images/boostcamp/a0e94839.png)

**ì˜ˆì œ ëª¨ë¸ ì ìš© ê²°ê³¼**

![]({{site.url}}/assets/images/boostcamp/be06489a.png)

**QAT ì ìš© ê³¼ì •**

1) Quantization configuration ì„ ìˆ˜í–‰(backend; X86 CPU: fbgemm, ARM CPU: qnnpack)
  ![]({{site.url}}/assets/images/boostcamp/d6a6fe8d.png)
2) fuse_modules ì ìš© ë° prepare_qat ìˆ˜í–‰
  ![]({{site.url}}/assets/images/boostcamp/580d8e23.png)
3) ëª¨ë¸ í•™ìŠµ(QAT) ìˆ˜í–‰
  ![]({{site.url}}/assets/images/boostcamp/a12f3ef0.png)
4) Convert ìˆ˜í–‰
  ![]({{site.url}}/assets/images/boostcamp/16e4916c.png)

**QAT ì ìš© ê³¼ì •; train í•¨ìˆ˜ snippet**

- PyTorch(v1.8)ì—ì„œ QAT ì˜ ê²½ìš° cpu ë§Œ ì§€ì›ì„ í•˜ëŠ” ìƒíƒœ. ë”°ë¼ì„œ, Fine tuning ì •ë„ë¡œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ ëŒ€ë‹¤ìˆ˜(epoch 10 ì´ë‚´)
- Epoch 3 ì´ˆê³¼ë¶€í„°ëŠ” quantization parameter ë¥¼ ê³ ì •
- Epoch 2 ì´ˆê³¼ë¶€í„°ëŠ” bn stats ì„ ê³ ì •
- Evaluate ì‹œì—ëŠ” quantization.convertë¥¼ ìˆ˜í–‰í•˜ì—¬ ì¸¡ì •í•¨ì„ ì£¼ëª©

![]({{site.url}}/assets/images/boostcamp/0dc70b55.png)

**QAT ê²°ê³¼ ì˜ˆì‹œ**

![]({{site.url}}/assets/images/boostcamp/3b241ad1.png)

![]({{site.url}}/assets/images/boostcamp/0e8a3e40.png)

**ë…¼ì˜**

- ëª¨ë¸ì´ ì‚¬ìš©ë  target device ì— ë”°ë¼ ì ìš©í•  ë°©ë²•ì´ ë‹¬ë¼ì§ˆ ê²ƒ
- ì¼ë°˜ì ì¸ gpu í™˜ê²½ì—ì„œ ì‰½ê²Œ ì†ë„ ë° ìš©ëŸ‰ì— ëŒ€í•œ ê°œì„ ì„ í•„ìš”ë¡œ í•œë‹¤ë©´ fp16 ë³€í™˜ì„  
  (+ í•™ìŠµë„ ë¹ ë¥´ê²Œí•˜ê³ ì í•œë‹¤ë©´ Mixed Precision)
- CPU ì—ì„œ inference ë¥¼ í•˜ê³ ì í•œë‹¤ë©´, fixed point quantization ì€ ê±°ì˜ í•„ìˆ˜ì 
- Dynamic $\rightarrow$ PTQ $\rightarrow$ QAT ìˆœìœ¼ë¡œ ì„±ëŠ¥ í•˜ë½ì˜ í­ì€ ì¤„ì–´ë“¤ì§€ë§Œ, ë” ë”ìš± ë§ì€ ì¡°ê±´ ë° ì‘ì—…ì´ ì¶”ê°€ë¨
- íŠ¹íˆ PyTorch ì˜ ê²½ìš° quantization ì— ëŒ€í•œ ì§€ì›ì´ ì™„ë²½í•˜ì§€ ì•Šì•„ ì œëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•ŠëŠ” ê²½ìš°ë“¤ë„ ì¡´ì¬


# Further Reading
 
- [Pytorch official docs: quantization](https://pytorch.org/docs/stable/quantization.html)










