---
title: "Day_50 01. Reducing Training Bias"

categories:
  - Boostcamp_AI_Tech/Week_12
tags:
  - MRC
---
  
# Reducing Training Bias

## 1. Definition of Bias

**Bias 의 종류**

**Bias in learning**

- 학습할 때 과적합을 막거나 사전 지식을 주입하기 위해 특정 형태의 함수를 선호하는 것 (inductive bias)
- 모델을 의도한대로 학습하는 것도 bias 라고 할 수 있음

**A Biased World**

- 현실 세계가 편향되어 있기 때문에 모델에 원치 않는 속성이 학습되는 것 (historical bias)
- 성별과 직업 간 관계 등 표면적인 상관관계 때문에 원치 않는 속성이 학습되는 것 (co-occurence bias)

**Bias in Data Generation**

- 입력과 출력을 정의한 방식 때문에 생기는 편향 (specification bias)
- 데이터를 샘플링한 방식 때문에 생기는 편향 (sampling bias)
- 어노테이터의 특성 때문에 생기는 편향 (annotator bias)

**Gender Bias**

- 대표적인 bias 예시
- 특정 성별과 행동을 연관시켜서 예측 오류가 발생

![]({{site.url}}/assets/images/boostcamp/c274986e.png)

cooking 이라는 개념을 봤을 때 여자만 cooking 을 하진 않음 그런데 어떤 모델이 항상 여자만 cooking 을 한다고 데이터를 통해서든 많이 보다보니
보다시피 남자가 cooking 을 하는 경우에도 agent 가 womon 으로 착각하는 경우가 발생함

이런 경우를 bias 라고 보면 됨

![]({{site.url}}/assets/images/boostcamp/68185c65.png)

다른 예시는 Google Translation 에서도 많이 보여지고 있음

터키어 "o bir doctor" 는 "그 사람은 의사다" 라는 의미이고 이 문장을 Google Translation 에 넣게되면 영어로는 "He is a doctor" 이라고 나옴

왜 이럴까? 학습한 데이터에서 실제로 doctor 가 남자인 경우가 많다보니까 model 은 doctor 라는 표현을 쓰면 아마도 남자이지 않을까라고 assume 을 
해버림

구글이 실제로 의도하지 않았다하더라도 실제로는 사회적으로 문제가 있기때문에 이런 부분을 해결하기 위해서 많은 노력들이 있음

**Sampling Bias**

<리터러시 다이제스트> 여론조사 (1936년)

- 표본 크기 : 240만 명 (사상 최대)
- 예측 : 루즈벨트 43% 알프레드 랜던 57% $\rightarrow$ 실제 : 루즈벨트 62% 알프레드 랜던 38%
- 설문 대상 : 잡지 정기구독자, 자동차 등록명부, 사교클럽 인명부 등 $\rightarrow$ 중산층 이상으로 표본이 왜곡
- 2년 후 리터러시 다이제스트 파산

![]({{site.url}}/assets/images/boostcamp/5de1910e.png)

샘플링을 할 때 랜덤하게(fair 하게) 샘플링 했어야하는데 편향된 상태로 샘플링이 되다보니 편향이 생김

이처럼 Gender bias 나 Sampling bias 는 피해야하는 것들이고 inductive bias 같이 항상 나쁘다고 느낄 필요는 없음

보통으로는 bias 는 문제가 있고 그걸 해결해야하는구나라고 생각해주면 좋을 것 같음

## 2. Bias in Open-domain Question Answering

**Retrieval-Reader Pipeline**

![]({{site.url}}/assets/images/boostcamp/a8356045.png)

**$\rightarrow$ We will focus on the bias in reader model**

**Training bias in reader model**

만약 reader 모델이 한정된 데이터셋에서만 학습이 된다면...

- Reader 은 항상 정답이 문서 내에 포함된 데이터쌍만(Positive)을 보게 됨
- 예) 특히 SQuAD 와 같은 (Context, Query, Answer)이 모두 포함된 데이터는 positvie 가 완전히 고정되어 있음
  - Inference 시 만약 데이터 내에서 찾아볼 수 없었던 새로운 문서를 준다면?
  - Reader 모델은 문서에 대한 독해 능력이 매우 떨어질 것이고, 결과적으로 정답을 내지 못할 것임

![]({{site.url}}/assets/images/boostcamp/d6b780a8.png)

**How to mitigate training bias?**

1. Train negative examples

훈련할 때 잘못된 예시를 보여줘야 retriever 이 negative 한 내용들을 먼 곳에 배치할 수 있음  
$\rightarrow$ Negative sample 도 완전히 다른 negative 와 비슷한 negative 에 대한 차이 고려 필요함

![]({{site.url}}/assets/images/boostcamp/36be64b2.png)

어떻게 좋은 negative sample 을 만들 수 있을까? (5강)

1) Corpus 내에서 랜덤하게 뽑기
2) 좀 더 헷갈리는 negative 샘플들 뽑기
- 높은 BM25 / TF-IDF 매칭 스코어를 가지지만, 답을 포함하지 않는 샘플
- 같은 문서에서 나온 다른 Passage/Question 선택하기

2. Add no answer bias

입력 시퀀스의 길이가 N 일시, 시퀀스의 길이 외 1개의 토큰이 더 있다고 생각하기  
$\rightarrow$ 훈련 모델의 마지막 레이어 weight 에 훈련 가능한 bias 를 하나 더 추가  
$\rightarrow$ Softmax 로 answer prediction 을 최종적으로 수행할 때, start end 확률이 해당 bias 위치에 있는 경우가
가장 확률이 높으면 이는 "대답 할 수 없다"라고 취급

## 3. Annotation Bias from Datasets

**What is annotation bias?**

Annotation bias :  
ODQA 학습 시 기존의 MRC 데이터셋 활용  
$\rightarrow$ ODQA 세팅에는 적합하지 않은 bias 가 데이터 제작 (annotation) 단계에서 발생할 수 있음

![]({{site.url}}/assets/images/boostcamp/7d090610.png)

![]({{site.url}}/assets/images/boostcamp/5d1bfec7.png)

질문하는 사람이 답을 알고 있지 않은게 실제 유저의 Question Answering 시나리오임

여러분이 agent 에게 질문을 할 때는 답을 모르니까 질문을 하잖아요? 그렇기 때문에 그 경우를 최대한 시뮬레이션해야 올바른(bias 가 없는) 데이터셋을
확보할 수 있는데 문제점은 실제로 데이터셋을 만들 때 그렇게 하기가 쉽지가 않다보니 <U>질문을 하는 사람이 답을 알고 있는 상태로 질문하는 편향</U>이 발생

이런 편향때문에 원치 않는 artifact 가 질문이나 답에 들어가는 경우가 많이 발생함

실제로 이런게 심한 케이스가 SQuAD 라고 볼 수 있음

![]({{site.url}}/assets/images/boostcamp/4d03e3ef.png)

$\rightarrow$ 질문을 하는 사람이 답을 알고 있음 / 질문과 evidence 문단 사이의 많은 단어가 겹치는 bias 발생 가능

질문과 지문을 보면서 만들다보니 단어가 겹칠 확률이 높음

$\rightarrow$ SQuAD : only 500+ wiki article $\rightarrow$ 학습 데이터의 분포 자체가 이미 bias 되어 있음

사람들이 많이 보는 500개 문장을 사용한 데이터라서 랜덤 샘플링한 데이터가 아니다보니 bias 가 존재

![]({{site.url}}/assets/images/boostcamp/dff0e0e3.png)

TriviaQA & SQuAD 같은 경우엔 심한 케이스고 Question 을 쓴 사람이 답을 알고있는 것 뿐만 아니라 SQuAD 같은 경우는 실제로 그 답이
어느 문단(paragraph) 에서 나왔는지 까지 알고있기 때문에 bias 가 더 심해지게 됨

**Effect of annotation bias**

Annotation bias:

ODQA 세팅에는 적합하지 않은 bias 가 데이터 제작(annotation) 단계에서 발생할 수 있음

$\rightarrow$ 데이터셋 별 성능 차이가 annotation bias 로 인 해 발생할 수 있음

(BM25: Sparse embedding / DPR: dense embedding)

![]({{site.url}}/assets/images/boostcamp/d0916777.png)

BM25 같은 경우 단어가 겹치는 경우에 더 잘찾을 수 있음

그래서 SQuAD 만 BM25 가 DPR 보다 성능이 좋음

하지만 SQuAD 에서는 이 둘을 합치면 점수가 더올라감

다른애들은 떨어질 수도 있고 올라갈 수도 있음

**Dealing with annotation bias**

Annotation 단계에서 발생할 수 있는 bias 를 인지하고, 이를 고려하여 데이터를 모아야 함

ex) ODQA 세팅과 유사한 데이터 수집 방법

$\rightarrow$ Natural Questions: Supporting evidence 가 주어지지 않은, 실제 유저의 question 들을 모아서 dataset 을 구성

https://ai.google.com/research/NaturalQuestions/visualization

![]({{site.url}}/assets/images/boostcamp/6603f3f4.png)

**Another bias in MRC dataset**

SQuAD : Passage 가 주어지고, 주어진 passage 내에서 질문과 답을 생성

$\rightarrow$ ODQA 에 applicable 하지 않은 질문들이 존재

![]({{site.url}}/assets/images/boostcamp/2e44084a.png)

"미국의 대통령이 누구인가?" 라고 질문했을 때 이 질문은 ODQA 에서는 상당히 적합하지 않음

지금 대통령인가? 5년전 대통령? 또는 10년전 대통령? 언제 때 대통령을 얘기하는 것인가에 대한 specification 이 필요한데
passage 가 주어진 상태에서는 specifier 가 되어있기 때문에 특정한 process 가 필요없을 수도 있음

그러다보니 ODQA 에서 특정하기 힘든 질문들이 나오는 경우가 많아지고 
위 그림속 "what did he do for $2 a day?" 예제같은 경우도 ODQA 에서 의미가 없는 질문들임










