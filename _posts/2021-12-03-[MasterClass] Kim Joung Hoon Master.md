---
title: "Day_84 [마스터클래스] 모델최적화대회 - 김정훈 마스터"

categories:
  - Boostcamp_AI_Tech/Week_18/Day_84
tags:
  - 마스터클래스
---

# [마스터클래스] 모델최적화대회 - 김정훈 마스터

## CV-19 : 빨간맛 조 - 정승균 캠퍼

VGG19 를 튜닝해서 좋은 결과를 얻었음

정사각형에 맞춰서 넣는게 성능이 좋았다고해서 비슷하게 진행했음

augmentation 은 Flip 하고 회전 사용한게 제일 좋았음

다른걸 적용해봐도 성능이 오르지 않았음

loss 는 crossentropy 사용했고 optimzier 는 Adam 을 사용했음

NAS 를 활용하지 않았음

이유

시간이 너무 오래 거릶

pre-trained weight 를 사용할 수 없다는 점

왜 VGG?

Inference 속도를 확인했을 때 어느 부분에서 시간 소요가 가장 많았냐고하면 Profiling 을 하는데 시간이 많이 소요된다는걸 발견했고

비교적 모델이 간단한 Alexnet 이나 VGG 가 속도를 줄일 수 있어서 사용했음

VGG 가 용량이 500M 정도 되는데 width, depth, resolution 을 건드려 보는게 좋다고 생각

layer 줄였을 때 성능이 조금 떨어지는데 시간이 많이 떨어짐

convolution 을 줄이는건 성능이 많이 저하됨

resolution 은 128x128 일 때 큰 성능차이가 없으면서 시간을 많이 줄일 수 있었음

> 멘토 피드백

멘토분들도 어떻게 했지? 라고 생각했음

수업의 설계의도를 먼저 말하면 프로젝트를 수행하는데 많은 시간을 쓴다고 알고있어서 버그가 많던 tune.py 를 돌려서 5일정도 된 결과를 학습을 길게 시켜서
baseline 으로 잡아놨던건데 돌려놓고 딴거하는(프로젝트 혹은 학습) 그런 의도가 있었음

그런데도 이렇게 좋은 방법을 적용하신게 놀라웠음

고생 많이하신것 같고 하나 더 덧붙여드리자면 pre-training 을 못쓴다는게 굉장히 큰 이슈인데 pre-training 을 사용하는 NAS 기법들이 있는데
그런쪽의 연구를 보는것도 좋을 것 같고 더해서 epoch 을 더 길게 가져가면 성능이 잘 나오긴 함

## CV-5조 친구 구했조 - 이경민 캠퍼

MobileNetV3 은 파라미터가 4.2M 개인데 Resnet18 은 11.1M 개 인데 속도가 오히려 Resnet 이 더 빠름

왜?

layer 의 개수때문이라고 생각

Layer 와 Time 의 관계는 Layer 가 늘어나면 Time 이 늘어난다라는 결과를 얻음

Layer 를 줄이고 Kernel 을 늘리자라는 생각으로 진행했음

> 멘토 피드백

Layer 를 줄이는게 효과적일 수 있음

layer 가 많아지면 많아질수록 GPU 가 중간결과를 기다려야하거나 그럴 수 있는데 CPU 에서 돌려야한다면 연산횟수가 적은게 속도향상이 큰 경우가 많음

하드웨어 dependency 에 맞게 잘 적용한 것 같음

접근 자체가 논리적이어서 좋은것 같음

## NLP15조 HappyFace - 안영진 캠퍼

pre-trained weight 이 없는 깡통 모델을 가지고 train 을 많이 시킨다고해도 성능이 잘나올까? 가 의문이었음

KD 를 한다고해도 적어도 200epoch 학습을 해야 성능이 올랐음

시간적 소요가 있음

그래서 ELimNet Method 방법을 사용했음

Top layer 들을 없애면 좋지 않을까?

Convolution layer 들을 2개 3개 날려봤음

결국 깡통보다는 pre-trained 된 모델이 성능이 좋더라

> 멘토 피드백

대회의 포맷 자체가 비전 task 여서 NLP 분들이 의욕을 내기는 좀 어려운 형태의 대회일 것 같다고 생각했는네 NLP 에 있는 아이디어를 가져오려고 해주신 것도
그렇고 감사합니다.

NLP 쪽이 경량화 needs 가 더 큼

제공했던 강의가 굉장히 간단한 내용이긴 하지만 현업에서 많이 쓰일 수 있었으면 좋겠다는 생각이 있음

여러분들이 접근하는 방법자체가 3개 팀이 다 다른데 정답은 없다고 생각하고 1등팀이 잘 한것은 맞지만 지금 해주신 분들의 장단점이 있을 것임

참여하신 모든 분들의 approach 들이 계속해서 발전해 나가는 여지가 있을 거라고 생각함

---

# Q & A

![]({{site.url}}/assets/images/278ed082.png)

최적화 경량화는 업계 성숙도로 봤을 때 아직이다라는 생각을 많이함

모델 개발하고 deploy 하는 서비스를 만드는 쪽의 개발 단계가 성숙한 단계가 아니다보니까 서비스를 만들어내고 해나가는 과정에 포커싱이 많이 되어있음

지금 경량화나 최적화를 전문적으로 하는 회사는 많지 않음

자기의 전문분야가 있고 서브로 최적화를 가지고 있는게 도움이 될 것 같음

주니어 개발자로 들어간다라고 할 때 제가 직접 채용과정에 참여한다면 뭘 했다가 중요한 시대는 아닌 것 같음

스택을 가지고 있다는 중요할 수 있는데 어떤 프로젝트를 진행을 했건 어떠한 전략으로 했느냐가 더 중요함

뭘 선택했느냐에 대한 정답이 없고 뭘 선택해야 할지를 모르는건데 어떤 기준으로 골랐는지가 중요하고 그걸 고르는 과정에서 팀원들이 어떻게 정했는지가
중요함

프로젝트를 준비하면서도 이걸 왜 이렇게 적용했고 왜 했는지를 잘 풀어내서 어필하는게 더 좋은것 같음

하나를 하더라도 장인정도의 깊이로 공부하는게 중요함

같이 공부해나가는 입장에서 서로 배워나갈 수 있는 사람임을 어필하는게 중요함

![]({{site.url}}/assets/images/72eaca53.png)

너무 케바케라 어려운 부분은 있음

하루이틀 돌려보고 결과가 잘 안나와서 넘어간 조원이 많다고 들었음

리소스가 많지 않다보니 인내심을 가지긴 어려웠을 것 같음

wandb 랑 DB 붙여놓고 일주일 지나면 대회성능이 나올 거라고 생각했는데 캠퍼들의 심리상태를 고려하지 못한 것 같음

search 할 때 데이터가 얼마나 있어야하느냐면 답변이 어렵지만 감이 생겨야 하는게 맞는것 같음

300장 이상은 있어야 돌아가고 1000장 이상은 되어야 의미가 있는 것 같음

하이퍼 파라미터도 데이터마다 다름.... 답변이 영양가가 없음...

현업에서 하이퍼 파라미터 search 에 대해서 굉장히 넓게 가져가고 NAS 에 맡기고 있음

현시점에서 이걸 정확히 아는 집단은 없다고 생각함

어떻게 하면 데이터에 들어있는 어떤 하이퍼파라미터가 적절한지 사람들은 감이 있는데 이걸 어떻게 내재화해서 search 구조를 만들어내는가가 추구하는 방향중에
하나일 수 있음

> 작은 기기에서 GPU 가 없어서 CPU 만 사용해야 하는 일이 많나요?

많습니다.

NVIDIA 에서 나온 잿슨 나노 10만원 짜리가 있는데 요구사항은 5만원짜리 CPU 만 있는 모델에서 돌리게 해주세요 라는 요청이 많이옴

회사의 입장에서 돈을 줄이려고 생각하기 때문에

> 경량화나 프로덕트 서빙에 주로 일하시는 분들도 sota 모델 같이 모델 자체에 대한 공부도 계속 하시나요?
 
경량화에서도 research 포지션이 있고 project 를 수행하는 포지션도 있음

경량화를 자동화하는 product 를 만드는 팀도 있음

이런 관점에서 봤을 때 research 를 하는 포지션은 SOTA 모델도 공부해야하고 product 를 수행하는 사람들은 파이프라인을 만드는걸 더 신경쓰는데
전부 다 공부해야 하지 않나 싶음

전문적으로 product serving 만 하는 사람들은 모델링을 몰라도 됨

스타트업 같은 경우는 그렇지 않아서 다른 겹치는 일들도 다 해야하기 때문에 research 만큼은 아니지만 어느 정도 알고 있어야 하는게 있음

모델링도해야하고 서빙도해야하는 경우도 있기 때문에 모든걸 다해야할 수 있음

회사마다 다른 것 같음

