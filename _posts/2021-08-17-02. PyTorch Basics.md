---
title: "Day_11 02. PyTorch Basics"

categories:
  - Boostcamp_AI_Tech/Week_3/Day_11
tags:
  - PyTorch
---

# PyTorch Basics

## PyTorch Operations

- numpy + AutoGrad

> Tensor

- 다차원 Arrays 를 표현하는 PyTorch 클래스
- 사실상 numpy 의 ndarray 와 동일
  - 그러므로 TensorFlow 의 Tensor 와도 동일
- Tensor 를 생성하는 함수도 거의 동일

- numpy - ndarray

```python
import numpy as np
n_array = np.arange(10).reshape(2, 5)
print(n_array)
print('ndim :', n_array.ndim, 'shape :', n_array.shape)
```

- pytorch - tensor
```python
import torch
t_array = torch.FloatTensor(n_array)
print(t_array)
print('ndim :', t_array.ndim, 'shape :', t_array.shape)
```

> Array to Tensor

- Tensor 생성은 list 나 ndarray 를 사용 가능

- data to tensor

```python
data = [[3, 5], [10, 5]]
x_data = torch.tensor(data)
x_data
```

- ndarray to tensor

```python
nd_array_ex = np.array(data)
tensor_array = torch.from_numpy(nd_array_ex)
tensor_array
```

> Tensor data types

- 기본적으로 tensor 가 가질 수 있는 data 타입은 numpy 와 동일
- 한 가지 차이가 있음
  - GPU tensor 를 쓸 수 있게 해주냐 아니냐의 차이

> numpy like operations

- 기본적으로 pytorch 의 대부분의 사용법이 그대로 적용됨
  - 슬라이싱
  - flatten()
  - ones_like
  - numpy()
  - shape
  - dtype

- pytorch 의 tensor 는 GPU 에 올려서 사용가능

> Tensor handling

- view, squeeze, unsqueeze 등으로 tensor 조정 가능
  - veiw : reshape 과 동일하게 tensor 의 shape 을 변환
    - copy 를 한 것이 아니라 기존의 메모리 주소를 공유하고 형태만 바꿔줌
  - squeeze : 차원의 개수가 1인 차원을 삭제 (압축)
  - unsqueeze : 차원의 개수가 1인 차원을 추가

> Tensor operations

- 기본적인 tensor 의 operations 는 numpy 와 동일

- 행렬곱셈 연산은 함수는 dot 이 아닌 mm 사용
    - mm : vector 간의 연산은 지원 안해줌
    - dot : vector 간의 연산

- mm 과 matmult 은 broadcasting 지원 차이
  - mm : broadcasting 지원 안해줌
  - matmul : braodcasting 지원

> Tensor operations for ML/DL formula

- nn.functional 모듈을 통해 다양한 수식 변환을 지원함

> AutoGrad

- PyTorch 의 핵심은 자동 미분의 지원 -> backward 함수 사용

- external_grad????????? 이게 뭘까??