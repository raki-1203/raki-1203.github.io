---
title: "[Week9] 한국어 언어 모델 학습 및 다중 과제 튜닝"
permalink: /Boostcamp_AI_Tech/Week_9/
layout: category
author_profile: true
sidebar:
    nav: "docs"
---

### [Day37] KLUE 1 ~ 2강

- [01. 인공지능과 자연어 처리]({{site.url}}/boostcamp_ai_tech/week_9/day_37/01.-AI-and-NLP/)
- [02. 자연어의 전처리]({{site.url}}/boostcamp_ai_tech/week_9/day_37/02.-Preprocessing-of-Natural-Language/)
- [오피스아워 AI 신약 개발]({{site.url}}/boostcamp_ai_tech/week_9/day_37/OfficeHour-AI-drug-development/)

### [Day38] KLUE 3 ~ 5강

- [01. BERT 언어모델 소개]({{site.url}}/boostcamp_ai_tech/week_9/day_38/01.-Introduce-BERT-Language-Model/)
- [02. 한국어 BERT 언어모델 학습]({{site.url}}/boostcamp_ai_tech/week_9/day_38/02.-Train-Korean-BERT-Language-Model/)
- [03. BERT 기반 단일 문장 분류 모델 학습]({{site.url}}/boostcamp_ai_tech/week_9/day_38/03.-single-sentence-classification-based-BERT-train/)


### [Week9 피어세션 정리](https://github.com/sangmandu/SangSangPlus/tree/main/Meet-up%20log/Week%200)

---
### 학습회고

Day 37

한국어 전처리 과정에 대한 실습코드를 통해서 굉장히 다양한 전처리 과정을 알아봤다.

아직 대회를 위한 baseline 코드도 보지 못했지만 대회에 굉장히 많은 도움을 받을 것 같다.

오늘은 강의를 정리하는데에도 시간이 많이 흘렀는데 얼른 집중해서 baseline 코드를 돌려봐야겠다.

Day 38

강의를 들으면서 transformers 라이브러리를 사용하는 방법에 대해 배우고 있다.

실습코드가 너무 훌륭해서 너무 좋다고 생각한다.

얼른 baseline 코드를 수정해서 실험에 알맞게 변경하고 많은 시도를 해보고 싶다.

baseline 코드에서 xlm-roberta-large 모델을 돌려놓고 내일 테스트해보려한다.

20 epoch 돌리는데 8시간40분정도 걸린다. 

돌리는 동안 EDA 나 해봐야겠다.